[
  {
    "objectID": "01_refresher.html#coordinates",
    "href": "01_refresher.html#coordinates",
    "title": "\n1  Refresher\n",
    "section": "\n1.2 Coordinates",
    "text": "1.2 Coordinates\nIn general, spatial data is structured like conventional/tidy data (e.g. data.frames, matrices), but has one additional dimension: every observation is linked to some sort of geo-spatial information. Most common types of spatial information are:\n\nPoints (one coordinate pair)\nLines (two coordinate pairs)\nPolygons (at least three coordinate pairs)\nRegular grids (one coordinate pair for centroid + raster / grid size)\n\n\n1.2.1 Coordinate reference system (CRS)\nIn its raw form, a pair of coordinates consists of two numerical values. For instance, the pair c(51.752595, -1.262801) describes the location of Nuffield College in Oxford (one point). The fist number represents the latitude (north-south direction), the second number is the longitude (west-east direction), both are in decimal degrees.\n\n\nFigure: Latitude and longitude, Source: Wikipedia\n\nHowever, we need to specify a reference point for latitudes and longitudes (in the Figure above: equator and Greenwich). For instance, the pair of coordinates above comes from Google Maps which returns GPS coordinates in ‘WGS 84’ (EPSG:4326).\n\n# Coordinate pairs of two locations\ncoords1 &lt;- c(51.752595, -1.262801)\ncoords2 &lt;- c(51.753237, -1.253904)\ncoords &lt;- rbind(coords1, coords2)\n\n# Conventional data frame\nnuffield.df &lt;- data.frame(name = c(\"Nuffield College\", \"Radcliffe Camera\"),\n                          address = c(\"New Road\", \"Radcliffe Sq\"),\n                          lat = coords[,1], lon = coords[,2])\n\nhead(nuffield.df)\n\n                    name      address      lat       lon\ncoords1 Nuffield College     New Road 51.75259 -1.262801\ncoords2 Radcliffe Camera Radcliffe Sq 51.75324 -1.253904\n\n# Combine to spatial data frame\nnuffield.spdf &lt;- st_as_sf(nuffield.df, \n                          coords = c(\"lon\", \"lat\"), # Order is important\n                          crs = 4326) # EPSG number of CRS\n\n# Map\nmapview(nuffield.spdf, zcol = \"name\")\n\n\n\n\n\n\n\n1.2.2 Projected CRS\nHowever, different data providers use different CRS. For instance, spatial data in the UK usually uses ‘OSGB 1936 / British National Grid’ (EPSG:27700). Here, coordinates are in meters, and projected onto a planar 2D space.\nThere are a lot of different CRS projections, and different national statistics offices provide data in different projections. Data providers usually specify which reference system they use. This is important as using the correct reference system and projection is crucial for plotting and manipulating spatial data.\nIf you do not know the correct CRS, try starting with a standards CRS like EPSG:4326 if you have decimal degree like coordinates. If it looks like projected coordinates, try searching for the country or region in CRS libraries like https://epsg.io/. However, you must check if the projected coordinates match their real location, e.g. using mapview().\n\n1.2.3 Why different projections?\nBy now, (most) people agree that the earth is not flat. So, to plot data on a 2D planar surface and to perform certain operations on a planar world, we need to make some re-projections. Depending on where we are, different re-projections of our data (globe in this case) might work better than others.\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nclass(world)\n\n[1] \"sf\"         \"data.frame\"\n\nst_crs(world)\n\nCoordinate Reference System:\n  User input: +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 \n  wkt:\nBOUNDCRS[\n    SOURCECRS[\n        GEOGCRS[\"unknown\",\n            DATUM[\"World Geodetic System 1984\",\n                ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                    LENGTHUNIT[\"metre\",1]],\n                ID[\"EPSG\",6326]],\n            PRIMEM[\"Greenwich\",0,\n                ANGLEUNIT[\"degree\",0.0174532925199433],\n                ID[\"EPSG\",8901]],\n            CS[ellipsoidal,2],\n                AXIS[\"longitude\",east,\n                    ORDER[1],\n                    ANGLEUNIT[\"degree\",0.0174532925199433,\n                        ID[\"EPSG\",9122]]],\n                AXIS[\"latitude\",north,\n                    ORDER[2],\n                    ANGLEUNIT[\"degree\",0.0174532925199433,\n                        ID[\"EPSG\",9122]]]]],\n    TARGETCRS[\n        GEOGCRS[\"WGS 84\",\n            DATUM[\"World Geodetic System 1984\",\n                ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                    LENGTHUNIT[\"metre\",1]]],\n            PRIMEM[\"Greenwich\",0,\n                ANGLEUNIT[\"degree\",0.0174532925199433]],\n            CS[ellipsoidal,2],\n                AXIS[\"latitude\",north,\n                    ORDER[1],\n                    ANGLEUNIT[\"degree\",0.0174532925199433]],\n                AXIS[\"longitude\",east,\n                    ORDER[2],\n                    ANGLEUNIT[\"degree\",0.0174532925199433]],\n            ID[\"EPSG\",4326]]],\n    ABRIDGEDTRANSFORMATION[\"Transformation from unknown to WGS84\",\n        METHOD[\"Geocentric translations (geog2D domain)\",\n            ID[\"EPSG\",9603]],\n        PARAMETER[\"X-axis translation\",0,\n            ID[\"EPSG\",8605]],\n        PARAMETER[\"Y-axis translation\",0,\n            ID[\"EPSG\",8606]],\n        PARAMETER[\"Z-axis translation\",0,\n            ID[\"EPSG\",8607]]]]\n\n# Extract a country and plot in current CRS (WGS84)\nger.spdf &lt;- world[world$name == \"Germany\", ]\nplot(st_geometry(ger.spdf))\n\n\n\n# Now, let's transform Germany into a CRS optimized for Iceland\nger_rep.spdf &lt;- st_transform(ger.spdf, crs = 5325)\nplot(st_geometry(ger_rep.spdf))\n\n\n\n\nDepending on the angle, a 2D projection of the earth looks different. It is important to choose a suitable projection for the available spatial data. For more information on CRS and re-projection, see e.g. Lovelace, Nowosad, and Muenchow (2019) or Stefan Jünger & Anne-Kathrin Stroppe’s GESIS workshop materials."
  },
  {
    "objectID": "01_refresher.html#importing-some-real-word-data",
    "href": "01_refresher.html#importing-some-real-word-data",
    "title": "1  Refresher",
    "section": "1.2 Importing some real word data",
    "text": "1.2 Importing some real word data\nsf imports many of the most common spatial data files, like geojson, gpkg, or shp.\n\n1.2.1 London shapefile (polygon)\nLets get some administrative boundaries for London from the London Datastore. We use the sf package and its funtion st_read() to import the data.\n\n# Create subdir (all data withh be stored in \"_data\")\ndn &lt;- \"_data\"\nifelse(dir.exists(dn), \"Exists\", dir.create(dn))\n\n[1] \"Exists\"\n\n# Download zip file and unzip\ntmpf &lt;- tempfile()\nboundary.link &lt;- \"https://data.london.gov.uk/download/statistical-gis-boundary-files-london/9ba8c833-6370-4b11-abdc-314aa020d5e0/statistical-gis-boundaries-london.zip\"\ndownload.file(boundary.link, tmpf)\nunzip(zipfile = tmpf, exdir = paste0(dn))\nunlink(tmpf)\n\n# This is a shapefile\n# We only need the MSOA layer for now\nmsoa.spdf &lt;- st_read(dsn = paste0(dn, \"/statistical-gis-boundaries-london/ESRI\"),\n                     layer = \"MSOA_2011_London_gen_MHW\" # Note: no file ending\n                     )\n\nReading layer `MSOA_2011_London_gen_MHW' from data source \n  `C:\\work\\Lehre\\Geodata_Spatial_Regression\\_data\\statistical-gis-boundaries-london\\ESRI' \n  using driver `ESRI Shapefile'\nSimple feature collection with 983 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n\nThe object msoa.spdf is our spatial data.frame. It looks essentially like a conventional data.frame, but has some additional attributes and geo-graphical information stored with it. Most importantaly, notice the column geometry, which contains a list of polygons. In most cases, we have one polygon for each line / observation.\n\nhead(msoa.spdf)\n\nSimple feature collection with 6 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180510.7 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   MSOA11CD                 MSOA11NM   LAD11CD              LAD11NM\n1 E02000001       City of London 001 E09000001       City of London\n2 E02000002 Barking and Dagenham 001 E09000002 Barking and Dagenham\n3 E02000003 Barking and Dagenham 002 E09000002 Barking and Dagenham\n4 E02000004 Barking and Dagenham 003 E09000002 Barking and Dagenham\n5 E02000005 Barking and Dagenham 004 E09000002 Barking and Dagenham\n6 E02000007 Barking and Dagenham 006 E09000002 Barking and Dagenham\n    RGN11CD RGN11NM USUALRES HHOLDRES COMESTRES POPDEN HHOLDS\n1 E12000007  London     7375     7187       188   25.5   4385\n2 E12000007  London     6775     6724        51   31.3   2713\n3 E12000007  London    10045    10033        12   46.9   3834\n4 E12000007  London     6182     5937       245   24.8   2318\n5 E12000007  London     8562     8562         0   72.1   3183\n6 E12000007  London     8791     8672       119   50.6   3441\n  AVHHOLDSZ                       geometry\n1       1.6 MULTIPOLYGON (((531667.6 18...\n2       2.5 MULTIPOLYGON (((548881.6 19...\n3       2.6 MULTIPOLYGON (((549102.4 18...\n4       2.6 MULTIPOLYGON (((551550 1873...\n5       2.7 MULTIPOLYGON (((549099.6 18...\n6       2.5 MULTIPOLYGON (((549819.9 18...\n\n\nShapefiles are still among the most common formats to store and transmit spatial data, despite them being inefficient (file size and file number).\nHowever, sf reads everything spatial, such as geo.json, which usually is more efficient, but less common (but we’re getting there).\n\n# Download file\nulez.link &lt;- \"https://data.london.gov.uk/download/ultra_low_emissions_zone/3d980a29-c340-4892-8230-ed40d8c7f32d/Ultra_Low_Emissions_Zone.json\"\ndownload.file(ulez.link, paste0(dn, \"/ulez.json\"))\n\n# Read geo.json\nst_layers(paste0(dn, \"/ulez.json\"))\n\nDriver: GeoJSON \nAvailable layers:\n                layer_name geometry_type features fields crs_name\n1 Ultra_Low_Emissions_Zone       Polygon        1      2   WGS 84\n\nulez.spdf &lt;- st_read(dsn = paste0(dn, \"/ulez.json\")) # here dsn is simply the file\n\nReading layer `Ultra_Low_Emissions_Zone' from data source \n  `C:\\work\\Lehre\\Geodata_Spatial_Regression\\_data\\ulez.json' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -0.167077 ymin: 51.48627 xmax: -0.07213563 ymax: 51.53182\nGeodetic CRS:  WGS 84\n\nhead(ulez.spdf)\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -0.167077 ymin: 51.48627 xmax: -0.07213563 ymax: 51.53182\nGeodetic CRS:  WGS 84\n                 boundary\n1 Ultra Low Emission Zone\n                                                       url\n1 https://tfl.gov.uk/modes/driving/ultra-low-emission-zone\n                        geometry\n1 POLYGON ((-0.1046873 51.531...\n\n\nAgain, this looks like a conventional data.frame but has the additional column geometry containing the coordinates of each observation. st_geometry() returns only the geographic object and st_drop_geometry() only the data.frame without the coordinates. We can plot the object using mapview().\n\nmapview(msoa.spdf[, \"POPDEN\"])\n\n\n\n\n\n\n\n\n1.2.2 Census API (admin units)\nNow that we have some boundaries and shapes of spatial units in London, we can start looking for different data sources to populate the geometries.\nA good source for demographic data is for instance the 2011 census. Below we use the nomis API to retrieve population data for London, See the Vignette for more information (Guest users are limited to 25,000 rows per query). Below is a wrapper to avoid some errors with sex and urban-rural cross-tabulation in some of the data.\n\n### For larger request, register and set key\n# Sys.setenv(NOMIS_API_KEY = \"XXX\")\n# nomis_api_key(check_env = TRUE)\n\nx &lt;- nomis_data_info()\n\n# Get London ids\nlondon_ids &lt;- msoa.spdf$MSOA11CD\n\n### Get key statistics ids\n# select requires tables (https://www.nomisweb.co.uk/sources/census_2011_ks)\n# Let's get KS201EW (ethnic group), KS205EW (passport held), and KS402EW (housing tenure)\n\n# Get internal ids\nstats &lt;- c(\"KS201EW\", \"KS402EW\", \"KS205EW\")\noo &lt;- which(grepl(paste(stats, collapse = \"|\"), x$name.value))\nksids &lt;- x$id[oo]\nksids # This are the internal ids\n\n[1] \"NM_608_1\" \"NM_612_1\" \"NM_619_1\"\n\n### look at meta information\nq &lt;- nomis_overview(ksids[1])\nhead(q)\n\n# A tibble: 6 × 2\n  name           value           \n  &lt;chr&gt;          &lt;list&gt;          \n1 analyses       &lt;named list [1]&gt;\n2 analysisname   &lt;chr [1]&gt;       \n3 analysisnumber &lt;int [1]&gt;       \n4 contact        &lt;named list [4]&gt;\n5 contenttypes   &lt;named list [1]&gt;\n6 coverage       &lt;chr [1]&gt;       \n\na &lt;- nomis_get_metadata(id = ksids[1], concept = \"GEOGRAPHY\", type = \"type\")\na # TYPE297 is MSOA level\n\n# A tibble: 24 × 3\n   id      label.en                                   description.en\n   &lt;chr&gt;   &lt;chr&gt;                                      &lt;chr&gt;         \n 1 TYPE265 NHS area teams                             NHS area teams\n 2 TYPE266 clinical commissioning groups              clinical comm…\n 3 TYPE267 built-up areas including subdivisions      built-up area…\n 4 TYPE269 built-up areas                             built-up areas\n 5 TYPE273 national assembly for wales electoral reg… national asse…\n 6 TYPE274 postcode areas                             postcode areas\n 7 TYPE275 postcode districts                         postcode dist…\n 8 TYPE276 postcode sectors                           postcode sect…\n 9 TYPE277 national assembly for wales constituencie… national asse…\n10 TYPE279 parishes 2011                              parishes 2011 \n# ℹ 14 more rows\n\nb &lt;- nomis_get_metadata(id = ksids[1], concept = \"MEASURES\", type = \"TYPE297\")\nb # 20100 is the measure of absolute numbers\n\n# A tibble: 2 × 3\n  id    label.en description.en\n  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;         \n1 20100 value    value         \n2 20301 percent  percent       \n\n### Query data in loop over the required statistics\nfor(i in ksids){\n\n  # Determin if data is divided by sex or urban-rural\n  nd &lt;- nomis_get_metadata(id = i)\n  if(\"RURAL_URBAN\" %in% nd$conceptref){\n    UR &lt;- TRUE\n  }else{\n    UR &lt;- FALSE\n  }\n  if(\"C_SEX\" %in% nd$conceptref){\n    SEX &lt;- TRUE\n  }else{\n    SEX &lt;- FALSE\n  }\n\n  # make data request\n  if(UR == TRUE){\n    if(SEX == TRUE){\n      tmp_en &lt;- nomis_get_data(id = i, time = \"2011\",\n                               geography = london_ids, # replace with \"TYPE297\" for all MSOAs\n                               measures = 20100, RURAL_URBAN = 0, C_SEX = 0)\n    }else{\n      tmp_en &lt;- nomis_get_data(id = i, time = \"2011\",\n                               geography = london_ids, # replace with \"TYPE297\" for all MSOAs\n                               measures = 20100, RURAL_URBAN = 0)\n    }\n  }else{\n    if(SEX == TRUE){\n      tmp_en &lt;- nomis_get_data(id = i, time = \"2011\",\n                               geography = london_ids, # replace with \"TYPE297\" for all MSOAs\n                               measures = 20100, C_SEX = 0)\n    }else{\n      tmp_en &lt;- nomis_get_data(id = i, time = \"2011\",\n                               geography = london_ids, # replace with \"TYPE297\" for all MSOAs\n                               measures = 20100)\n    }\n\n  }\n\n  # Append (in case of different regions)\n  ks_tmp &lt;- tmp_en\n\n  # Make lower case names\n  names(ks_tmp) &lt;- tolower(names(ks_tmp))\n  names(ks_tmp)[names(ks_tmp) == \"geography_code\"] &lt;- \"msoa11\"\n  names(ks_tmp)[names(ks_tmp) == \"geography_name\"] &lt;- \"name\"\n\n  # replace weird cell codes\n  onlynum &lt;- which(grepl(\"^[[:digit:]]+$\", ks_tmp$cell_code))\n  if(length(onlynum) != 0){\n    code &lt;- substr(ks_tmp$cell_code[-onlynum][1], 1, 7)\n    if(is.na(code)){\n      code &lt;- i\n    }\n    ks_tmp$cell_code[onlynum] &lt;- paste0(code, \"_\", ks_tmp$cell_code[onlynum])\n  }\n\n  # save codebook\n  ks_cb &lt;- unique(ks_tmp[, c(\"date\", \"cell_type\", \"cell\", \"cell_code\", \"cell_name\")])\n\n  ### Reshape\n  ks_res &lt;- tidyr::pivot_wider(ks_tmp, id_cols = c(\"msoa11\", \"name\"),\n                               names_from = \"cell_code\",\n                               values_from = \"obs_value\")\n\n  ### Merge\n  if(i == ksids[1]){\n    census_keystat.df &lt;- ks_res\n    census_keystat_cb.df &lt;- ks_cb\n  }else{\n    census_keystat.df &lt;- merge(census_keystat.df, ks_res, by = c(\"msoa11\", \"name\"), all = TRUE)\n    census_keystat_cb.df &lt;- rbind(census_keystat_cb.df, ks_cb)\n  }\n\n}\n\n\n# Descriptions are saved in the codebook\nhead(census_keystat_cb.df)\n\n# A tibble: 6 × 5\n   date cell_type     cell cell_code   cell_name                    \n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;                        \n1  2011 Ethnic Group     0 KS201EW0001 All usual residents          \n2  2011 Ethnic Group   100 KS201EW_100 White                        \n3  2011 Ethnic Group     1 KS201EW0002 White: English/Welsh/Scottis…\n4  2011 Ethnic Group     2 KS201EW0003 White: Irish                 \n5  2011 Ethnic Group     3 KS201EW0004 White: Gypsy or Irish Travel…\n6  2011 Ethnic Group     4 KS201EW0005 White: Other White           \n\nsave(census_keystat_cb.df, file = \"_data/Census_codebook.RData\")\n\nNow, we have one file containing the geometries of MSOAs and one file with the census information on ethnic groups. Obviously, we can easily merge them together using the MSOA identifiers.\n\nmsoa.spdf &lt;- merge(msoa.spdf, census_keystat.df,\n                   by.x = \"MSOA11CD\", by.y = \"msoa11\", all.x = TRUE)\n\nAnd we can, for instance, plot the spatial distribution of ethnic groups.\n\nmsoa.spdf$per_white &lt;- msoa.spdf$KS201EW_100 / msoa.spdf$KS201EW0001 * 100\n\nmapview(msoa.spdf[, \"per_white\"])\n\n\n\n\n\n\nIf you’re interested in more data sources, see for instance APIs for social scientists: A collaborative review by Paul C. Bauer, Camille Landesvatter, Lion Behrens. It’s a collection of several APIs for social sciences.\n\n\n1.2.3 Gridded data\nSo far, we have queried data on administrative units. However, often data comes on other spatial scales. For instance, we might be interested in the amount of air pollution, which is provided on a regular grid across the UK from Defra.\n\n# Download\npol.link &lt;- \"https://uk-air.defra.gov.uk/datastore/pcm/mapno22011.csv\"\ndownload.file(pol.link, paste0(dn, \"/mapno22011.csv\"))\npol.df &lt;- read.csv(paste0(dn, \"/mapno22011.csv\"), skip = 5, header = T, sep = \",\",\n                      stringsAsFactors = F, na.strings = \"MISSING\")\n\nhead(pol.df)\n\n  ukgridcode      x       y no22011\n1      54291 460500 1221500      NA\n2      54292 461500 1221500      NA\n3      54294 463500 1221500      NA\n4      54979 458500 1220500      NA\n5      54980 459500 1220500      NA\n6      54981 460500 1220500      NA\n\n\nThe data comes as point data with x and y as coordinates. We have to transform this into spatial data first. We first setup a spatial points object with st_as_sf. Subsequently, we transform the point coordinates into a regular grid. We use a buffer method st_buffer with “diameter”, and only one segment per quadrant (nQuadSegs). This gives us a 1x1km regular grid.\n\n# Build spatial object\npol.spdf &lt;- st_as_sf(pol.df, coords = c(\"x\", \"y\"),\n                    crs = 27700)\n\n# we transform the point coordinates into a regular grid with \"diameter\" 500m\npol.spdf &lt;- st_buffer(pol.spdf, dist = 500, nQuadSegs  = 1,\n                      endCapStyle = 'SQUARE')\n\n# Plot NO2\nplot(pol.spdf[, \"no22011\"], border = NA)\n\n\n\n\n\n\n1.2.4 OpenStreetMap (points)\nAnother interesting data source is the OpenStreetMap API, which provides information about the geographical location of a serious of different indicators. Robin Lovelace provides a nice introduction to the osmdata API. Available features can be found on OSM wiki.\nFirst we create a bounding box of where we want to query data. st_bbox() can be used to get bounding boxes of an existing spatial object (needs CRS = 4326). An alternative would be to use opq(bbox = 'greater london uk').\n\n# bounding box of where we want to query data\nq &lt;- opq(bbox = st_bbox(st_transform(msoa.spdf, 4326)))\n\nAnd we want to get data for all pubs and bars which are within this bounding box.\n\n# First build the query of location of pubs in London\nosmq &lt;- add_osm_feature(q, key = \"amenity\", value = \"pub\")\n\n# And then query the data\npubs.osm &lt;- osmdata_sf(osmq)\n\nRight now there are some results in polygons, some in points, and they overlap. Often, data from OSM needs some manual cleaning. Sometimes the same features are represented by different spatial objects (e.g. points + polygons).\n\n# Make unique points / polygons\npubs.osm &lt;- unique_osmdata(pubs.osm)\n\n# Get points and polygons (there are barley any pubs as polygons, so we ignore them)\npubs.points &lt;- pubs.osm$osm_points\npubs.polys &lt;- pubs.osm$osm_multipolygons\n\n# # Drop OSM file\n# rm(pubs.osm); gc()\n\n# Reduce to point object only\npubs.spdf &lt;- pubs.points\n\n# Reduce to a few variables\npubs.spdf &lt;- pubs.spdf[, c(\"osm_id\", \"name\", \"addr:postcode\", \"diet:vegan\")]\n\nAgain, we can inspect the results with mapview.\n\nmapview(st_geometry(pubs.spdf))\n\n\n\n\n\nNote that OSM is solely based on contribution by users, and the quality of OSM data varies. Usually data quality is better in larger cities, and better for more stable features (such as hospitals, train stations, highways) rahter than pubs or restaurants which regularly appear and disappear. However, data from London Datastore would indicate more pubs than what we find with OSM.\n\n\n1.2.5 Save\nWe will store the created data to use them again in the next session.\n\nsave(msoa.spdf, file = \"_data/msoa_spatial.RData\")\nsave(ulez.spdf, file = \"_data/ulez_spatial.RData\")\nsave(pol.spdf, file = \"_data/pollution_spatial.RData\")\nsave(pubs.spdf, file = \"_data/pubs_spatial.RData\")\n\n\n\n\n\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. 1st ed. Chapman & Hall/CRC the R Series. Boca Raton: Chapman & Hall/CRC."
  },
  {
    "objectID": "02_spatial-data.html#manipulation-and-linkage",
    "href": "02_spatial-data.html#manipulation-and-linkage",
    "title": "\n2  Data Manipulation & Visualization\n",
    "section": "\n2.1 Manipulation and linkage",
    "text": "2.1 Manipulation and linkage\nHaving data with geo-spatial information allows to perform a variety of methods to manipulate and link different data sources. Commonly used methods include 1) subsetting, 2) point-in-polygon operations, 3) distance measures, 4) intersections or buffer methods.\nThe online Vignettes of the sf package provide a comprehensive overview of the multiple ways of spatial manipulations.\nCheck if data is on common projection\n\nst_crs(msoa.spdf) == st_crs(pol.spdf)\n\n[1] FALSE\n\nst_crs(msoa.spdf) == st_crs(pubs.spdf)\n\n[1] FALSE\n\nst_crs(msoa.spdf) == st_crs(ulez.spdf)\n\n[1] FALSE\n\n# MSOA in different crs --&gt; transform\npol.spdf &lt;- st_transform(pol.spdf, crs = st_crs(msoa.spdf))\npubs.spdf &lt;- st_transform(pubs.spdf, crs = st_crs(msoa.spdf))\nulez.spdf &lt;- st_transform(ulez.spdf, crs = st_crs(msoa.spdf))\n\n\n# Check if all geometries are valid, and make valid if needed\nmsoa.spdf &lt;- st_make_valid(msoa.spdf)\n\n\n2.1.1 Subsetting\nWe can subset spatial data in a similar way as we subset conventional data.frames or matrices. For instance, below we simply reduce the pollution grid across the UK to observations in London only.\n\n# Subset to pollution estimates in London\npol_sub.spdf &lt;- pol.spdf[msoa.spdf, ] # or:\npol_sub.spdf &lt;- st_filter(pol.spdf, msoa.spdf)\nmapview(pol_sub.spdf)\n\n\n\n\n\n\nOr we can reverse the above and exclude all intersecting units by specifying st_disjoint as alternative spatial operation using the op = option (note the empty space for column selection). st_filter() with the .predicate option does the same job. See the sf Vignette for more operations.\n\n# Subset pubs to pubs not in the ulez area\nsub2.spdf &lt;- pubs.spdf[ulez.spdf, , op = st_disjoint] # or:\nsub2.spdf &lt;- st_filter(pubs.spdf, ulez.spdf, .predicate = st_disjoint)\nmapview(sub2.spdf)\n\n\n\n\n\n\nWe can easily create indicators of whether an MSOA is within ulez or not.\n\nmsoa.spdf$ulez &lt;- 0\n\n# intersecting lsoas\nwithin &lt;- msoa.spdf[ulez.spdf,]\n\n# use their ids to create binary indicator \nmsoa.spdf$ulez[which(msoa.spdf$MSOA11CD %in% within$MSOA11CD)] &lt;- 1\ntable(msoa.spdf$ulez)\n\n\n  0   1 \n954  29 \n\n\n\n2.1.2 Point in polygon\nWe are interested in the number of pubs in each MSOA. So, we count the number of points in each polygon.\n\n# Assign MSOA to each point\npubs_msoa.join &lt;- st_join(pubs.spdf, msoa.spdf, join = st_within)\n\n# Count N by MSOA code (drop geometry to speed up)\npubs_msoa.join &lt;- dplyr::count(st_drop_geometry(pubs_msoa.join),\n                               MSOA11CD = pubs_msoa.join$MSOA11CD,\n                               name = \"pubs_count\")\nsum(pubs_msoa.join$pubs_count)\n\n[1] 1601\n\n# Merge and replace NAs with zero (no matches, no pubs)\nmsoa.spdf &lt;- merge(msoa.spdf, pubs_msoa.join,\n                   by = \"MSOA11CD\", all.x = TRUE)\nmsoa.spdf$pubs_count[is.na(msoa.spdf$pubs_count)] &lt;- 0\n\n\n2.1.3 Distance measures\nWe might be interested in the distance to the nearest pub. Here, we use the package nngeo to find k nearest neighbours with the respective distance.\n\n# Use geometric centroid of each MSOA\ncent.sp &lt;- st_centroid(msoa.spdf[, \"MSOA11CD\"])\n\nWarning: st_centroid assumes attributes are constant over\ngeometries\n\n# Get K nearest neighbour with distance\nknb.dist &lt;- st_nn(cent.sp, \n                  pubs.spdf,\n                  k = 1,             # number of nearest neighbours\n                  returnDist = TRUE, # we also want the distance\n                  progress = FALSE)\n\nprojected points\n\nmsoa.spdf$dist_pubs &lt;- unlist(knb.dist$dist)\nsummary(msoa.spdf$dist_pubs)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   9.079  305.149  565.018  701.961  948.047 3735.478 \n\n\n\n2.1.4 Intersections + Buffers\nWe may also want the average pollution within 1 km radius around each MSOA centroid. Note that it is usually better to use a ego-centric method where you calculate the average within a distance rather than using the characteristic of the intersecting cells only (Lee et al. 2008; Mohai and Saha 2007).\nTherefore, we first create a buffer with st_buffer() around each midpoint and subsequently use st_intersetion() to calculate the overlap.\n\n# Create buffer (1km radius)\ncent.buf &lt;- st_buffer(cent.sp, \n                      dist = 1000) # dist in meters\nmapview(cent.buf)\n\n\n\n\n\n# Add area of each buffer (in this constant) \ncent.buf$area &lt;- as.numeric(st_area(cent.buf))\n\n# Calculate intersection of pollution grid and buffer\nint.df &lt;- st_intersection(cent.buf, pol.spdf)\n\nWarning: attribute variables are assumed to be spatially constant\nthroughout all geometries\n\nint.df$int_area &lt;- as.numeric(st_area(int.df)) # area of intersection\n\n# Area of intersection as share of buffer\nint.df$area_per &lt;- int.df$int_area / int.df$area\n\nAnd we use the percent overalp areas as the weights to calculate a weighted mean.\n\n# Aggregate as weighted mean\nint.df &lt;- st_drop_geometry(int.df)\nint.df$no2_weighted &lt;- int.df$no22011 * int.df$area_per\nint.df &lt;- aggregate(list(no2 = int.df[, \"no2_weighted\"]), \n                    by = list(MSOA11CD = int.df$MSOA11CD),\n                    sum)\n\n# Merge back to spatial data.frame\nmsoa.spdf &lt;- merge(msoa.spdf, int.df, by = \"MSOA11CD\", all.x = TRUE)\n\nmapview(msoa.spdf[, \"no2\"])\n\n\n\n\n\n\nNote: for buffer related methods, it often makes sense to use population weighted centroids instead of geographic centroids (see here for MSOA population weighted centroids). However, often this information is not available.\n\n2.1.5 Air pollution and ethnic minorities\nWith a few lines of code, we have compiled an original dataset containing demographic information, air pollution, and some infrastructural information.\nLet’s see what we can do with it.\n\n# Define ethnic group shares\nmsoa.spdf$per_mixed &lt;- msoa.spdf$KS201EW_200 / msoa.spdf$KS201EW0001 * 100\nmsoa.spdf$per_asian &lt;- msoa.spdf$KS201EW_300 / msoa.spdf$KS201EW0001 * 100\nmsoa.spdf$per_black &lt;- msoa.spdf$KS201EW_400 / msoa.spdf$KS201EW0001 * 100\nmsoa.spdf$per_other &lt;- msoa.spdf$KS201EW_500 / msoa.spdf$KS201EW0001 * 100\n\n# Define tenure\nmsoa.spdf$per_owner &lt;- msoa.spdf$KS402EW_100 / msoa.spdf$KS402EW0001 * 100\nmsoa.spdf$per_social &lt;- msoa.spdf$KS402EW_200 / msoa.spdf$KS402EW0001 * 100\n\n# Non British passport\nmsoa.spdf$per_nonUK &lt;- (msoa.spdf$KS205EW0001 - msoa.spdf$KS205EW0003)/ msoa.spdf$KS205EW0001 * 100\nmsoa.spdf$per_nonEU &lt;- (msoa.spdf$KS205EW0001 - msoa.spdf$KS205EW0003 -\n                          msoa.spdf$KS205EW0004 - msoa.spdf$KS205EW0005  - \n                          msoa.spdf$KS205EW0006)/ msoa.spdf$KS205EW0001 * 100\nmsoa.spdf$per_nonUK_EU &lt;- (msoa.spdf$KS205EW0005  + msoa.spdf$KS205EW0006)/ msoa.spdf$KS205EW0001 * 100\n\n\n# Run regression\nmod1.lm &lt;- lm(no2 ~ per_mixed + per_asian + per_black + per_other +\n                per_owner + per_social + pubs_count + POPDEN + ulez,\n              data = msoa.spdf)\n\n# summary\nscreenreg(list(mod1.lm), digits = 3)\n\n\n========================\n             Model 1    \n------------------------\n(Intercept)   36.899 ***\n              (1.311)   \nper_mixed     -0.078    \n              (0.099)   \nper_asian      0.017 *  \n              (0.007)   \nper_black     -0.085 ***\n              (0.016)   \nper_other      0.469 ***\n              (0.047)   \nper_owner     -0.205 ***\n              (0.013)   \nper_social    -0.056 ***\n              (0.013)   \npubs_count     0.220 ***\n              (0.040)   \nPOPDEN         0.036 ***\n              (0.003)   \nulez           9.358 ***\n              (0.678)   \n------------------------\nR^2            0.773    \nAdj. R^2       0.771    \nNum. obs.    983        \n========================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nFor some examples later, we also add data on house prices. We use the median house prices in 2017 from the London Datastore.\n\n# Download\nhp.link &lt;- \"https://data.london.gov.uk/download/average-house-prices/bdf8eee7-41e1-4d24-90ce-93fe5cf040ae/land-registry-house-prices-MSOA.csv\"\nhp.df &lt;- read.csv(hp.link)\nhp.df &lt;- hp.df[which(hp.df$Measure == \"Median\" &\n                       grepl(\"2011\", hp.df$Year)), ]\ntable(hp.df$Year)\n\n\nYear ending Dec 2011 Year ending Jun 2011 Year ending Mar 2011 \n                 983                  983                  983 \nYear ending Sep 2011 \n                 983 \n\n# Aggregate across 2011 values\nhp.df$med_house_price &lt;- as.numeric(hp.df$Value)\nhp.df &lt;- aggregate(hp.df[, \"med_house_price\", drop = FALSE],\n                   by = list(MSOA11CD = hp.df$Code),\n                   FUN = function(x) mean(x, na.rm = TRUE))\n\n# Merge spdf and housing prices\nmsoa.spdf &lt;- merge(msoa.spdf, hp.df,\n                   by = \"MSOA11CD\",\n                   all.x = TRUE, all.y = FALSE)\nhist(log(msoa.spdf$med_house_price))\n\n\n\n\n\n2.1.6 Save spatial data\n\n# Save\nsave(msoa.spdf, file = \"_data/msoa2_spatial.RData\")"
  },
  {
    "objectID": "02_spatial-data.html#visualization",
    "href": "02_spatial-data.html#visualization",
    "title": "\n2  Data Manipulation & Visualization\n",
    "section": "\n2.2 Visualization",
    "text": "2.2 Visualization\nA large advantage of spatial data is that different data sources can be connected and combined. Another nice advantage is: you can create very nice maps. And it’s quite easy to do! Stefan Jünger & Anne-Kathrin Stroppe provide more comprehensive materials on mapping in their GESIS workshop on geospatial techniques in R.\nMany packages and functions can be used to plot maps of spatial data. For instance, ggplot as a function to plot spatial data using geom_sf(). I am personally a fan of tmap, which makes many steps easier (but sometimes is less flexible).\nA great tool for choosing coulour is for instance Colorbrewer. viridisLite provides another great resource to chose colours.\n\n2.2.1 Tmaps\nFor instance, lets plot the NO2 estimates using tmap + tm_fill() (there are lots of alternatives like tm_shape, tm_points(), tm_dots()).\n\n# Define colours\ncols &lt;- viridis(n = 7, direction = 1, option = \"C\")\n\nmp1 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = \"no2\", \n          style = \"fisher\", # algorithm to def cut points\n          n = 7, # Number of requested cut points\n          palette = cols, # colours\n          alpha = 1, # transparency \n          title = \"NO2\", \n          legend.hist = FALSE # histogram next to map?\n          ) +\n  tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) \n\nmp1\n\n\n\n\nTmap allows to easily combine different objects by defining a new object via tm_shape().\n\n# Define colours\ncols &lt;- viridis(n = 7, direction = 1, option = \"C\")\n\nmp1 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = \"no2\", \n          style = \"fisher\", # algorithm to def cut points\n          n = 7, # Number of requested cut points\n          palette = cols, # colours\n          alpha = 1, # transparency \n          title = \"NO2\", \n          legend.hist = FALSE # histogram next to map?\n          ) +\n  tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(ulez.spdf) +\n  tm_borders(col = \"red\", lwd = 1, alpha = 1) \n\nmp1\n\n\n\n\nAnd it is easy to change the layout.\n\n# Define colours\ncols &lt;- viridis(n = 7, direction = 1, option = \"C\")\n\nmp1 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = \"no2\", \n          style = \"fisher\", # algorithm to def cut points\n          n = 7, # Number of requested cut points\n          palette = cols, # colours\n          alpha = 1, # transparency \n          title = expression('in'~mu*'g'/m^{3}), \n          legend.hist = FALSE # histogram next to map?\n          ) +\n  tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(ulez.spdf) +\n  tm_borders(col = \"red\", lwd = 1, alpha = 1) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"NO2\", \n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8)\n\nmp1\n\n\n\n\nWe can also add some map information from OSM. However, it’s sometimes a bit tricky with the projection. That’s why we switch into the OSM projection here. Note that this osm query is build on retiring packages.\n\n# Save old projection\ncrs_orig &lt;- st_crs(msoa.spdf)\n\n# Change projection\nulez.spdf &lt;- st_transform(ulez.spdf, 4326)\nmsoa.spdf &lt;- st_transform(msoa.spdf, 4326)\n\n# Get OSM data for background\nosm_tmp &lt;- read_osm(st_bbox(msoa.spdf), ext = 1, \n                    type = \"stamen-toner\", zoom = NULL) \n\nPlease note that rgdal will be retired during October 2023,\nplan transition to sf/stars/terra functions using GDAL and PROJ\nat your earliest convenience.\nSee https://r-spatial.org/r/2023/05/15/evolution4.html and https://github.com/r-spatial/evolution\nrgdal: version: 1.6-7, (SVN revision 1203)\nGeospatial Data Abstraction Library extensions to R successfully loaded\nLoaded GDAL runtime: GDAL 3.6.2, released 2023/01/02\nPath to GDAL shared files: C:/Users/qtnztru/AppData/Local/R/win-library/4.3/rgdal/gdal\n GDAL does not use iconv for recoding strings.\nGDAL binary built with GEOS: TRUE \nLoaded PROJ runtime: Rel. 9.2.0, March 1st, 2023, [PJ_VERSION: 920]\nPath to PROJ shared files: C:/Users/qtnztru/AppData/Local/R/win-library/4.3/rgdal/proj\nPROJ CDN enabled: FALSE\nLinking to sp version:1.6-1\nTo mute warnings of possible GDAL/OSR exportToProj4() degradation,\nuse options(\"rgdal_show_exportToProj4_warnings\"=\"none\") before loading sp or rgdal.\n\n# Define colours\ncols &lt;- viridis(n = 7, direction = 1, option = \"C\")\n\nmp1 &lt;-  tm_shape(osm_tmp) + tm_rgb() +\n  tm_shape(msoa.spdf) + \n  tm_fill(col = \"no2\", \n          style = \"fisher\", # algorithm to def cut points\n          n = 7, # Number of requested cut points\n          palette = cols, # colours\n          alpha = 0.8, # transparency \n          title = expression('in'~mu*'g'/m^{3}), \n          legend.hist = FALSE # histogram next to map?\n          ) +\n  #tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(ulez.spdf) +\n  tm_borders(col = \"red\", lwd = 1, alpha = 1) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"NO2\", \n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8)\n\nmp1\n\n\n\n\nTmap also makes it easy to combine single maps\n\n# Define colours\ncols1 &lt;- viridis(n = 7, direction = 1, option = \"C\")\n\n# Define colours\ncols2 &lt;- viridis(n = 7, direction = 1, option = \"D\")\n\nmp1 &lt;-  tm_shape(osm_tmp) + tm_rgb() +\n  tm_shape(msoa.spdf) + \n  tm_fill(col = \"no2\", \n          style = \"fisher\", # algorithm to def cut points\n          n = 7, # Number of requested cut points\n          palette = cols1, # colours\n          alpha = 0.8, # transparency \n          title = expression('in'~mu*'g'/m^{3}), \n          legend.hist = FALSE # histogram next to map?\n          ) +\n  #tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(ulez.spdf) +\n  tm_borders(col = \"red\", lwd = 1, alpha = 1) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"NO2\", \n            main.title.position = \"center\",\n            main.title.size = 1.4,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8)\n\nmp2 &lt;-  tm_shape(osm_tmp) + tm_rgb() +\n  tm_shape(msoa.spdf) + \n  tm_fill(col = \"per_black\", \n          style = \"fisher\", # algorithm to def cut points\n          n = 7, # Number of requested cut points\n          palette = cols2, # colours\n          alpha = 0.8, # transparency \n          title = \"% white\", \n          legend.hist = FALSE # histogram next to map?\n          ) +\n  #tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(ulez.spdf) +\n  tm_borders(col = \"red\", lwd = 1, alpha = 1) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"Ethnic Black inhabitants\", \n            main.title.position = \"center\",\n            main.title.size = 1.4,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8)\n\ntmap_arrange(mp1, mp2, ncol = 2, nrow = 1)\n\n\n\n\nAnd you can easily export those to png or pdf\n\npng(file = paste(\"London.png\", sep = \"\"), width = 14, height = 7, units = \"in\", \n    res = 100, bg = \"white\")\npar(mar=c(0,0,3,0))\npar(mfrow=c(1,1),oma=c(0,0,0,0))\ntmap_arrange(mp1, mp2, ncol = 2, nrow = 1)\ndev.off()\n\npng \n  2 \n\n\n\n2.2.2 ggplot\n\ngp &lt;- ggplot(msoa.spdf)+\n    geom_sf(aes(fill = no2))+\n    scale_fill_viridis_c(option = \"B\")+\n    coord_sf(datum = NA)+\n    theme_map()+\n    theme(legend.position = c(.9, .6))\ngp\n\n\n\n\n\n# Get some larger scale boundaries\nborough.spdf &lt;- st_read(dsn = paste0(\"_data\", \"/statistical-gis-boundaries-london/ESRI\"),\n                     layer = \"London_Borough_Excluding_MHW\" # Note: no file ending\n                     )\n\nReading layer `London_Borough_Excluding_MHW' from data source \n  `C:\\work\\Lehre\\Geodata_Spatial_Regression\\_data\\statistical-gis-boundaries-london\\ESRI' \n  using driver `ESRI Shapefile'\nSimple feature collection with 33 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n# transform to only inner lines\nborough_inner &lt;- ms_innerlines(borough.spdf)\n\n# Plot with inner lines\ngp &lt;- ggplot(msoa.spdf)+\n    geom_sf(aes(fill = no2), color = NA)+\n    scale_fill_viridis_c(option = \"A\")+\n    geom_sf(data = borough_inner, color = \"gray92\")+\n    geom_sf(data = ulez.spdf, color = \"red\", fill = NA)+\n    coord_sf(datum = NA)+\n    theme_map()+\n    labs(fill = \"NO2\")+\n    theme(legend.position = c(.9, .6))\ngp"
  },
  {
    "objectID": "02_spatial-data.html#exercise",
    "href": "02_spatial-data.html#exercise",
    "title": "\n2  Data Manipulation & Visualization\n",
    "section": "\n2.3 Exercise",
    "text": "2.3 Exercise\n\nPlease calculate the distance of each MSOA to the London city center\n\n\nuse google maps to get lon and lat,\nus st_as_sf() to create the spatial point\nuse st_distance() to calculate the distance\n\n\n### Distance to city center\n# Define centre\ncentre &lt;- st_as_sf(data.frame(lon = -0.128120855701165, \n                              lat = 51.50725909644806),\n                   coords = c(\"lon\", \"lat\"), \n                   crs = 4326)\n# Reproject\ncentre &lt;- st_transform(centre, crs = st_crs(msoa.spdf))\n# Calculate distance\nmsoa.spdf$dist_centre &lt;- as.numeric(st_distance(msoa.spdf, centre)) / 1000\n# hist(msoa.spdf$dist_centre)\n\n\nCan you create a plot with the distance to the city centre and pub counts next to each otter?\n\n\n# Define colours\ncols &lt;- viridis(n = 10, direction = 1, option = \"B\")\ncols2 &lt;- viridis(n = 10, direction = 1, option = \"E\")\n\n\nmp1 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = \"dist_centre\", \n          style = \"fisher\", # algorithm to def cut points\n          n = 10, # Number of requested cut points\n          palette = cols, # colours\n          alpha = 1, # transparency \n          title = \"Distance\", \n          legend.hist = FALSE # histogram next to map?\n          ) +\n  tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"Dist centre\", \n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8)\n\n\nmp2 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = \"dist_centre\", \n          style = \"quantile\", # algorithm to def cut points\n          n = 10, # Number of requested cut points\n          palette = cols, # colours\n          alpha = 1, # transparency \n          title = \"Distance\", \n          legend.hist = FALSE # histogram next to map?\n          ) +\n  tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"Dist centre\", \n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8)\n\n\nmp3 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = \"pubs_count\", \n          style = \"fisher\", # algorithm to def cut points\n          n = 10, # Number of requested cut points\n          palette = cols, # colours\n          alpha = 1, # transparency \n          title = \"Count\", \n          legend.hist = FALSE # histogram next to map?\n          ) +\n  tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"Pubs\", \n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8)\n\n\nmp4 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = \"pubs_count\", \n          style = \"quantile\", # algorithm to def cut points\n          n = 10, # Number of requested cut points\n          palette = cols, # colours\n          alpha = 1, # transparency \n          title = \"Count\", \n          legend.hist = FALSE # histogram next to map?\n          ) +\n  tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"Pubs\", \n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8)\n\n\ntmap_arrange(mp1, mp2, mp3, mp4, ncol = 2, nrow = 2)\n\n\n\n\n\n2.3.1 Geogrphic cter\n\n# Make one single schape\nlondon &lt;- st_union(msoa.spdf)\n\n# Calculate center\ncent &lt;- st_centroid(london)\n\nmapview(cent)\n\n\n\n\n\n\n\n\n\n\n\nLee, Barrett A., Sean F. Reardon, Glenn Firebaugh, Chad R. Farrell, Stephen A. Matthews, and David O’Sullivan. 2008. “Beyond the Census Tract: Patterns and Determinants of Racial Segregation at Multiple Geographic Scales.” American Sociological Review 73 (5): 766–91. https://doi.org/10.1177/000312240807300504.\n\n\nMohai, Paul, and Robin Saha. 2007. “Racial Inequality in the Distribution of Hazardous Waste: A National-Level Reassessment.” Social Problems 54 (3): 343–70. https://doi.org/10.1525/sp.2007.54.3.343."
  },
  {
    "objectID": "03_weights.html#spatial-interdependence",
    "href": "03_weights.html#spatial-interdependence",
    "title": "3  Spatial Relationships W",
    "section": "3.1 Spatial interdependence",
    "text": "3.1 Spatial interdependence\nWe can not only use coordinates and geo-spatial information to connect different data sources, we can also explicitly model spatial (inter)dependence in the analysis of our data. In many instance, accounting for spatial dependence might even be necessary to avoid biased point estimates and standard errors, as observations are often not independent and identically distributed.\nTobler’s first law of geography has been used extensively (11,584 citation in 2023-06) to describe spatial dependence: ‘Everything is related to everything else, but near things are more related than distant things’ (Tobler 1970).\n\n\n\n\n\n\nNote\n\n\n\nTobler’s first law is a bit of story\nAnd it has been labeled as an excuse to not think too much about the reasons for spatial dependence or auto-correlation. For instance, measurement error, omitted variables, or inappropriate levels of aggregation are among reasons for auto-correlation (Pebesma and Bivand 2023).\n\n\nWe will come back to the reasons of spatial dependence. However, for now, we are interested in some tools to detect and analyse spatial relations.\nTo analyse spatial relations, we first need to define some sort of connectivity between units (e.g. similar to network analysis). There are some obvious candidates that be used to define these relations here: adjacency and proximity."
  },
  {
    "objectID": "03_weights.html#normalization-of-boldsymbolmathbfw",
    "href": "03_weights.html#normalization-of-boldsymbolmathbfw",
    "title": "3  Spatial Relationships W",
    "section": "3.3 Normalization of \\({\\boldsymbol{\\mathbf{W}}}\\)",
    "text": "3.3 Normalization of \\({\\boldsymbol{\\mathbf{W}}}\\)\nNormalizing ensures that the parameter space of the spatial multiplier is restricted to \\(-1 &lt; \\rho &gt; 1\\), and the multiplier matrix is non-singular (don’t worry, more on this later).\nThe main message: Normalizing your weights matrix is always a good idea. Otherwise, the spatial parameters might blow up – if you can estimate the model at all. It also ensure easy interpretation of spillover effects.\nAgain, how to normalize a weights matrix is subject of debate (LeSage and Pace 2014; Neumayer and Plümper 2016).\n\n3.3.1 Row-normalization\nRow-normalization divides each non-zero weight by the sum of all weights of unit \\(i\\), which is the sum of the row.\n\\[\n\\frac{w_{ij}}{\\sum_j^n w_{ij}}\n\\]\n\nWith contiguity weights, spatially lagged variables contain mean of this variable among the neighbours of \\(i\\)\nProportions between units such as distances get lost (can be bad!)\nCan induce asymmetries: \\(w_{ij} \\neq w_{ji}\\)\n\nFor instance, we can use row-normalization for the Queens neighbours created above, and create a neighbours list with spatial weights.\n\nqueens.lw &lt;- nb2listw(queens.nb,\n                      style = \"W\") # W ist row-normalization\nsummary(queens.lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 983 \nNumber of nonzero links: 5648 \nPercentage nonzero weights: 0.5845042 \nAverage number of links: 5.745677 \nLink number distribution:\n\n  2   3   4   5   6   7   8   9  10  11  12  13 \n  9  39 130 264 273 169  66  19   5   6   2   1 \n9 least connected regions:\n160 270 475 490 597 729 755 778 861 with 2 links\n1 most connected region:\n946 with 13 links\n\nWeights style: W \nWeights constants summary:\n    n     nn  S0       S1      S2\nW 983 966289 983 355.1333 4017.47\n\n\nTo see what happened, let’s look at our example in matrix format again.\n\n# transform into matrix with row-normalization\nW_norm &lt;- nb2mat(queens.nb, style = \"W\")\nprint(W_norm[1:10, 1:10])\n\n   [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n1     0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n2     0 0.0000000 0.1666667 0.0000000 0.0000000 0.0000000 0.0000000\n3     0 0.1428571 0.0000000 0.0000000 0.1428571 0.0000000 0.0000000\n4     0 0.0000000 0.0000000 0.0000000 0.0000000 0.2000000 0.0000000\n5     0 0.0000000 0.2000000 0.0000000 0.0000000 0.2000000 0.2000000\n6     0 0.0000000 0.0000000 0.1666667 0.1666667 0.0000000 0.1666667\n7     0 0.0000000 0.0000000 0.0000000 0.1666667 0.1666667 0.0000000\n8     0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.1666667\n9     0 0.0000000 0.0000000 0.0000000 0.0000000 0.1666667 0.1666667\n10    0 0.0000000 0.0000000 0.2000000 0.0000000 0.2000000 0.0000000\n        [,8]      [,9]     [,10]\n1  0.0000000 0.0000000 0.0000000\n2  0.0000000 0.0000000 0.0000000\n3  0.0000000 0.0000000 0.0000000\n4  0.0000000 0.0000000 0.2000000\n5  0.0000000 0.0000000 0.0000000\n6  0.0000000 0.1666667 0.1666667\n7  0.1666667 0.1666667 0.0000000\n8  0.0000000 0.0000000 0.0000000\n9  0.0000000 0.0000000 0.1666667\n10 0.0000000 0.2000000 0.0000000\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nOverall, how many neighbours does unit 9 have (including all columns)? How do you know?\n\n\n\nrowSums(W)[9]\n\nWe can also use the nb object to see which ones the neighbours are. Here, for instance, neighbours of unit 6:\n\nqueens.nb[6]\n\n[[1]]\n[1]   4   5   7   9  10 462\n\n\nThis fits to what we see in the matrix above.\n\n\n\n\n\n\nWarning\n\n\n\nNote that row-normalization has some undesirable properties when we use some non-contigutiy based neighbour relations, such as distance based neighbours.\nThe problem: It obscures the proportion due to dividing by a row-specific value.\n\n\nLet’s construct a hypothetical example\n\n# Subset of 5 units\nsub.spdf &lt;- msoa.spdf[c(4, 5, 6, 102, 150), ]\nmapview(sub.spdf)\n\n\n\n\n\n\nWe construct the inverse-distance weighted 2 nearest neighbors.\n\n# 2 closest neighbours\nsub.coords &lt;- st_geometry(st_centroid(sub.spdf))\n\nWarning: st_centroid assumes attributes are constant over\ngeometries\n\nknn.nb &lt;- knearneigh(sub.coords, \n                     k = 2) # number of nearest neighbours\n\nWarning in knearneigh(sub.coords, k = 2): k greater than one-third\nof the number of data points\n\nknn.nb &lt;- knn2nb(knn.nb)\nsummary(knn.nb)\n\nNeighbour list object:\nNumber of regions: 5 \nNumber of nonzero links: 10 \nPercentage nonzero weights: 40 \nAverage number of links: 2 \nNon-symmetric neighbours list\nLink number distribution:\n\n2 \n5 \n5 least connected regions:\n1 2 3 4 5 with 2 links\n5 most connected regions:\n1 2 3 4 5 with 2 links\n\n# listw with inverse-distance based weights\nsub.lw &lt;- nb2listwdist(knn.nb,\n                       x = sub.coords, # needed for idw\n                       type = \"idw\", # inverse distance weighting\n                       alpha = 1, # the decay parameter for distance weighting\n                       style = \"raw\") # without normalization\nW_sub &lt;- listw2mat(sub.lw)\nformatC(W_sub, format = \"f\", digits = 6)\n\n  [,1]       [,2]       [,3]       [,4]       [,5]      \n1 \"0.000000\" \"0.000414\" \"0.000723\" \"0.000000\" \"0.000000\"\n2 \"0.000414\" \"0.000000\" \"0.000962\" \"0.000000\" \"0.000000\"\n3 \"0.000723\" \"0.000962\" \"0.000000\" \"0.000000\" \"0.000000\"\n4 \"0.000000\" \"0.000033\" \"0.000032\" \"0.000000\" \"0.000000\"\n5 \"0.000049\" \"0.000000\" \"0.000049\" \"0.000000\" \"0.000000\"\n\n\nAs you can see, units 1, 2, 3 have relatively proximate neighbours (.e.g inverse distance 0.000962: 3 zeros). Units 4 and 5, in contrast, have only very distant neighbours (e.g. inverse distance 0.000049: 4 zeros).\nNow, see what happens when we use row-normalization.\n\nsub.lw &lt;- nb2listwdist(knn.nb,\n                       x = sub.coords, # needed for idw\n                       type = \"idw\", # inverse distance weighting\n                       alpha = 1, # the decay parameter for distance weighting\n                       style = \"W\") # for row normalization\nW_sub &lt;- listw2mat(sub.lw)\nformatC(W_sub, format = \"f\", digits = 6)\n\n  [,1]       [,2]       [,3]       [,4]       [,5]      \n1 \"0.000000\" \"0.364083\" \"0.635917\" \"0.000000\" \"0.000000\"\n2 \"0.300879\" \"0.000000\" \"0.699121\" \"0.000000\" \"0.000000\"\n3 \"0.429123\" \"0.570877\" \"0.000000\" \"0.000000\" \"0.000000\"\n4 \"0.000000\" \"0.507955\" \"0.492045\" \"0.000000\" \"0.000000\"\n5 \"0.499360\" \"0.000000\" \"0.500640\" \"0.000000\" \"0.000000\"\n\n\nAll rows sum up to 1, but the strength of the relation is now similar for the distant units 4 and 5, and the proximate units 1, 2, 3.\n\n\n3.3.2 Maximum eigenvalues normalization\nMaximum eigenvalues normalization divides each non-zero weight by the overall maximum eigenvalue \\(\\lambda_{max}\\). Each element of \\(\\boldsymbol{\\mathbf{W}}\\) is divided by the same scalar parameter, which preserves the relations.\n\\[\n\\frac{\\boldsymbol{\\mathbf{W}}}{\\lambda_{max}}\n\\]\n\nInterpretation may become more complicated\nKeeps proportions of connectivity strengths across units (relevant esp. for distance based \\(\\boldsymbol{\\mathbf{W}}\\))\n\nWe use eigenvalue normalization for the inverse distance neighbours. We use nb2listwdist() to create weight inverse distance based weights and normalize in one step.\n\ncoords &lt;- st_geometry(st_centroid(msoa.spdf))\n\nWarning: st_centroid assumes attributes are constant over\ngeometries\n\nidw.lw &lt;- nb2listwdist(dist_3.nb,\n                       x = coords, # needed for idw\n                       type = \"idw\", # inverse distance weighting\n                       alpha = 1, # the decay parameter for distance weighting\n                       style = \"minmax\") # for eigenvalue normalization\nsummary(idw.lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 983 \nNumber of nonzero links: 22086 \nPercentage nonzero weights: 2.285652 \nAverage number of links: 22.46796 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 \n 4  3  7 13 11 14 14 17 26 22 26 30 33 34 46 34 59 43 38 30 25 19 \n23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 \n22 15 21 14 23 17 17 23 28 19 26 24 29 24 27 25 22 18  8 10 12  5 \n45 46 47 \n 3  2  1 \n4 least connected regions:\n158 160 463 959 with 1 link\n1 most connected region:\n545 with 47 links\n\nWeights style: minmax \nWeights constants summary:\n         n     nn       S0       S1       S2\nminmax 983 966289 463.6269 23.92505 1117.636\n\n\nExamples from above: See how this keeps the proportions in our example. Instead of transforming values to sum up to 1 in each row, we now have much smaller values for 4 and 5 then we have for the proximate units 1, 2, 3.\n\nsub.lw &lt;- nb2listwdist(knn.nb,\n                       x = sub.coords, # needed for idw\n                       type = \"idw\", # inverse distance weighting\n                       alpha = 1, # the decay parameter for distance weighting\n                       style = \"minmax\") # for eigenvalue normalization\nW_sub &lt;- listw2mat(sub.lw)\nformatC(W_sub, format = \"f\", digits = 6)\n\n  [,1]       [,2]       [,3]       [,4]       [,5]      \n1 \"0.000000\" \"0.245687\" \"0.429123\" \"0.000000\" \"0.000000\"\n2 \"0.245687\" \"0.000000\" \"0.570877\" \"0.000000\" \"0.000000\"\n3 \"0.429123\" \"0.570877\" \"0.000000\" \"0.000000\" \"0.000000\"\n4 \"0.000000\" \"0.019663\" \"0.019047\" \"0.000000\" \"0.000000\"\n5 \"0.029099\" \"0.000000\" \"0.029174\" \"0.000000\" \"0.000000\""
  },
  {
    "objectID": "04_dependence.html#global-autocorrelation",
    "href": "04_dependence.html#global-autocorrelation",
    "title": "4  Detecting Spatial Dependence",
    "section": "4.1 Global Autocorrelation",
    "text": "4.1 Global Autocorrelation\nIf spatially close observations are more likely to exhibit similar values, we cannot handle observations as if they were independent.\n\\[\n\\mathrm{E}(\\varepsilon_i\\varepsilon_j)\\neq \\mathrm{E}(\\varepsilon_i)\\mathrm{E}(\\varepsilon_j) = 0\n\\]\nThis violates a basic assumption of the conventional OLS model. We will talk more about whether that is good or bad (any guess?).\n\n4.1.1 Visualization\nThere is one very easy and intuitive way of detecting spatial autocorrelation: Just look at the map. We do so by using tmap for plotting the share of home owners.\n\nmp1 &lt;- tm_shape(msoa.spdf) +\n  tm_fill(col = \"per_owner\", \n          #style = \"cont\",\n          style = \"fisher\", n = 8,\n          title = \"Median\", \n          palette = viridis(n = 8, direction = -1, option = \"C\"),\n          legend.hist = TRUE) +\n  tm_borders(col = \"black\", lwd = 1) +\n  tm_layout(legend.frame = TRUE, legend.bg.color = TRUE,\n            #legend.position = c(\"right\", \"bottom\"),\n            legend.outside = TRUE,\n            main.title = \"Percent home owners\", \n            main.title.position = \"center\",\n            title.snap.to.legend = TRUE) \n\nmp1 \n\n\n\n\nWe definitely see some clusters with spatial units having a low share of home owner (e.g. in the city center), and other clusters where home ownership is high (e.g. suburbs in the south and east, such as Bromley or Havering).\nHowever, this is (to some degree) dependent on how we define cutoffs and coloring of the map: the Modifiable Areal Unit Problem (Wong 2009).\n\n\n\n\n\n\nQuestion\n\n\n\nWhich of the following three checkerboards has no (or the lowest) autocorrelation?\n\n\n\nWould your answer be the same if we would aggregate the data to four larger areas / districts using the average within each of the four districts?\n\n\n4.1.2 Moran’s I\nThe most common and well known statistic for spatial dependence or autocorrelation is Moran’s I, which goes back to Moran (1950) and Cliff and Ord (1972). For more extensive materials on Moran’s I see for instance Kelejian and Piras (2017), Chapter 11.\nTo calculate Moran’s I, we first define a neighbours weights matrix W.\nGlobal Moran’s I test statistic: \\[      \n        \\begin{equation}\n        \\boldsymbol{\\mathbf{I}}  = \\frac{N}{S_0}  \n        \\frac{\\sum_i\\sum_j w_{ij}(y_i-\\bar{y})(y_j-\\bar{y})}\n            {\\sum_i (y_i-\\bar{y})^2}, \\text{where } S_0 = \\sum_{i=1}^N\\sum_{j=1}^N w_{ij}\n        \\end{equation}\n\\] It is often written with deviations \\(z\\)\n\\[      \n        \\begin{equation}\n        \\boldsymbol{\\mathbf{I}}  = \\frac{N}{S_0}  \n        \\frac{\\sum_i\\sum_j w_{ij}(z_i)(z_j)}\n            {\\sum_i (z_i)^2}, \\text{where } S_0 = \\sum_{i=1}^N\\sum_{j=1}^N w_{ij}\n        \\end{equation}\n\\]\nNote that in the case of row-standardized weights, \\(S_0 = N\\). The \\(I\\) can be interpreted as: Relation of the deviation from the mean value between unit \\(i\\) and neighbours of unit \\(i\\). Basically, this measures correlation between neighbouring values.\n\nNegative values: negative autocorrelation\nAround zero: no autocorrelation\nPositive values: positive autocorrelation\n\nTo calculate Moran’s I, we first need to define the relationship between units. As in the previous example, we define contiguity weights and distance-based weights.\n\n# Contiguity (Queens) neighbours weights\nqueens.nb &lt;- poly2nb(msoa.spdf, \n                     queen = TRUE, \n                     snap = 1) # we consider points in 1m distance as 'touching'\nqueens.lw &lt;- nb2listw(queens.nb,\n                      style = \"W\")\n\n# Neighbours within 3km distance\ncoords &lt;- st_geometry(st_centroid(msoa.spdf))\n\nWarning: st_centroid assumes attributes are constant over\ngeometries\n\ndist_3.nb &lt;- dnearneigh(coords, \n                        d1 = 0, d2 = 3000)\nidw.lw &lt;- nb2listwdist(dist_3.nb,\n                       x = coords, # needed for idw\n                       type = \"idw\", # inverse distance weighting\n                       alpha = 1, # the decay parameter for distance weighting\n                       style = \"minmax\") # for eigenvalue normalization\n\nSubsequently, we can calculate the average correlation between neighbouring units.\nFor contiguity weights, we get:\n\n# Global Morans I test of housing values based on contiguity weights\nmoran.test(msoa.spdf$per_owner, listw = queens.lw, alternative = \"two.sided\")\n\n\n    Moran I test under randomisation\n\ndata:  msoa.spdf$per_owner  \nweights: queens.lw    \n\nMoran I statistic standard deviate = 38.161, p-value &lt;\n2.2e-16\nalternative hypothesis: two.sided\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.728706855      -0.001018330       0.000365663 \n\n\nAnd for inverse distance weighting, we get:\n\n# Global Morans I test of housing values based on idw\nmoran.test(msoa.spdf$per_owner, listw = idw.lw, alternative = \"two.sided\")\n\n\n    Moran I test under randomisation\n\ndata:  msoa.spdf$per_owner  \nweights: idw.lw    \n\nMoran I statistic standard deviate = 65.853, p-value &lt;\n2.2e-16\nalternative hypothesis: two.sided\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.6838957350     -0.0010183299      0.0001081719 \n\n\nInterpretation: In both cases, we have very strong autocorrelation between neighbouring/closer units (~.7). It barely matters which of the weights matrices we use. This autocorrelation is highly significant. we can thus reject the Null that units are independent of each other (at least at this spatial level and for the share of home owners).\n\n\n4.1.3 Residual-based Moran’s I\nWe can also use the same Moran’s I test to inspect spatial autocorrelation in residuals from an estimated linear model.\nLet’s start with an intercept only model.\n\nlm0 &lt;- lm(per_owner ~ 1, msoa.spdf)\nlm.morantest(lm0, listw = queens.lw, alternative = \"two.sided\")\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = per_owner ~ 1, data = msoa.spdf)\nweights: queens.lw\n\nMoran I statistic standard deviate = 38.177, p-value &lt;\n2.2e-16\nalternative hypothesis: two.sided\nsample estimates:\nObserved Moran I      Expectation         Variance \n    0.7287068548    -0.0010183299     0.0003653613 \n\n\nThis is exactly what we have received in the general case of Moran’s I.\nNow, lets add some predictors. For instance, the distance to the city centre, and the population density may be strongly related to the home ownership rates and explain parts of the spatial dependence.\n\n### Distance to city center\n# Define centre\ncentre &lt;- st_as_sf(data.frame(lon = -0.128120855701165, \n                              lat = 51.50725909644806),\n                   coords = c(\"lon\", \"lat\"), \n                   crs = 4326)\n# Reproject\ncentre &lt;- st_transform(centre, crs = st_crs(msoa.spdf))\n# Calculate distance\nmsoa.spdf$dist_centre &lt;- as.numeric(st_distance(msoa.spdf, centre)) / 1000\n# hist(msoa.spdf$dist_centre)\n\n### Run model with predictors\nlm1 &lt;- lm(per_owner ~ dist_centre + POPDEN, msoa.spdf)\nlm.morantest(lm1, listw = queens.lw, alternative = \"two.sided\")\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = per_owner ~ dist_centre + POPDEN, data =\nmsoa.spdf)\nweights: queens.lw\n\nMoran I statistic standard deviate = 22.674, p-value &lt;\n2.2e-16\nalternative hypothesis: two.sided\nsample estimates:\nObserved Moran I      Expectation         Variance \n    0.4298146060    -0.0024065617     0.0003633607 \n\n\nThere is still considerable auto-correlation in the residuals. However, we have reduce it by a substantial amount with two very simple control variables.\n\n\n4.1.4 Semivariogram\nThe sample variogram \\(\\gamma(h)\\) for distance intervals \\(h_i\\) describes the average square difference between the points in this distance interval:\n\\[\n\\hat{\\gamma}(h_i) = \\frac{1}{2N(h_i)}\\sum_{j=1}^{N(h_i)}(z(s_i)-z(s_i+h'))^2, \\ \\ h_{i,0} \\le h' &lt; h_{i,1}\n\\tag{4.1}\\]\nwith the number of available pairs \\(N(h_i)\\) in each distance interval \\(h_i\\). Basically, it is the variance within each distance interval.\nFor more information, see for instance the Geospatial Data Science in R by Zia Ahmed or Pebesma and Bivand (2023).\nTo calculate the empirical semi-vriogram, we can use the package gstat with the function variogram().\n\n# Variogram No2\nv.no2 &lt;- variogram(no2 ~ 1, msoa.spdf)\n\nPlease note that rgdal will be retired during October 2023,\nplan transition to sf/stars/terra functions using GDAL and PROJ\nat your earliest convenience.\nSee https://r-spatial.org/r/2023/05/15/evolution4.html and https://github.com/r-spatial/evolution\nrgdal: version: 1.6-7, (SVN revision 1203)\nGeospatial Data Abstraction Library extensions to R successfully loaded\nLoaded GDAL runtime: GDAL 3.6.2, released 2023/01/02\nPath to GDAL shared files: C:/Users/qtnztru/AppData/Local/R/win-library/4.3/rgdal/gdal\n GDAL does not use iconv for recoding strings.\nGDAL binary built with GEOS: TRUE \nLoaded PROJ runtime: Rel. 9.2.0, March 1st, 2023, [PJ_VERSION: 920]\nPath to PROJ shared files: C:/Users/qtnztru/AppData/Local/R/win-library/4.3/rgdal/proj\nPROJ CDN enabled: FALSE\nLinking to sp version:1.6-1\nTo mute warnings of possible GDAL/OSR exportToProj4() degradation,\nuse options(\"rgdal_show_exportToProj4_warnings\"=\"none\") before loading sp or rgdal.\n\nplot(v.no2, xlim = c(0, 1.075 * max(v.no2$dist)),\n     ylim = c(-10, 1.05 * max(v.no2$gamma)))\n\n\n\n\nAbove graphs shows that the variance within each distance interval gradually increases, up to a distance of ~ 18km, and then level off at a relative constant level. Lower variances within lower values of distances means that observations are more similar to each other the closer they are.\nWe can also try to fit a model that resembles the spatial structure. This becomes important when we want to perform spatial interpolation (e.g. to impute missings).\n\n\n\nTheoretical exponential semi-variogram model. Source: https://www.aspexit.com/variogram-and-spatial-autocorrelation\n\n\n\n# Intial parameter set by eye esitmation\nm.no2 &lt;- vgm(60, \"Cir\", 20000, 0)  # Sill, model, range, nugget\n# least square fit\nm.f.v.no2 &lt;- fit.variogram(v.no2, m.no2)\n\n\n#### Plot varigram and fitted model:\nplot(v.no2, pl = FALSE, \n     model = m.f.v.no2,\n     col=\"blue\", \n     cex = 0.9, \n     lwd = 0.5,\n     lty = 1,\n     pch = 19,\n     main = \"Variogram and Fitted Model\",\n     xlab = \"Distance (m)\",\n     ylab = \"Semivariance\")"
  },
  {
    "objectID": "04_dependence.html#local-autocorrelation",
    "href": "04_dependence.html#local-autocorrelation",
    "title": "4  Detecting Spatial Dependence",
    "section": "4.2 Local Autocorrelation",
    "text": "4.2 Local Autocorrelation\nThe Global Moran’s I statistic above summarizes the spatial pattern by a single value. Although this is helpful to get a feeling of the strength of the general spatial association, it is often more helpful to inspect the spatial pattern in more detail.\nThe most prominent measure is the Local Indicators of Spatial Association (LISA) (Anselin 1995). LISA measures assess the importance and significance of a satistic at different spatial locations. For more information see for instance the GeoData Materials by Luc Anselin.\nFor instance, we can use the Moran Plot to identify how single (pairs of) units contribute to the overall dependence.\n\nmp &lt;- moran.plot(msoa.spdf$per_owner, queens.lw)\n\n\n\n\nIn the lower left corner, we see units with a low-low share of home ownership: focal and neighbouring units have a low share of home owners. In the top right corner, by contrast, we see high-high units.\nAnd we can plot influence values on the Overall Moran statistic.\n\nmsoa.spdf$hat_value &lt;- mp$hat \nmp1 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = \"hat_value\", \n          palette = viridis(n = 10, direction = -1, option = \"C\"),\n          ) +\n  tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"Influence\", \n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8)\n\nmp1"
  },
  {
    "objectID": "04_dependence.html#local-morans-i",
    "href": "04_dependence.html#local-morans-i",
    "title": "4  Detecting Spatial Dependence",
    "section": "4.3 Local Moran’s I",
    "text": "4.3 Local Moran’s I\nLocal Moran’s I is a local version of the overall Moran’s I to identify local clusters and local spatial outliers (Anselin 1995). The Local Moran’s I is just a local version which is calculated for each location:\n\\[      \n        \\begin{equation}\n        \\boldsymbol{\\mathbf{I}}_i  =  \n        \\frac{z_i \\sum_j w_{ij}z_j}\n            {\\sum_i (z_i)^2 / (n-1)}, \\text{where }\n        \\end{equation}\n\\] We use the unfction localmoran() to calculate the local test statistic .\n\nloci &lt;- localmoran(msoa.spdf$per_owner, listw = queens.lw)\nhead(loci)\n\n           Ii          E.Ii      Var.Ii       Z.Ii Pr(z != E(Ii))\n1  0.42322928 -1.285364e-04 0.011367934  3.9706976   7.166249e-05\n2 -0.12775982 -2.229957e-05 0.003634711 -2.1187688   3.411001e-02\n3  0.38111534 -6.569549e-04 0.091630752  1.2611995   2.072370e-01\n4  1.02874685 -1.428679e-03 0.279333375  1.9491704   5.127507e-02\n5  0.08553291 -2.108521e-04 0.041275789  0.4220412   6.729949e-01\n6 -0.24014505 -2.228818e-04 0.036321252 -1.2588964   2.080678e-01\n\n\nIt also has an attribute with the Moran plot quadrant of each observation.\n\nhead(attr(loci, \"quadr\"))\n\n       mean    median     pysal\n1   Low-Low   Low-Low   Low-Low\n2  Low-High  Low-High  Low-High\n3 High-High High-High High-High\n4 High-High High-High High-High\n5 High-High High-High High-High\n6  Low-High  Low-High  Low-High\n\n\nThis returns a data.frame with local moran statisic, the expectation of local moran statistic, its variance, and a p value for the satistical significance of each unit. Note that we obviously have a problem of multiple comparisons here and thus may want to correct the significance level, e.g. by Bonferroni adjustment (Bivand and Wong 2018).\n\nloci.df &lt;- data.frame(loci)\nnames(loci.df) &lt;- gsub(\"\\\\.\", \"\", names(loci.df))\nmsoa.spdf$loci &lt;- loci.df$Ii\nmsoa.spdf$p_value &lt;- loci.df$PrzEIi\nmsoa.spdf$p_value_adj1 &lt;- p.adjust(loci.df$PrzEIi, \"BY\")\nmsoa.spdf$p_value_adj2 &lt;- p.adjust(loci.df$PrzEIi, \"bonferroni\")\n\n\nmp1 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = c(\"loci\", \"p_value\", \"p_value_adj1\", \"p_value_adj2\"),\n          palette = viridis(n = 10, direction = -1, option = \"C\"),\n          ) +\n  tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"left\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"Local Morans I\", \n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            panel.labels = c(\"Morans I\",\n                               \"P value\",\n                               \"p value BY\",\n                             \"p value Bonferroni\"))\n\nmp1\n\n\n\n\nSomething you can often see are so called LISA hotspot maps. They are based on the same idea as the moran plot, and show cluster of high-high and low-low values. We can use the hotspot function to identify the clusters, with a cutoff for singificance and the adjustment for multiple testing.\n\n# Calculate clusters\nmsoa.spdf$lisa_cluster &lt;- hotspot(loci, \n                                  \"Pr(z != E(Ii))\", \n                                  cutoff = 0.05, \n                                  quadrant.type = \"mean\",\n                                  p.adjust = \"BY\")\n\n# Map\nmp1 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = c(\"lisa_cluster\"),\n          palette = viridis(n = 3, direction = -1, option = \"D\"),\n          colorNA = \"white\") +\n  tm_borders(col = \"grey70\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"left\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"Home Ownership \\n LISA Clusters p(BY) &lt; 0.05\", \n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,)\n\nmp1\n\n\n\n\nNote that it is not suggested to interpret those cluster as singificant in the strict statistical sense. Pebesma and Bivand (2023) suggest to speak of interesting clusters. After all, this is an explorative approach. Nevertheless, it can help to identify spatial patterns and clusters.\nThere are more ways of calculating these hotspot maps and more choices on the cutoffs and calculation of the statistical significance. For more materials see Chapter 15 of Pebesma and Bivand (2023)."
  },
  {
    "objectID": "04_dependence.html#exercise",
    "href": "04_dependence.html#exercise",
    "title": "4  Detecting Spatial Dependence",
    "section": "4.5 Exercise",
    "text": "4.5 Exercise\n\nPlease calculate a neighbours weights matrix of the nearest 10 neighbours (see spdep::knearneigh()), and create a listw object using row normalization.\n\n\ncoords &lt;- st_centroid(msoa.spdf)\n\nWarning: st_centroid assumes attributes are constant over\ngeometries\n\nk10.nb &lt;- knearneigh(coords, k = 10)\n\n\nCan you create a map containing the City of London (MSOA11CD = “E02000001”) and its then nearest neighbours?\n\n\ni &lt;- which(msoa.spdf$MSOA11CD == \"E02000001\")\n\n# Extract neigbours\nj &lt;- k10.nb$nn[i,]\n\nmapview(list(msoa.spdf[i,], msoa.spdf[j,]), col.regions = c(\"red\", \"blue\"))\n\n\n\n\n\n\n\nChose another characteristics from the data (e.g. ethnic groups or house prices) and calculate global Moran’s I for it.\n\n\n# Gen nb object\nk10.nb &lt;- knn2nb(k10.nb)\n\n# Gen listw object\nk10.listw &lt;- nb2listw(k10.nb, style = \"W\")\n\n# MOran test\nmoran.test(msoa.spdf$per_white, listw = k10.listw)\n\n\n    Moran I test under randomisation\n\ndata:  msoa.spdf$per_white  \nweights: k10.listw    \n\nMoran I statistic standard deviate = 55.733, p-value &lt;\n2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.7623842505     -0.0010183299      0.0001876235 \n\n\n\nProduce a LISA cluster map for the characteristic you have chosen.\n\n\nloci2 &lt;- localmoran(msoa.spdf$per_white, listw = k10.listw)\n\n# Calculate clusters\nmsoa.spdf$lisa_cluster &lt;- hotspot(loci2, \n                                  \"Pr(z != E(Ii))\", \n                                  cutoff = 0.05, \n                                  quadrant.type = \"mean\",\n                                  p.adjust = \"BY\")\n\n# Map\nmp1 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = c(\"lisa_cluster\"),\n          palette = viridis(n = 3, direction = -1, option = \"D\"),\n          colorNA = \"white\") +\n  tm_borders(col = \"grey70\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"left\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"Percentage White \\n LISA Clusters p(BY) &lt; 0.05\", \n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,)\n\nmp1\n\n\n\n\n\n\n\n\n\n\nAnselin, Luc. 1995. “Local Indicators of Spatial Association-LISA.” Geographical Analysis 27 (2): 93–115. https://doi.org/10.1111/j.1538-4632.1995.tb00338.x.\n\n\nBivand, Roger, and David W. S. Wong. 2018. “Comparing Implementations of Global and Local Indicators of Spatial Association.” TEST 27 (3): 716–48. https://doi.org/10.1007/s11749-018-0599-x.\n\n\nCliff, Andrew, and Keith Ord. 1972. “Testing for Spatial Autocorrelation Among Regression Residuals.” Geographical Analysis 4 (3): 267–84. https://doi.org/10.1111/j.1538-4632.1972.tb00475.x.\n\n\nKelejian, Harry H., and Gianfranco Piras. 2017. Spatial Econometrics. Elsevier. https://doi.org/10.1016/C2016-0-04332-2.\n\n\nMoran, P. A. P. 1950. “Notes on Continuous Stochastic Phenomena.” Biometrika 37 (1/2): 17. https://doi.org/10.2307/2332142.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With Applications in R. First. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nWong, David. 2009. “The Modifiable Areal Unit Problem (MAUP).” In The Sage Handbook of Spatial Analysis, edited by A. Stewart Fotheringham and Peter Rogerson, 105–24. Los Angeles and London: Sage."
  },
  {
    "objectID": "05_regression-theory.html#why-do-we-need-spatial-regression-models",
    "href": "05_regression-theory.html#why-do-we-need-spatial-regression-models",
    "title": "5  Spatial Regression Models",
    "section": "5.1 Why do we need spatial regression models",
    "text": "5.1 Why do we need spatial regression models\n\n5.1.1 Non-spatial OLS\nLet us start with a linear model, where \\(\\boldsymbol{\\mathbf{y}}\\) is the outcome or dependent variable (\\(N \\times 1\\)), \\(\\boldsymbol{\\mathbf{X}}\\) are various exogenous covariates (\\(N \\times k\\)), and \\(\\boldsymbol{\\mathbf{\\varepsilon}}\\) (\\(N \\times 1\\)) is the error term. We are usually interested in the coefficient vector \\(\\boldsymbol{\\mathbf{\\beta}}\\) (\\(k \\times 1\\)) and its insecurity estimates.\n\\[\n{\\boldsymbol{\\mathbf{y}}}={\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{\\varepsilon}}}\n\\] The work-horse for estimating \\(\\boldsymbol{\\mathbf{\\beta}}\\) in the social science is the OLS estimator (Wooldridge 2010).\n\\[\n\\hat{\\beta}=({\\boldsymbol{\\mathbf{X}}}^\\intercal{\\boldsymbol{\\mathbf{X}}})^{-1}{\\boldsymbol{\\mathbf{X}}}^\\intercal{\\boldsymbol{\\mathbf{y}}}.\n\\]\n\n\n\n\n\n\nOLS assumptions I\n\n\n\n\n\\(\\mathrm{E}(\\epsilon_i|\\boldsymbol{\\mathbf{X}}_i) = 0\\): for every value of \\(X\\), the average / expectation of the error term \\(\\boldsymbol{\\mathbf{\\varepsilon}}\\) equals zero – put differently: the error term is independent of \\(X\\),\nthe observations of the sample are independent and identically distributed (i.i.d),\nthe fourth moments of the variables \\(\\boldsymbol{\\mathbf{X}}_i\\) and \\(Y_i\\) are positive and definite – put differently: extreme values / outliers are very very rare,\n\\(\\text{rank}(\\boldsymbol{\\mathbf{X}}) = K\\): the matrix \\(\\boldsymbol{\\mathbf{X}}\\) has full rank – put differently: no perfect multicollinearity between the covariates,\n\n\n\n\n\n\n\n\n\nOLS assumptions II\n\n\n\n\n\\(\\mathrm{Var}(\\varepsilon|x) = \\sigma^2\\): the error terms \\(\\varepsilon\\) are homoskedastic / have the same variance given any value of the explanatory variable,\n\\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\): the error terms \\(\\varepsilon\\) are normally distributed (conditional on the explanatory variables \\(X_i\\)).\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhich of the six assumptions above may be violated by spatial dependence?\n\n\n\n\n\n5.1.2 Problem of ignoring spatial dependence\nDoes spatial dependence influence the results / coefficient estimates of non-spatial regression models, or in other words: is ignoring spatial dependence harmful?\nI’ve heard different answers, ranging from “It only affects the standard errors” to “it always introduces bias”. As so often, the true (or best?) answer is somewhere in the middle: it depends (Betz, Cook, and Hollenbach 2020; Cook, Hays, and Franzese 2020; Pace and LeSage 2010; Rüttenauer 2022).\nThe easiest way to think of it is analogous to the omit variable bias (Betz, Cook, and Hollenbach 2020; Cook, Hays, and Franzese 2020):\n\\[\nplim~\\hat{\\beta}_{OLS}= \\beta  + \\gamma \\frac{\\mathrm{Cov}(\\boldsymbol{\\mathbf{x}}, \\boldsymbol{\\mathbf{z}})}{\\mathrm{Var}(\\boldsymbol{\\mathbf{x}})},\n\\]\nwhere \\(z\\) is some omit variable, and \\(\\gamma\\) is the conditional effect of \\(\\boldsymbol{\\mathbf{z}}\\) on \\(\\boldsymbol{\\mathbf{y}}\\). Now imagine that the neighbouring values of the dependent variable \\(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{y}}\\) are autocorrelated to focal unit which we denote with \\(\\rho &gt; 0\\), and that the covariance between the focal unit’s exogenous covariates and \\(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{y}}\\) is not zero. Then we will have an omitted variable bias due to spatial dependence:\n\\[\nplim~\\hat{\\beta}_{OLS}= \\beta  + \\rho \\frac{\\mathrm{Cov}(\\boldsymbol{\\mathbf{x}}, \\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{y}})}{\\mathrm{Var}(\\boldsymbol{\\mathbf{x}})} \\neq \\beta,\n\\]\nFor completeness, the entire bias is a bit more complicated (Pace and LeSage 2010; Rüttenauer 2022) and looks like:\n\\[\nplim~\\hat{\\beta}=\\frac{\\sum_{ij}({\\boldsymbol{\\mathbf{M}}}(\\delta){\\boldsymbol{\\mathbf{M}}}(\\delta)^\\intercal\\circ{\\boldsymbol{\\mathbf{M}}}(\\rho))_{ij}}\n{\\mathrm{tr}({\\boldsymbol{\\mathbf{M}}}(\\delta){\\boldsymbol{\\mathbf{M}}}(\\delta)^\\intercal)}\\beta \\\\\n+\\frac{\\sum_{ij}({\\boldsymbol{\\mathbf{M}}}(\\delta){\\boldsymbol{\\mathbf{M}}}(\\delta)^\\intercal\\circ{\\boldsymbol{\\mathbf{M}}}(\\rho){\\boldsymbol{\\mathbf{W}}})_{ij}}\n{\\mathrm{tr}({\\boldsymbol{\\mathbf{M}}}(\\delta){\\boldsymbol{\\mathbf{M}}}(\\delta)^\\intercal)}\\theta,\n\\] where \\(\\circ\\) denotes the Hadamard product, \\({\\boldsymbol{\\mathbf{M}}}(\\delta)=({\\boldsymbol{\\mathbf{I}}}_N-\\delta{\\boldsymbol{\\mathbf{W}}})^{-1}\\), and \\({\\boldsymbol{\\mathbf{M}}}(\\rho)=({\\boldsymbol{\\mathbf{I}}}_N-\\rho{\\boldsymbol{\\mathbf{W}}})^{-1}\\).\n\n\n(Don’t worry, no need to learn by hard!!)\n\n\nEssentially, the non-spatial OLS estimator \\(\\beta_{OLS}\\) is biased in the presence of either (Pace and LeSage 2010; Rüttenauer 2022):\n\nSpatial autocorrelation in the dependent variable (\\(\\rho\\neq0\\)) and spatial autocorrelation in the covariate (\\(\\delta\\neq0\\)). This bias increases with \\(\\rho\\), \\(\\delta\\), and \\(\\beta\\).\nLocal spatial spillover effects (\\(\\theta\\neq0\\)) and spatial autocorrelation in the covariate (\\(\\delta\\neq0\\)). This is analogous to the omitted variable bias resulting from the omission of \\({\\boldsymbol{\\mathbf{W}}} {\\boldsymbol{\\mathbf{x}}}\\). It increases with \\(\\theta\\) and \\(\\delta\\), but additionally with \\(\\rho\\) if \\(\\theta\\neq0\\) and \\(\\delta\\neq0\\).\nAn omitted variable and \\(\\mathrm{E}({\\boldsymbol{\\mathbf{\\varepsilon}}}|{\\boldsymbol{\\mathbf{x}}})\\neq0\\). This non-spatial omitted variable bias \\(\\gamma\\) is amplified by spatial dependence in the disturbances (\\(\\lambda\\)) and spatial autocorrelation in the dependent variable (\\(\\rho\\)), but also increases with positive values of \\(\\delta\\) if either \\(\\rho\\neq 0\\) or \\(\\lambda\\neq 0\\). Obviously, it also increases with \\(\\gamma\\)."
  },
  {
    "objectID": "05_regression-theory.html#spatial-regression-models",
    "href": "05_regression-theory.html#spatial-regression-models",
    "title": "5  Spatial Regression Models",
    "section": "5.2 Spatial Regression Models",
    "text": "5.2 Spatial Regression Models\nBroadly, spatial dependence or clustering in some characteristic can be the result of three different processes:\n\n\n\n\nflowchart TD\n  id1{Spatial dependence} --&gt; A[Spatial interdepence]\n  id1 --&gt; B[Spillovers from covariates]\n  id1 --&gt; C[Clustering unobservables]\n\n\n\n\n\nStrictly speaking, there are some other possibilities too, such as measurement error or the wrong choice on the spatial level. For instance, imagine we have a city-specific characteristic (e.g. public spending) allocated to neighbourhood units. Obviously, this will introduce heavy autocorrelation on the neigbourhood level by construction.\nThere are three basic ways of incorporating spatial dependence, which then can be further combined. As before, the \\(N \\times N\\) spatial weights matrix \\(\\boldsymbol{\\mathbf{W}}\\) defines the spatial relationship between units.\n\n5.2.1 Spatial Error Model (SEM)\n\nClustering on Unobservables\n\n\\[\n        \\begin{equation}\n        \\begin{split}\n        {\\boldsymbol{\\mathbf{y}}}&=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+{\\boldsymbol{\\mathbf{u}}},\\\\\n        {\\boldsymbol{\\mathbf{u}}}&=\\lambda{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{u}}}+{\\boldsymbol{\\mathbf{\\varepsilon}}}\n        \\end{split}\n        \\end{equation}\n\\]\n\\(\\lambda\\) denotes the strength of the spatial correlation in the errors of the model: your errors influence my errors.\n\n\\(&gt; 0\\): positive error dependence,\n\\(&lt; 0\\): negative error dependence,\n\\(= 0\\): traditional OLS model.\n\n\\(\\lambda\\) is defined in the range \\([-1, +1]\\).\n\n\n5.2.2 Spatial Autoregressive Model (SAR)\n\nInterdependence\n\n\\[\n    \\begin{equation}\n        {\\boldsymbol{\\mathbf{y}}}=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+\\rho{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{y}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{\\varepsilon}}}\n        \\end{equation}  \n\\]\n\\(\\rho\\) denotes the strength of the spatial correlation in the dependent variable (spatial autocorrelation): your outcome influences my outcome.\n\n\\(&gt; 0\\): positive spatial dependence,\n\\(&lt; 0\\): negative spatial dependence,\n\\(= 0\\): traditional OLS model.\n\n\\(\\rho\\) is defined in the range \\([-1, +1]\\).\n\n\n5.2.3 Spatially lagged X Model (SLX)\n\nSpillovers in Covariates\n\n\\[\n        \\begin{equation}\n        {\\boldsymbol{\\mathbf{y}}}=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\theta}}}+ {\\boldsymbol{\\mathbf{\\varepsilon}}}\n        \\end{equation}\n\\]\n\\(\\theta\\) denotes the strength of the spatial spillover effects from covariate(s) on the dependent variable: your covariates influence my outcome.\n\\(\\theta\\) is basically like any other coefficient from a covariate. It is thus not bound to any range.\nMoreover, there are models combining two sets of the above specifications.\n\n\n5.2.4 Spatial Durbin Model (SDM)\n\nInterdependence\nSpillovers in Covariates\n\n\\[\n        \\begin{equation}\n        {\\boldsymbol{\\mathbf{y}}}=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+\\rho{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{y}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\theta}}}+ {\\boldsymbol{\\mathbf{\\varepsilon}}}\n        \\end{equation}\n\\]\n\n\n5.2.5 Spatial Durbin Error Model (SDEM)\n\nClustering on Unobservables\nSpillovers in Covariates\n\n\\[\n    \\begin{equation}\n        \\begin{split}\n        {\\boldsymbol{\\mathbf{y}}}&=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\theta}}}+ {\\boldsymbol{\\mathbf{u}}},\\\\\n        {\\boldsymbol{\\mathbf{u}}}&=\\lambda{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{u}}}+{\\boldsymbol{\\mathbf{\\varepsilon}}}\n        \\end{split}\n        \\end{equation}\n\\]\n\n\n5.2.6 Combined Spatial Autocorrelation Model (SAC)\n\nClustering on Unobservables\nInterdependence\n\n\\[\n        \\begin{equation}\n        \\begin{split}\n        {\\boldsymbol{\\mathbf{y}}}&=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+\\rho{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{y}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{u}}},\\\\\n        {\\boldsymbol{\\mathbf{u}}}&=\\lambda{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{u}}}+{\\boldsymbol{\\mathbf{\\varepsilon}}}\n        \\end{split}\n        \\end{equation}\n\\]\n\n\n5.2.7 General Nesting Spatial Model (GNS)\n\nClustering on Unobservables\nInterdependence\nSpillovers in Covariates\n\n\\[\n        \\begin{equation}\n        \\begin{split}\n        {\\boldsymbol{\\mathbf{y}}}&=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+\\rho{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{y}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\theta}}}+ {\\boldsymbol{\\mathbf{u}}},\\\\\n        {\\boldsymbol{\\mathbf{u}}}&=\\lambda{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{u}}}+{\\boldsymbol{\\mathbf{\\varepsilon}}}\n        \\end{split}\n        \\end{equation}\n\\]\n\n\n\n\n\n\nManski’s reflection problem\n\n\n\nThe General Nesting Spatial Model (GNS) is only weakly (or not?) identifiable (Gibbons and Overman 2012).\nIt’s analogous to Manski’s reflection problem on neighbourhood effects : If people in the same group behave similar, this can be because a) imitating behaviour of the group, b) exogenous characteristics of the group influence the behaviour, and c) members of the same group are exposed to the same external circumstances. We just cannot separate those in observational data.\n\n\nNote that all of these models assume different data generating processes (DGP) leading to the spatial pattern. Although there are specifications tests, it is generally not possible to let the data decide which one is the true underlying DGP (Cook, Hays, and Franzese 2020; Rüttenauer 2022). However, there might be theoretical reasons to guide the model specification (Cook, Hays, and Franzese 2020).\nJust because SAR is probably the most commonly used model does not make it the best choice. In contrast, various studies (Halleck Vega and Elhorst 2015; Rüttenauer 2022; Wimpy, Whitten, and Williams 2021) highlight the advantages of the relative simple SLX model. Moreover, this specification can basically be incorporated in any other statistical method.\n\n\n5.2.8 A note on missings\nMissing values create a problem in spatial data analysis. For instance, in a local spillover model with an average of 10 neighbours, two initial missing values will lead to 20 missing values in the spatially lagged variable. For global spillover models, one initial missing will ‘flow’ through the neighbourhood system until the cutoff point (and create an excess amount of missings).\nDepending on the data, units with missings can either be dropped and omitted from the initial weights creation, or we need to impute the data first, e.g. using interpolation or Kriging."
  },
  {
    "objectID": "05_regression-theory.html#mini-example",
    "href": "05_regression-theory.html#mini-example",
    "title": "5  Spatial Regression Models",
    "section": "5.3 Mini Example",
    "text": "5.3 Mini Example\nLet’s try to make sense of this. We rely on a mini example using a few units in Camden\n\nsub.spdf &lt;- msoa.spdf[c(172, 175, 178, 179, 181, 182), ]\nmapview(sub.spdf)\n\n\n\n\n\n\nWe then construct queens neighbours, and have a look at the resulting non-normalized matrix \\(\\boldsymbol{\\mathbf{W}}\\).\n\nqueens.nb &lt;- poly2nb(sub.spdf, queen = TRUE, snap = 1)\nW &lt;- nb2mat(queens.nb, style = \"B\")\nW\n\n    [,1] [,2] [,3] [,4] [,5] [,6]\n172    0    0    1    0    0    0\n175    0    0    0    1    0    1\n178    1    0    0    1    1    0\n179    0    1    1    0    1    1\n181    0    0    1    1    0    1\n182    0    1    0    1    1    0\nattr(,\"call\")\nnb2mat(neighbours = queens.nb, style = \"B\")\n\n\nWe have selected 6 units. So, \\(\\boldsymbol{\\mathbf{W}}\\) is a \\(6 \\times 6\\) matrix. we see that observation 1 has one neighbour: observation 3. Observation 2 has two nieghbours: observation 4 and observation 6. The diagonal is zero: no unit is a neighbour of themselves.\nNo we row-normalize this matrix.\n\nqueens.lw &lt;- nb2listw(queens.nb,\n                      style = \"W\")\nW_rn &lt;- listw2mat(queens.lw)\nW_rn\n\n         [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n172 0.0000000 0.0000000 1.0000000 0.0000000 0.0000000 0.0000000\n175 0.0000000 0.0000000 0.0000000 0.5000000 0.0000000 0.5000000\n178 0.3333333 0.0000000 0.0000000 0.3333333 0.3333333 0.0000000\n179 0.0000000 0.2500000 0.2500000 0.0000000 0.2500000 0.2500000\n181 0.0000000 0.0000000 0.3333333 0.3333333 0.0000000 0.3333333\n182 0.0000000 0.3333333 0.0000000 0.3333333 0.3333333 0.0000000\n\n\nNo every single weight \\(w_{ij}\\) is divided by the total number of neighbours \\(n_i\\) of the focal unit. For observation 1, observation 3 is the only neighbour, thus a weight = 1. FOr observation two, both neighbours have a weight of 1/2. For obervation 3 (with three neighbours) each neighbour got a weight of 1/3.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat happens if we multiply this matrix \\(\\boldsymbol{\\mathbf{W}}\\) with a \\(N \\times 1\\) vector \\(\\boldsymbol{\\mathbf{y}}\\) or \\(\\boldsymbol{\\mathbf{x}}\\)?\n\n\nA short reminder on matrix multiplication.\n\\[\n\\boldsymbol{\\mathbf{W}} * \\boldsymbol{\\mathbf{y}} =\n\\begin{bmatrix}\nw_{11} & w_{12} & w_{13}\\\\\nw_{21} & w_{22} & w_{23}\\\\\nw_{31} & w_{32} & w_{33}\n\\end{bmatrix} *\n\\begin{bmatrix}\ny_{11} \\\\\ny_{21} \\\\\ny_{31}  \n\\end{bmatrix}\\\\\n= \\begin{bmatrix}\nw_{11}y_{11} + w_{12}y_{21} + w_{13}y_{31}\\\\\nw_{21}y_{11} + w_{22}y_{21} + w_{23}y_{31}\\\\\nw_{31}y_{11} + w_{32}y_{21} + w_{33}y_{31}  \n\\end{bmatrix}\n\\]\nEach line of \\(\\boldsymbol{\\mathbf{W}} * \\boldsymbol{\\mathbf{y}}\\) just gives a weighted average of the other \\(y\\)-values \\(y_j\\) in the sample. In case of the row-normalization, each neighbour gets the same weight \\(\\frac{1}{n_i}\\). This is simply the mean of \\(y_j\\) of the neighbours in case of a row-normalized contiguity weights matrix.\nNote that the mean interpretation is only valid with row-normalization. What would we get with inverse-distance based weights?\nLet’s look at this in our example\n\ny &lt;- sub.spdf$med_house_price\nx &lt;- sub.spdf$pubs_count\n\nW_rn\n\n         [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n172 0.0000000 0.0000000 1.0000000 0.0000000 0.0000000 0.0000000\n175 0.0000000 0.0000000 0.0000000 0.5000000 0.0000000 0.5000000\n178 0.3333333 0.0000000 0.0000000 0.3333333 0.3333333 0.0000000\n179 0.0000000 0.2500000 0.2500000 0.0000000 0.2500000 0.2500000\n181 0.0000000 0.0000000 0.3333333 0.3333333 0.0000000 0.3333333\n182 0.0000000 0.3333333 0.0000000 0.3333333 0.3333333 0.0000000\n\ny\n\n[1] 376812.5 414625.0 713125.0 322750.0 495000.0 364000.0\n\nx\n\n[1] 1 3 3 1 9 7\n\nW_rn_y &lt;- W_rn %*% y\nW_rn_x &lt;- W_rn %*% x\nW_rn_y\n\n        [,1]\n172 713125.0\n175 343375.0\n178 398187.5\n179 496687.5\n181 466625.0\n182 410791.7\n\nW_rn_x\n\n        [,1]\n172 3.000000\n175 4.000000\n178 3.666667\n179 5.500000\n181 3.666667\n182 4.333333\n\n\nLet’s check if our interpretation is true\n\nW_rn_y[1] == y[3]\n\n[1] TRUE\n\nW_rn_y[2] == mean(y[c(4, 6)])\n\n[1] TRUE\n\nW_rn_y[4] == mean(y[c(2, 3, 5, 6)])\n\n[1] TRUE"
  },
  {
    "objectID": "05_regression-theory.html#real-example",
    "href": "05_regression-theory.html#real-example",
    "title": "5  Spatial Regression Models",
    "section": "5.4 Real Example",
    "text": "5.4 Real Example\nFirst, we need the a spatial weights matrix.\n\n# Contiguity (Queens) neighbours weights\nqueens.nb &lt;- poly2nb(msoa.spdf, \n                     queen = TRUE, \n                     snap = 1) # we consider points in 1m distance as 'touching'\nqueens.lw &lt;- nb2listw(queens.nb,\n                      style = \"W\")\n\nWe can estimate spatial models using spatialreg.\n\n5.4.1 SAR\nLets estimate a spatial SAR model using the lagsarlm() with contiguity weights. We use median house value as depended variable, and include population density (POPDEN), the air pollution (no2), and the share of ethnic minorities (per_mixed, per_asian, per_black, per_other).\n\nmod_1.sar &lt;- lagsarlm(log(med_house_price) ~ log(no2) + log(POPDEN) + \n                        per_mixed + per_asian + per_black + per_other,  \n                      data = msoa.spdf, \n                      listw = queens.lw,\n                      Durbin = FALSE) # we could here extend to SDM\nsummary(mod_1.sar)\n\n\nCall:\nlagsarlm(formula = log(med_house_price) ~ log(no2) + log(POPDEN) + \n    per_mixed + per_asian + per_black + per_other, data = msoa.spdf, \n    listw = queens.lw, Durbin = FALSE)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.5281789 -0.1220524 -0.0099245  0.0992203  1.0936745 \n\nType: lag \nCoefficients: (asymptotic standard errors) \n               Estimate  Std. Error  z value  Pr(&gt;|z|)\n(Intercept)  3.17383180  0.29041604  10.9286 &lt; 2.2e-16\nlog(no2)     0.39705423  0.04452880   8.9168 &lt; 2.2e-16\nlog(POPDEN) -0.05583014  0.01242876  -4.4920 7.055e-06\nper_mixed    0.01851577  0.00579832   3.1933  0.001407\nper_asian   -0.00228346  0.00045876  -4.9775 6.442e-07\nper_black   -0.01263650  0.00100282 -12.6009 &lt; 2.2e-16\nper_other   -0.00161419  0.00289082  -0.5584  0.576582\n\nRho: 0.66976, LR test value: 473.23, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.025311\n    z-value: 26.461, p-value: &lt; 2.22e-16\nWald statistic: 700.19, p-value: &lt; 2.22e-16\n\nLog likelihood: 196.7203 for lag model\nML residual variance (sigma squared): 0.035402, (sigma: 0.18815)\nNumber of observations: 983 \nNumber of parameters estimated: 9 \nAIC: -375.44, (AIC for lm: 95.786)\nLM test for residual autocorrelation\ntest value: 8.609, p-value: 0.0033451\n\n\nThis looks pretty much like a conventional model output, with some additional information: a highly significant mod_1.sar$rho of 0.67 indicates strong positive spatial autocorrelation.\nRemember that is the coefficient for the term \\(\\boldsymbol{\\mathbf{y}} = \\rho \\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{y}} \\ldots\\). It is bound to be below 1 for positive autocorrelation.\nIn substantive terms, house prices in the focal unit positively influence house prices in neighbouring units, which again influences house prices among the neighbours of these neighbours, and so on (we’ll get back to this).\n\n\n\n\n\n\nWarning\n\n\n\nThe coefficients of covariates in a SAR model are not marginal or partical effects, because of the spillovers and feedback loops in \\(\\boldsymbol{\\mathbf{y}}\\) (see below)!\nFrom the coefficient, we can only interpret the direction: there’s a positive effect of air pollution and a negative effect of population sensity, and so on…\n\n\n\n\n5.4.2 SEM\nSEM models can be estimated using errorsarlm().\n\nmod_1.sem &lt;- errorsarlm(log(med_house_price) ~ log(no2) + log(POPDEN) +\n                          per_mixed + per_asian + per_black + per_other,  \n                        data = msoa.spdf, \n                        listw = queens.lw,\n                        Durbin = FALSE) # we could here extend to SDEM\nsummary(mod_1.sem)\n\n\nCall:\nerrorsarlm(formula = log(med_house_price) ~ log(no2) + log(POPDEN) + \n    per_mixed + per_asian + per_black + per_other, data = msoa.spdf, \n    listw = queens.lw, Durbin = FALSE)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.581785 -0.105218 -0.012758  0.094430  0.913425 \n\nType: error \nCoefficients: (asymptotic standard errors) \n               Estimate  Std. Error  z value  Pr(&gt;|z|)\n(Intercept) 12.92801104  0.35239139  36.6865 &lt; 2.2e-16\nlog(no2)     0.15735296  0.10880727   1.4462 0.1481317\nlog(POPDEN) -0.08316270  0.01254315  -6.6301 3.354e-11\nper_mixed   -0.03377962  0.00811054  -4.1649 3.115e-05\nper_asian   -0.00413115  0.00096849  -4.2656 1.994e-05\nper_black   -0.01653816  0.00126741 -13.0488 &lt; 2.2e-16\nper_other   -0.01693012  0.00462999  -3.6566 0.0002556\n\nLambda: 0.88605, LR test value: 623.55, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.015803\n    z-value: 56.068, p-value: &lt; 2.22e-16\nWald statistic: 3143.6, p-value: &lt; 2.22e-16\n\nLog likelihood: 271.8839 for error model\nML residual variance (sigma squared): 0.026911, (sigma: 0.16405)\nNumber of observations: 983 \nNumber of parameters estimated: 9 \nAIC: NA (not available for weighted model), (AIC for lm: 95.786)\n\n\nIn this case mod_1.sem$lambda gives us the spatial parameter. A highly significant lambda of 0.89 indicates that the errors are highly spatially correlated (e.g. due to correlated unobservables). Again, $= 1 $ would be the maximum.\nIn spatial error models, we can interpret the coefficients directly, as in a conventional linear model.\n\n\n5.4.3 SLX\nSLX models can either be estimated with lmSLX() directly, or by creating \\(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{X}}\\) manually and plugging it into any available model-fitting function.\n\nmod_1.slx &lt;- lmSLX(log(med_house_price) ~ log(no2) + log(POPDEN) + \n                     per_mixed + per_asian + per_black + per_other,  \n                   data = msoa.spdf, \n                   listw = queens.lw, \n                   Durbin = TRUE) # use a formula to lag only specific covariates\nsummary(mod_1.slx)\n\n\nCall:\nlm(formula = formula(paste(\"y ~ \", paste(colnames(x)[-1], collapse = \"+\"))), \n    data = as.data.frame(x), weights = weights)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50809 -0.16605 -0.01817  0.13055  1.09039 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     10.582440   0.153862  68.779  &lt; 2e-16 ***\nlog.no2.        -0.440727   0.181063  -2.434  0.01511 *  \nlog.POPDEN.     -0.076840   0.017345  -4.430 1.05e-05 ***\nper_mixed       -0.033042   0.011298  -2.925  0.00353 ** \nper_asian       -0.002381   0.001474  -1.615  0.10655    \nper_black       -0.016229   0.001801  -9.009  &lt; 2e-16 ***\nper_other       -0.020391   0.006564  -3.107  0.00195 ** \nlag.log.no2.     0.993602   0.199370   4.984 7.38e-07 ***\nlag.log.POPDEN.  0.113262   0.028752   3.939 8.76e-05 ***\nlag.per_mixed    0.126069   0.014294   8.820  &lt; 2e-16 ***\nlag.per_asian   -0.003828   0.001661  -2.305  0.02140 *  \nlag.per_black   -0.018054   0.002241  -8.056 2.30e-15 ***\nlag.per_other    0.048139   0.007971   6.039 2.20e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2262 on 970 degrees of freedom\nMultiple R-squared:  0.653, Adjusted R-squared:  0.6487 \nF-statistic: 152.1 on 12 and 970 DF,  p-value: &lt; 2.2e-16\n\n\nIn SLX models, we can simply interpret the coefficients of direct and indirect (spatially lagged) covariates.\nFor instance, lets look at population density:\n\n\n\n\n\n\nInterpretaion SLX\n\n\n\n\nA high population density in the focal unit is related to lower house prices (a 1% increase in population density decreses house prices by -0.08%), but\nA high population density in the neighbouring areas is related to higher house prices (while keeping population density in the focal unit constant). A 1% increase in the average population density across the adjacent neighbourhoods increases house prices in the focal unit by 0.11%)\n\nPotential interpretation: areas with a low population density in central regions of the city (high pop density in surrounding neighbourhoods) have higher house prices. We could try testing this interpretation by including the distance to the city center as a control.\n\n\nAlso note how the air pollution coefficient has changed here, with a negative effect in the focal unit and positive one amonf the neighbouring units.\nAn alternative way of estimating the same model is lagging the covariates first.\n\n# Loop through vars and create lagged variables\nmsoa.spdf$log_POPDEN &lt;- log(msoa.spdf$POPDEN)\nmsoa.spdf$log_no2 &lt;- log(msoa.spdf$no2)\nmsoa.spdf$log_med_house_price &lt;- log(msoa.spdf$med_house_price)\n\nvars &lt;- c(\"log_med_house_price\", \"log_no2\", \"log_POPDEN\", \n          \"per_mixed\", \"per_asian\", \"per_black\", \"per_other\",\n          \"per_owner\", \"per_social\", \"pubs_count\")\nfor(v in vars){\n  msoa.spdf[, paste0(\"w.\", v)] &lt;- lag.listw(queens.lw,\n                                            var = st_drop_geometry(msoa.spdf)[, v])\n}\n\n# Alternatively:\nw_vars &lt;- create_WX(st_drop_geometry(msoa.spdf[, vars]),\n                    listw = queens.lw,\n                    prefix = \"w\")\n\nhead(w_vars)\n\n  w.log_med_house_price w.log_no2 w.log_POPDEN w.per_mixed\n1              12.98382  3.843750     4.662014    4.748368\n2              12.28730  3.098960     3.300901    3.978275\n3              12.21207  3.206338     4.009795    3.997487\n4              12.18176  3.169934     3.630360    2.759082\n5              12.11159  3.221203     3.993660    3.930061\n6              12.08393  3.217865     3.876070    3.419488\n  w.per_asian w.per_black w.per_other w.per_owner w.per_social\n1   23.899916    7.879758   3.2080074    25.75738     33.85580\n2   19.951593   10.451828   1.6368986    66.42278     15.75042\n3   20.793559   12.965863   1.7526693    58.72637     21.38169\n4    7.633439   12.135478   0.6992118    66.52519     19.70500\n5   12.791140   16.108948   1.3817357    53.05539     29.44022\n6    8.997514   15.312652   0.9611710    59.49460     23.81126\n  w.pubs_count\n1    8.5454545\n2    0.6666667\n3    0.2857143\n4    0.2000000\n5    0.4000000\n6    0.1666667\n\n\nAnd subsequently we use those new variables in a linear model.\n\nmod_1.lm &lt;- lm (log(med_house_price) ~ log(no2) + log(POPDEN) + \n                  per_mixed + per_asian + per_black + per_other +\n                  w.log_no2 + w.log_POPDEN +\n                  w.per_mixed + w.per_asian + w.per_black + w.per_other,\n                data = msoa.spdf)\nsummary(mod_1.lm)\n\n\nCall:\nlm(formula = log(med_house_price) ~ log(no2) + log(POPDEN) + \n    per_mixed + per_asian + per_black + per_other + w.log_no2 + \n    w.log_POPDEN + w.per_mixed + w.per_asian + w.per_black + \n    w.per_other, data = msoa.spdf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50809 -0.16605 -0.01817  0.13055  1.09039 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  10.582440   0.153862  68.779  &lt; 2e-16 ***\nlog(no2)     -0.440727   0.181063  -2.434  0.01511 *  \nlog(POPDEN)  -0.076840   0.017345  -4.430 1.05e-05 ***\nper_mixed    -0.033042   0.011298  -2.925  0.00353 ** \nper_asian    -0.002381   0.001474  -1.615  0.10655    \nper_black    -0.016229   0.001801  -9.009  &lt; 2e-16 ***\nper_other    -0.020391   0.006564  -3.107  0.00195 ** \nw.log_no2     0.993602   0.199370   4.984 7.38e-07 ***\nw.log_POPDEN  0.113262   0.028752   3.939 8.76e-05 ***\nw.per_mixed   0.126069   0.014294   8.820  &lt; 2e-16 ***\nw.per_asian  -0.003828   0.001661  -2.305  0.02140 *  \nw.per_black  -0.018054   0.002241  -8.056 2.30e-15 ***\nw.per_other   0.048139   0.007971   6.039 2.20e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2262 on 970 degrees of freedom\nMultiple R-squared:  0.653, Adjusted R-squared:  0.6487 \nF-statistic: 152.1 on 12 and 970 DF,  p-value: &lt; 2.2e-16\n\n\nLooks pretty similar to lmSLX() results, and it should! A big advantage of the SLX specification is that we can use the lagged variables in basically all methods which take variables as inputs, such as non-linear models, matching algorithms, and machine learning tools.\nMoreover, using the lagged variables gives a high degree of freedom. For instance, we could (not saying that it necessarily makes sense):\n\nUse different weights matrices for different variables\nInclude higher order neighbours using nblag() (with an increasing number of orders we go towards a more global model, but we estimate a coefficient for each spillover, instead of estimating just one)\nUse machine learning techniques to determine the best fitting weights specification.\n\n\n\n5.4.4 SDEM\nSDEM models can be estimated using errorsarlm() with the additional option Durbin = TRUE.\n\nmod_1.sdem &lt;- errorsarlm(log(med_house_price) ~ log(no2) + log(POPDEN) +\n                          per_mixed + per_asian + per_black + per_other,  \n                        data = msoa.spdf, \n                        listw = queens.lw,\n                        Durbin = TRUE) # we could here extend to SDEM\nsummary(mod_1.sdem)\n\n\nCall:\nerrorsarlm(formula = log(med_house_price) ~ log(no2) + log(POPDEN) + \n    per_mixed + per_asian + per_black + per_other, data = msoa.spdf, \n    listw = queens.lw, Durbin = TRUE)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.617795 -0.106380 -0.014832  0.095826  0.927446 \n\nType: error \nCoefficients: (asymptotic standard errors) \n                  Estimate Std. Error  z value  Pr(&gt;|z|)\n(Intercept)     10.4422703  0.3652148  28.5921 &lt; 2.2e-16\nlog(no2)        -0.2057493  0.1264914  -1.6266 0.1038248\nlog(POPDEN)     -0.0769743  0.0132094  -5.8272 5.635e-09\nper_mixed       -0.0222406  0.0079705  -2.7904 0.0052649\nper_asian       -0.0037484  0.0010054  -3.7284 0.0001927\nper_black       -0.0179751  0.0012383 -14.5161 &lt; 2.2e-16\nper_other       -0.0150218  0.0044895  -3.3460 0.0008199\nlag.log(no2)     1.0004491  0.1739833   5.7503 8.911e-09\nlag.log(POPDEN) -0.0054241  0.0327802  -0.1655 0.8685763\nlag.per_mixed    0.0669699  0.0169349   3.9545 7.668e-05\nlag.per_asian   -0.0018566  0.0015957  -1.1635 0.2446368\nlag.per_black   -0.0079949  0.0024833  -3.2195 0.0012842\nlag.per_other    0.0273378  0.0087430   3.1268 0.0017671\n\nLambda: 0.76173, LR test value: 455.7, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.024949\n    z-value: 30.531, p-value: &lt; 2.22e-16\nWald statistic: 932.15, p-value: &lt; 2.22e-16\n\nLog likelihood: 300.847 for error model\nML residual variance (sigma squared): 0.027504, (sigma: 0.16584)\nNumber of observations: 983 \nNumber of parameters estimated: 15 \nAIC: NA (not available for weighted model), (AIC for lm: -117.99)\n\n\nAnd this SDEM can be interpreted like a combination of SEM and SLX.\nFirst, we still see highly significant auto-correlation in the error term. However, it’s lower in magnitude now that we also include the \\(\\boldsymbol{\\mathbf{W}} X\\) terms.\nSecond, the coefficients tell a smimilar story as in the SLX (use the same interpretation), but some coefficient magnitudes have become smaller.\n\n\n5.4.5 SDM\nSDM models can be estimated using lagsarlm() with the additional option Durbin = TRUE.\n\nmod_1.sdm &lt;- lagsarlm(log(med_house_price) ~ log(no2) + log(POPDEN) + \n                        per_mixed + per_asian + per_black + per_other,  \n                      data = msoa.spdf, \n                      listw = queens.lw,\n                      Durbin = TRUE) # we could here extend to SDM\nsummary(mod_1.sdm)\n\n\nCall:\nlagsarlm(formula = log(med_house_price) ~ log(no2) + log(POPDEN) + \n    per_mixed + per_asian + per_black + per_other, data = msoa.spdf, \n    listw = queens.lw, Durbin = TRUE)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.614314 -0.107947 -0.013509  0.092234  0.917398 \n\nType: mixed \nCoefficients: (asymptotic standard errors) \n                  Estimate Std. Error  z value  Pr(&gt;|z|)\n(Intercept)      2.7843426  0.2944721   9.4554 &lt; 2.2e-16\nlog(no2)        -0.3112762  0.1308101  -2.3796 0.0173312\nlog(POPDEN)     -0.0802866  0.0125213  -6.4120 1.436e-10\nper_mixed       -0.0368998  0.0081596  -4.5223 6.118e-06\nper_asian       -0.0033726  0.0010636  -3.1711 0.0015189\nper_black       -0.0159770  0.0013006 -12.2848 &lt; 2.2e-16\nper_other       -0.0209743  0.0047369  -4.4279 9.516e-06\nlag.log(no2)     0.4880923  0.1456778   3.3505 0.0008067\nlag.log(POPDEN)  0.0781188  0.0207600   3.7629 0.0001679\nlag.per_mixed    0.0640880  0.0104646   6.1243 9.110e-10\nlag.per_asian    0.0017665  0.0012101   1.4598 0.1443498\nlag.per_black    0.0070487  0.0017938   3.9295 8.511e-05\nlag.per_other    0.0284822  0.0057774   4.9299 8.226e-07\n\nRho: 0.73126, LR test value: 501.83, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.025889\n    z-value: 28.246, p-value: &lt; 2.22e-16\nWald statistic: 797.86, p-value: &lt; 2.22e-16\n\nLog likelihood: 323.9111 for mixed model\nML residual variance (sigma squared): 0.026633, (sigma: 0.1632)\nNumber of observations: 983 \nNumber of parameters estimated: 15 \nAIC: -617.82, (AIC for lm: -117.99)\nLM test for residual autocorrelation\ntest value: 36.704, p-value: 1.3747e-09\n\n\nAnd this SDM can be interpreted like a combination of SAR and SLX.\nFirst, there’s still substantial auto-correlation in \\(\\boldsymbol{\\mathbf{y}}\\), and this has become even stronger as compared to SAR.\nSecond, we can interpret the direction of the effect, but we cannot interpret the coefficient as marginal effects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBetz, Timm, Scott J. Cook, and Florian M. Hollenbach. 2020. “Spatial Interdependence and Instrumental Variable Models.” Political Science Research and Methods 8 (4): 646–61. https://doi.org/10.1017/psrm.2018.61.\n\n\nCook, Scott J., Jude C. Hays, and Robert J. Franzese. 2020. “Model Specification and Spatial Interdependence.” In The Sage Handbook of Research Methods in Political Science and International Relations, edited by Luigi Curini and Robert Franzese, 1st ed, 730–47. Thousand Oaks: SAGE Inc.\n\n\nFranzese, Robert J., and Jude C. Hays. 2007. “Spatial Econometric Models of Cross-Sectional Interdependence in Political Science Panel and Time-Series-Cross-Section Data.” Political Analysis 15 (2): 140–64. https://doi.org/10.1093/pan/mpm005.\n\n\nGibbons, Steve, and Henry G. Overman. 2012. “Mostly Pointless Spatial Econometrics?” Journal of Regional Science 52 (2): 172–91. https://doi.org/10.1111/j.1467-9787.2012.00760.x.\n\n\nHalleck Vega, Solmaria, and J. Paul Elhorst. 2015. “The SLX Model.” Journal of Regional Science 55 (3): 339–63. https://doi.org/10.1111/jors.12188.\n\n\nKelejian, Harry H., and Gianfranco Piras. 2017. Spatial Econometrics. Elsevier. https://doi.org/10.1016/C2016-0-04332-2.\n\n\nLeSage, James P. 2014. “What Regional Scientists Need to Know about Spatial Econometrics.” The Review of Regional Studies 44 (1): 13–32. https://doi.org/https://dx.doi.org/10.2139/ssrn.2420725.\n\n\nLeSage, James P., and R. Kelley Pace. 2009. Introduction to Spatial Econometrics. Statistics, Textbooks and Monographs. Boca Raton: CRC Press.\n\n\nPace, R. Kelley, and James P. LeSage. 2010. “Omitted Variable Biases of OLS and Spatial Lag Models.” In Progress in Spatial Analysis, edited by Antonio Páez, Julie Gallo, Ron N. Buliung, and Sandy Dall’erba, 17–28. Berlin and Heidelberg: Springer.\n\n\nRüttenauer, Tobias. 2022. “Spatial Regression Models: A Systematic Comparison of Different Model Specifications Using Monte Carlo Experiments.” Sociological Methods & Research 51 (2): 728–59. https://doi.org/10.1177/0049124119882467.\n\n\nWimpy, Cameron, Guy D. Whitten, and Laron K. Williams. 2021. “X Marks the Spot: Unlocking the Treasure of Spatial-X Models.” The Journal of Politics 83 (2): 722–39. https://doi.org/10.1086/710089.\n\n\nWooldridge, Jeffrey M. 2010. Econometric Analysis of Cross Section and Panel Data. Cambridge, Mass.: MIT Press."
  },
  {
    "objectID": "07_impacts.html#coefficient-estimates-neq-marginal-effects",
    "href": "07_impacts.html#coefficient-estimates-neq-marginal-effects",
    "title": "\n7  Spatial Impacts\n",
    "section": "\n7.1 Coefficient estimates \\(\\neq\\) `marginal’ effects",
    "text": "7.1 Coefficient estimates \\(\\neq\\) `marginal’ effects\n\n\n\n\n\n\nWarning\n\n\n\nDo not interpret coefficients as marginal effects in SAR, SAC, and SDM!!\n\n\nAt first glance, the specifications presented above seem relatively similar in the way of modelling spatial effects. Yet, they differ in very important aspects.\nFirst, models with an endogenous spatial term (SAR, SAC, and SDM) assume a very different spatial dependence structure than models with only exogenous spatial terms as SLX and SDEM specifications. While the first three assume global spatial dependence, the second two assume local spatial dependence (Anselin 2003; Halleck Vega and Elhorst 2015; LeSage and Pace 2009).\nSecond, the interpretation of the coefficients differs greatly between models with and without endogenous effects. This becomes apparent when considering the reduced form of the equations above. Exemplary using the SAR model, the reduced form is given by:\n\\[\n\\begin{split}\n{\\boldsymbol{\\mathbf{y}}}-\\rho{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{y}}} &={\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{\\varepsilon}}}, \\nonumber \\\\\n({\\boldsymbol{\\mathbf{I}}_N}-\\rho {\\boldsymbol{\\mathbf{W}}}){\\boldsymbol{\\mathbf{y}}} &={\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{\\varepsilon}}}\\nonumber, \\\\\n{\\boldsymbol{\\mathbf{y}}} &=({\\boldsymbol{\\mathbf{I}}_N}-\\rho {\\boldsymbol{\\mathbf{W}}})^{-1}({\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{\\varepsilon}}}),\n\\end{split}\n\\]\nwhere \\({\\boldsymbol{\\mathbf{I}}_N}\\) is an \\(N \\times N\\) diagonal matrix (diagonal elements equal 1, 0 otherwise). This contains no spatially lagged dependent variable on the right-hand side.\nIf we want to interpret coefficient, we are usually in marginal or partial effects (the association between a unit change in \\(X\\) and \\(Y\\)). We obtain these effects by looking at the first derivative.\nWhen taking the first derivative of the explanatory variable \\({\\boldsymbol{\\mathbf{x}}}_k\\) from the reduced form in (\\(\\ref{eq:sarred}\\)) to interpret the partial effect of a unit change in variable \\({\\boldsymbol{\\mathbf{x}}}_k\\) on \\({\\boldsymbol{\\mathbf{y}}}\\), we receive\n\\[\n\\frac{\\partial {\\boldsymbol{\\mathbf{y}}}}{\\partial {\\boldsymbol{\\mathbf{x}}}_k}=\\underbrace{({\\boldsymbol{\\mathbf{I}}_N}-\\rho {\\boldsymbol{\\mathbf{W}}})^{-1}}_{N \\times N}\\beta_k,\n\\]\nfor each covariate \\(k=\\{1,2,...,K\\}\\). As can be seen, the partial derivative with respect to \\({\\boldsymbol{\\mathbf{x}}}_k\\) produces an \\(N \\times N\\) matrix, thereby representing the partial effect of each unit \\(i\\) onto the focal unit \\(i\\) itself and all other units .\nNote that the diagonal elements of \\(({\\boldsymbol{\\mathbf{I}}_N}-\\rho {\\boldsymbol{\\mathbf{W}}})^{-1}\\) are not zero anymore (as they are in \\(\\boldsymbol{\\mathbf{W}}\\)). Look at the following minimal example:\n\\[\n\\begin{split}\n\\tilde{\\boldsymbol{\\mathbf{W}}} = \\begin{pmatrix}\n      0 & 1 & 0 & 1 & 0 \\\\\n      1 & 0 & 1 & 0 & 1 \\\\\n      0 & 1 & 0 & 1 & 0 \\\\\n      1 & 0 & 1 & 0 & 1 \\\\\n      0 & 1 & 0 & 1 & 0\n      \\end{pmatrix}, \\mathrm{and~normalized} ~\n\\boldsymbol{\\mathbf{W}} = \\begin{pmatrix}\n      0 & 0.5 & 0 & 0.5 & 0 \\\\\n      0.33 & 0 & 0.33 & 0 & 0.33 \\\\\n      0 & 0.5 & 0 & 0.5 & 0 \\\\\n      0.33 & 0 & 0.33 & 0 & 0.33 \\\\\n      0 & 0.5 & 0 & 0.5 & 0\n      \\end{pmatrix}      \n\\end{split}\n\\]\nand\n\\[\n\\rho = 0.6,\n\\]\nthen\n\\[\n\\begin{split}\n\\rho \\boldsymbol{\\mathbf{W}} = \\begin{pmatrix}\n      0 & 0.3 & 0 & 0.3 & 0 \\\\\n      0.2 & 0 & 0.2 & 0 & 0.2 \\\\\n      0 & 0.3 & 0 & 0.3 & 0 \\\\\n      0.2 & 0 & 0.2 & 0 & 0.2 \\\\\n      0 & 0.3 & 0 & 0.3 & 0\n      \\end{pmatrix}.\n\\end{split}\n\\]\nIf we want to get the total effect of \\(X\\) on \\(Y\\) we need to add the direct association within \\(i\\) and \\(j\\) and so on…\n\\[\n\\begin{split}\n\\boldsymbol{\\mathbf{I}}_N - \\rho \\boldsymbol{\\mathbf{W}} &=\n\\begin{pmatrix}\n      1 & 0 & 0 & 0 & 0 \\\\\n      0 & 1 & 0 & 0 & 0 \\\\\n      0 & 0 & 1 & 0 & 0 \\\\\n      0 & 0 & 0 & 1 & 0 \\\\\n      0 & 1 & 0 & 0 & 1\n      \\end{pmatrix} -\n\\begin{pmatrix}\n      0 & 0.3 & 0 & 0.3 & 0 \\\\\n      0.2 & 0 & 0.2 & 0 & 0.2 \\\\\n      0 & 0.3 & 0 & 0.3 & 0 \\\\\n      0.2 & 0 & 0.2 & 0 & 0.2 \\\\\n      0 & 0.3 & 0 & 0.3 & 0\n      \\end{pmatrix}\\\\\n& = \\begin{pmatrix}\n      1 & -0.3 & 0 & -0.3 & 0 \\\\\n      -0.2 & 1 & -0.2 & 0 & -0.2 \\\\\n      0 & 0.3 & 1 & 0.3 & 0 \\\\\n      -0.2 & 0 & -0.2 & 1 & -0.2 \\\\\n      0 & -0.3 & 0 & -0.3 & 1\n      \\end{pmatrix}.\n\\end{split}\n\\]\nAnd finally we take the inverse of that\n\\[\n\\begin{split}\n(\\boldsymbol{\\mathbf{I}}_N - \\rho \\boldsymbol{\\mathbf{W}})^{-1} &=\n\\begin{pmatrix}\n      1 & -0.3 & 0 & -0.3 & 0 \\\\\n      -0.2 & 1 & -0.2 & 0 & -0.2 \\\\\n      0 & 0.3 & 1 & 0.3 & 0 \\\\\n      -0.2 & 0 & -0.2 & 1 & -0.2 \\\\\n      0 & -0.3 & 0 & -0.3 & 1\n      \\end{pmatrix}^{-1}\\\\\n&=\n\\begin{pmatrix}\n      \\color{red}{1.1875} & 0.46875 & 0.1875 & 0.46875 & 0.1875 \\\\\n      0.3125 & \\color{red}{1.28125} & 0.3125 & 0.28125 & 0.3125 \\\\\n      0.1875 & 0.46875 & \\color{red}{1.1875} & 0.46875 & 0.1875 \\\\\n      0.3125 & 0.28125 & 0.3125 & \\color{red}{1.28125} & 0.3125 \\\\\n      0.1875 & 0.46875 & 0.1875 & 0.46875 & \\color{red}{1.1875}\n      \\end{pmatrix}.\n\\end{split}\n\\]\nAs you can see, \\((\\boldsymbol{\\mathbf{I}}_N - \\rho \\boldsymbol{\\mathbf{W}})^{-1}\\) has \\(&gt;1\\): these are feedback loops. My \\(X\\) influences my \\(Y\\) directly, but my \\(Y\\) then influences my neigbour’s \\(Y\\), which then influences my \\(Y\\) again (also also other neighbour’s \\(Y\\)s). Thus the influence of my \\(X\\) on my \\(Y\\) includes a spatial multiplier.\nCheck yourself:\n\nI = diag(5)\nrho = 0.6\nW = matrix(c(0 , 0.5 , 0 , 0.5 , 0,\n            1/3 , 0 , 1/3 , 0 , 1/3,\n            0 , 0.5 , 0 , 0.5 , 0,\n            1/3 , 0 , 1/3 , 0 , 1/3,\n            0 , 0.5 , 0 , 0.5 , 0), ncol = 5, byrow = TRUE)\n\n(IrW = I - rho*W)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  1.0 -0.3  0.0 -0.3  0.0\n[2,] -0.2  1.0 -0.2  0.0 -0.2\n[3,]  0.0 -0.3  1.0 -0.3  0.0\n[4,] -0.2  0.0 -0.2  1.0 -0.2\n[5,]  0.0 -0.3  0.0 -0.3  1.0\n\n# (I - rho*W)^-1\n(M = solve(IrW))\n\n       [,1]    [,2]   [,3]    [,4]   [,5]\n[1,] 1.1875 0.46875 0.1875 0.46875 0.1875\n[2,] 0.3125 1.28125 0.3125 0.28125 0.3125\n[3,] 0.1875 0.46875 1.1875 0.46875 0.1875\n[4,] 0.3125 0.28125 0.3125 1.28125 0.3125\n[5,] 0.1875 0.46875 0.1875 0.46875 1.1875\n\n\nThe diagonal elements of \\(M\\) indicate how each unit \\(i\\) influences itself (change of \\(x_i\\) on change of \\(y_i\\)), and each off-diagonal elements in column \\(j\\) represents the effect of \\(j\\) on each other unit \\(i\\) (change of \\(x_j\\) on change of \\(y_i\\)).\n\\[\n\\begin{split}\n\\begin{pmatrix}\n      1.1875 & \\color{red}{0.46875} & 0.1875 & 0.46875 & 0.1875 \\\\\n      0.3125 & 1.28125 & 0.3125 & 0.28125 & 0.3125 \\\\\n      0.1875 & 0.46875 & 1.1875 & 0.46875 & 0.1875 \\\\\n      0.3125 & 0.28125 & 0.3125 & 1.28125 & 0.3125 \\\\\n      0.1875 & 0.46875 & \\color{blue}{0.1875} & 0.46875 & 1.1875\n      \\end{pmatrix}.\n\\end{split}\n\\]\nFor instance, \\(\\color{red}{W_{12}}\\) indicates that unit 2 has an influence of 0.46875 on unit 1. On the other hand, \\(\\color{blue}{W_{53}}\\) indicates that unit 3 has an influence of magnitude 0.1875 on unit 5.\n\n\n\n\n\n\nQuestion\n\n\n\nWhy does unit 3 have any effect o unit 5? According to \\(\\boldsymbol{\\mathbf{W}}\\) those two units are no neighbours \\(w_{53} = 0\\)!"
  },
  {
    "objectID": "07_impacts.html#global-and-local-spillovers",
    "href": "07_impacts.html#global-and-local-spillovers",
    "title": "\n7  Spatial Impacts\n",
    "section": "\n7.2 Global and local spillovers",
    "text": "7.2 Global and local spillovers\nThe kind of indirect spillover effects in SAR, SAC, and SDM models differs from the kind of indirect spillover effects in SLX and SDEM models: while the first three specifications represent global spillover effects, the latter three represent local spillover effects (Anselin 2003; LeSage and Pace 2009; LeSage 2014).\n\n7.2.1 Local spillovers\nIn case of SLX and SDEM the spatial spillover effects can be interpreted as the effect of a one unit change of \\({\\boldsymbol{\\mathbf{x}}}_k\\) in the spatially weighted neighbouring observations on the dependent variable of the focal unit: the weighted average among neighbours; when using a row-normalised contiguity weights matrix, \\({\\boldsymbol{\\mathbf{W}}} {\\boldsymbol{\\mathbf{x}}}_k\\) is the mean value of \\({\\boldsymbol{\\mathbf{x}}}_k\\) in the neighbouring units.\nAssume we have \\(k =2\\) covariates, then\n\\[\n\\begin{split}\n\\underbrace{\\boldsymbol{\\mathbf{W}}}_{N \\times N}  \\underbrace{\\boldsymbol{\\mathbf{X}}}_{N \\times 2} \\underbrace{\\boldsymbol{\\mathbf{\\theta}}}_{2 \\times 1} & =\n\\begin{pmatrix}\n      0 & 0.5 & 0 & 0.5 & 0 \\\\\n      0.33 & 0 & 0.33 & 0 & 0.33 \\\\\n      0 & 0.5 & 0 & 0.5 & 0 \\\\\n      0.33 & 0 & 0.33 & 0 & 0.33 \\\\\n      0 & 0.5 & 0 & 0.5 & 0\n  \\end{pmatrix}\n  \\begin{pmatrix}\n      3 & 100 \\\\\n      4 & 140 \\\\\n      1 & 200 \\\\\n      7 & 70  \\\\\n      5 & 250\n  \\end{pmatrix}\n    \\begin{pmatrix}\n      \\theta_1 \\\\\n      \\theta_2\n  \\end{pmatrix}\\\\\n& =   \n\\begin{pmatrix}\n      6 & 105 \\\\\n      3 & 190 \\\\\n      6 & 105 \\\\\n      3 & 190  \\\\\n      6 & 105\n  \\end{pmatrix}\n\\begin{pmatrix}\n      \\theta_1 \\\\\n      \\theta_2\n  \\end{pmatrix}\\\\\n\\end{split}\n\\]\n\nX &lt;- cbind(x1 = c(3,4,1,8,5),\n           x2 = c(100,140,200,70,270))\n(WX &lt;-  W %*% X)\n\n     x1  x2\n[1,]  6 105\n[2,]  3 190\n[3,]  6 105\n[4,]  3 190\n[5,]  6 105\n\n\nThus, only direct neighbours – as defined in \\({\\boldsymbol{\\mathbf{W}}}\\) – contribute to those local spillover effects. The \\(\\hat{\\boldsymbol{\\mathbf{\\theta}}}\\) coefficients only estimate how my direct neighbour’s \\(\\boldsymbol{\\mathbf{X}}\\) values influence my own outcome \\(\\boldsymbol{\\mathbf{y}}\\).\nThere are no higher order neighbours involved (as long as we do not model them), nor are there any feedback loops due to interdependence.\n\n7.2.2 Global spillovers\nIn contrast, spillover effects in SAR, SAC, and SDM models do not only include direct neighbours but also neighbours of neighbours (second order neighbours) and further higher-order neighbours. This can be seen by rewriting the inverse \\(({\\boldsymbol{\\mathbf{I}}_N}-\\rho {\\boldsymbol{\\mathbf{W}}})^{-1}\\) as power series:A power series of \\(\\sum\\nolimits_{k=0}^\\infty {\\boldsymbol{\\mathbf{W}}}^k\\) converges to \\(({\\boldsymbol{\\mathbf{I}}}-{\\boldsymbol{\\mathbf{W}}})^{-1}\\) if the maximum absolute eigenvalue of \\({\\boldsymbol{\\mathbf{W}}} &lt; 1\\), which is ensured by standardizing \\({\\boldsymbol{\\mathbf{W}}}\\).}\n\\[\n\\begin{split}\n({\\boldsymbol{\\mathbf{I}}_N}-\\rho {\\boldsymbol{\\mathbf{W}}})^{-1}\\beta_k\n=({\\boldsymbol{\\mathbf{I}}_N} + \\rho{\\boldsymbol{\\mathbf{W}}} + \\rho^2{\\boldsymbol{\\mathbf{W}}}^2 + \\rho^3{\\boldsymbol{\\mathbf{W}}}^3 + ...)\\beta_k\n= ({\\boldsymbol{\\mathbf{I}}_N} + \\sum_{h=1}^\\infty \\rho^h{\\boldsymbol{\\mathbf{W}}}^h)\\beta_k ,\n\\end{split}\n\\]\nwhere the identity matrix represents the direct effects and the sum represents the first and higher order indirect effects and the above mentioned feedback loops. This implies that a change in one unit \\(i\\) does not only affect the direct neighbours but passes through the whole system towards higher-order neighbours, where the impact declines with distance within the neighbouring system. Global indirect impacts thus are `multiplied’ by influencing direct neighbours as specified in \\(\\boldsymbol{\\mathbf{W}}\\) and indirect neighbours not connected according to \\(\\boldsymbol{\\mathbf{W}}\\), with additional feedback loops between those neighbours.\n\\[\n\\begin{split}\n\\underbrace{(\\underbrace{\\boldsymbol{\\mathbf{I}}_N}_{N \\times N} - \\underbrace{\\rho}_{\\hat{=} 0.6} \\underbrace{\\boldsymbol{\\mathbf{W}}}_{N \\times N})^{-1}}_{N \\times N} \\beta_k\n&=\n\\begin{pmatrix}\n      1.\\color{red}{1875} & 0.46875 & 0.1875 & 0.46875 & 0.1875 \\\\\n      0.3125 & 1.\\color{red}{28125} & 0.3125 & 0.28125 & 0.3125 \\\\\n      0.1875 & 0.46875 & 1.\\color{red}{1875} & 0.46875 & 0.1875 \\\\\n      0.3125 & 0.28125 & 0.3125 & 1.\\color{red}{28125} & 0.3125 \\\\\n      0.1875 & 0.46875 & 0.1875 & 0.46875 & 1.\\color{red}{1875}\n      \\end{pmatrix}\n  \\begin{matrix}\n      (\\beta_1 + \\beta_2)\\\\\n  \\end{matrix}\\\\.\n\\end{split}\n\\]\nAll diagonal elements of \\(\\mathrm{diag}({\\boldsymbol{\\mathbf{W}}})=w_{ii}=0\\). However, diagonal elements of higher order neighbours are not zero \\(\\mathrm{diag}({\\boldsymbol{\\mathbf{W}}}^2)=\\mathrm{diag}({\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{W}}})\\neq0\\).\nIntuitively, \\(\\rho{\\boldsymbol{\\mathbf{W}}}\\) only represents the effects between direct neighbours (and the focal unit is not a neighbour of the focal unit itself), whereas \\(\\rho^2{\\boldsymbol{\\mathbf{W}}}^2\\) contains the effects of second order neighbours, where the focal unit is a second order neighbour of the focal unit itself. Thus, \\(({\\boldsymbol{\\mathbf{I}}_N}-\\rho {\\boldsymbol{\\mathbf{W}}})^{-1}\\beta_k\\) includes feedback effects from \\(\\rho^2{\\boldsymbol{\\mathbf{W}}}^2\\) on (they are part of the direct impacts according to the summary measures below). This is way the diagonal above \\(\\geq 1\\).\nIn consequence, local and global spillover effects represent two distinct kinds of spatial spillover effects (LeSage 2014). The interpretation of local spillover effects is straightforward: it represents the effect of all neighbours as defined by \\({\\boldsymbol{\\mathbf{W}}}\\) (the average over all neighbours in case of a row-normalised weights matrix).\nFor instance, the environmental quality in the focal unit itself but also in neighbouring units could influence the attractiveness of a district and its house prices. In this example it seems reasonable to assume that we have local spillover effects: only the environmental quality in directly contiguous units (e.g. in walking distance) is relevant for estimating the house prices.\nIn contrast, interpreting global spillover effects can be a bit more difficult. Intuitively, the global spillover effects can be seen as a kind of diffusion process. For example, an exogenous event might increase the house prices in one district of a city, thus leading to an adaptation of house prices in neighbouring districts, which then leads to further adaptations in other units (the neighbours of the neighbours), thereby globally diffusing the effect of the exogenous event due to the endogenous term.\nYet, those processes happen over time. In a cross-sectional framework, the global spillover effects are hard to interpret. Anselin (2003) proposes an interpretation as an equilibrium outcome, where the partial impact represents an estimate of how this long-run equilibrium would change due to a change in \\({\\boldsymbol{\\mathbf{x}}}_k\\) (LeSage 2014)."
  },
  {
    "objectID": "07_impacts.html#summary-impact-measures",
    "href": "07_impacts.html#summary-impact-measures",
    "title": "\n7  Spatial Impacts\n",
    "section": "\n7.3 Summary impact measures",
    "text": "7.3 Summary impact measures\nNote that the derivative in SAR, SAC, and SDM is a \\(N \\times N\\) matrix, returning individual effects of each unit on each other unit, differentiated in direct, indirect, and total impacts.\n\\[\n\\begin{split}\n(\\boldsymbol{\\mathbf{I}}_N - \\rho \\boldsymbol{\\mathbf{W}})^{-1} \\beta &=\n\\begin{pmatrix}\n      \\color{red}{1.1875} & \\color{blue}{0.46875} & \\color{blue}{0.1875} & \\color{blue}{0.46875} & \\color{blue}{0.1875} \\\\\n      \\color{blue}{0.3125} & \\color{red}{1.28125} & \\color{blue}{0.3125} & \\color{blue}{0.28125} & \\color{blue}{0.3125} \\\\\n      \\color{blue}{0.1875} & \\color{blue}{0.46875} & \\color{red}{1.1875} & \\color{blue}{0.46875} & \\color{blue}{0.1875} \\\\\n      \\color{blue}{0.3125} & \\color{blue}{0.28125} & \\color{blue}{0.3125} & \\color{red}{1.28125} & \\color{blue}{0.3125} \\\\\n      \\color{blue}{0.1875} & \\color{blue}{0.46875} & \\color{blue}{0.1875} & \\color{blue}{0.46875} & \\color{red}{1.1875}\n      \\end{pmatrix} \\beta\n\\end{split}\n\\]\nHowever, the individual effects (how \\(i\\) influences \\(j\\)) mainly vary because of variation in \\({\\boldsymbol{\\mathbf{W}}}\\). \n\n\n\n\n\n\nDo not interpret these as “estimated” individual impacts\n\n\n\nWe estimate two scalar parameters in a SAR model: \\(\\beta\\) for the direct coefficient and \\(rho\\) for the auto-regressive parameter.\nAll variation in the effects matrix \\((\\boldsymbol{\\mathbf{I}}_N - \\rho \\boldsymbol{\\mathbf{W}})^{-1}\\) comes from the relationship in \\(\\boldsymbol{\\mathbf{W}}\\) which we have given a-priori!\n\n\nSince reporting the individual partial effects is usually not of interest, LeSage and Pace (2009) proposed to average over these effect matrices. While the average diagonal elements of the effects matrix \\((\\boldsymbol{\\mathbf{I}}_N - \\rho \\boldsymbol{\\mathbf{W}})^{-1}\\) represent the so called direct impacts of variable \\({\\boldsymbol{\\mathbf{x}}}_k\\), the average column-sums of the off-diagonal elements represent the so called indirect impacts (or spatial spillover effects).\ndirect impacts refer to an average effect of a unit change in \\(x_i\\) on \\(y_i\\), and the indirect (spillover) impacts indicate how a change in \\(x_i\\), on average, influences all neighbouring units \\(y_j\\).\nThough previous literature (Halleck Vega and Elhorst 2015; LeSage and Pace 2009) has established the notation of direct and indirect impacts, it is important to note that also the direct impacts comprise a spatial `multiplier’ component if we specify an endogenous lagged depended variable, as a change in \\(\\boldsymbol{\\mathbf{x}}_i\\) influences \\(\\boldsymbol{\\mathbf{y}}_i\\), which influences \\(\\boldsymbol{\\mathbf{y}}_j\\), which in turn influences \\(\\boldsymbol{\\mathbf{y}}_i\\).\nUsually, one should use summary measures to report effects in spatial models (LeSage and Pace 2009). Halleck Vega and Elhorst (2015) provide a nice summary of the impacts for each model:\n\n\n\n\n\n\n\n\nModel\nDirect Impacts\nIndirect Impacts\ntype\n\n\n\nOLS/SEM\n\\(\\beta_k\\)\n–\n–\n\n\nSAR/SAC\n\nDiagonal elements of \\(({\\boldsymbol{\\mathbf{I}}}-\\rho{\\boldsymbol{\\mathbf{W}}})^{-1}\\beta_k\\)\n\n\nOff-diagonal elements of \\(({\\boldsymbol{\\mathbf{I}}}-\\rho{\\boldsymbol{\\mathbf{W}}})^{-1}\\beta_k\\)\n\nglobal\n\n\nSLX/SDEM\n\\(\\beta_k\\)\n\\(\\theta_k\\)\nlocal\n\n\nSDM\n\nDiagonal elements of \\(({\\boldsymbol{\\mathbf{I}}}-\\rho{\\boldsymbol{\\mathbf{W}}})^{-1}\\left[\\beta_k+{\\boldsymbol{\\mathbf{W}}}\\theta_k\\right]\\)\n\n\nOff-diagonal elements of \\(({\\boldsymbol{\\mathbf{I}}}-\\rho{\\boldsymbol{\\mathbf{W}}})^{-1}\\left[\\beta_k+{\\boldsymbol{\\mathbf{W}}}\\theta_k\\right]\\)\n\nglobal\n\n\n\n\\[\n\\begin{split}\n(\\boldsymbol{\\mathbf{I}}_N - \\rho \\boldsymbol{\\mathbf{W}})^{-1} \\beta &=\n\\begin{pmatrix}\n      \\color{red}{1.1875} & \\color{blue}{0.46875} & \\color{blue}{0.1875} & \\color{blue}{0.46875} & \\color{blue}{0.1875} \\\\\n      \\color{blue}{0.3125} & \\color{red}{1.28125} & \\color{blue}{0.3125} & \\color{blue}{0.28125} & \\color{blue}{0.3125} \\\\\n      \\color{blue}{0.1875} & \\color{blue}{0.46875} & \\color{red}{1.1875} & \\color{blue}{0.46875} & \\color{blue}{0.1875} \\\\\n      \\color{blue}{0.3125} & \\color{blue}{0.28125} & \\color{blue}{0.3125} & \\color{red}{1.28125} & \\color{blue}{0.3125} \\\\\n      \\color{blue}{0.1875} & \\color{blue}{0.46875} & \\color{blue}{0.1875} & \\color{blue}{0.46875} & \\color{red}{1.1875}\n      \\end{pmatrix} \\beta\n\\end{split}\n\\]\nThe different indirect effects / spatial effects mean conceptually different things:\n\nGlobal spillover effects: SAR, SAC, SDM\nLocal spillover effects: SLX, SDEM\n\n\n\n\n\n\n\nCommon ratio between direct and indirect impacts in SAR and SAC\n\n\n\nNote that impacts in SAR only estimate one single spatial multiplier coefficient. Thus direct and indirect impacts are bound to a common ratio, say \\(\\phi\\), across all covariates.\nif \\(\\beta_1^{direct} = \\phi\\beta_1^{indirect}\\), then \\(\\beta_2^{direct} = \\phi\\beta_2^{indirect}\\), \\(\\beta_k^{direct} = \\phi\\beta_k^{indirect}\\).\n\n\nWe can calculate these impacts using impacts() with simulated distributions, e.g. for the SAR model:\n\nmod_1.sar.imp &lt;- impacts(mod_1.sar, listw = queens.lw, R = 300)\nsummary(mod_1.sar.imp, zstats = TRUE, short = TRUE)\n\nImpact measures (lag, exact):\n                  Direct     Indirect        Total\nlog(no2)     0.447853184  0.754466618  1.202319802\nlog(POPDEN) -0.062973027 -0.106086209 -0.169059236\nper_mixed    0.020884672  0.035182931  0.056067603\nper_asian   -0.002575602 -0.004338934 -0.006914536\nper_black   -0.014253206 -0.024011369 -0.038264575\nper_other   -0.001820705 -0.003067212 -0.004887917\n========================================================\nSimulation results ( variance matrix):\n========================================================\nSimulated standard errors\n                  Direct     Indirect       Total\nlog(no2)    0.0536003318 0.0990634779 0.141350664\nlog(POPDEN) 0.0142688313 0.0266256548 0.040024687\nper_mixed   0.0064627371 0.0116836166 0.017928737\nper_asian   0.0005076349 0.0008471775 0.001310367\nper_black   0.0009608325 0.0023226340 0.002762977\nper_other   0.0036215066 0.0061434841 0.009753437\n\nSimulated z-values:\n                 Direct    Indirect       Total\nlog(no2)      8.3920278   7.6554574   8.5474782\nlog(POPDEN)  -4.4790128  -4.0615635  -4.2986487\nper_mixed     3.2050580   3.0044281   3.1132161\nper_asian    -5.1054662  -5.1472654  -5.3056592\nper_black   -14.8525625 -10.3707376 -13.8829437\nper_other    -0.3517062  -0.3538222  -0.3534556\n\nSimulated p-values:\n            Direct     Indirect   Total     \nlog(no2)    &lt; 2.22e-16 1.9318e-14 &lt; 2.22e-16\nlog(POPDEN) 7.4989e-06 4.8745e-05 1.7184e-05\nper_mixed   0.0013504  0.0026608  0.0018506 \nper_asian   3.2998e-07 2.6431e-07 1.1227e-07\nper_black   &lt; 2.22e-16 &lt; 2.22e-16 &lt; 2.22e-16\nper_other   0.7250586  0.7234721  0.7237469 \n\n# Alternative with traces (better for large W)\nW &lt;- as(queens.lw, \"CsparseMatrix\")\ntrMatc &lt;- trW(W, type = \"mult\",\n              m = 30) # number of powers\nmod_1.sar.imp2 &lt;- impacts(mod_1.sar, \n                          tr = trMatc, # trace instead of listw\n                          R = 300, \n                          Q = 30) # number of power series used for approximation\nsummary(mod_1.sar.imp2, zstats = TRUE, short = TRUE)\n\nImpact measures (lag, trace):\n                  Direct     Indirect        Total\nlog(no2)     0.447853101  0.754459497  1.202312598\nlog(POPDEN) -0.062973015 -0.106085208 -0.169058223\nper_mixed    0.020884668  0.035182599  0.056067267\nper_asian   -0.002575601 -0.004338893 -0.006914494\nper_black   -0.014253203 -0.024011142 -0.038264346\nper_other   -0.001820704 -0.003067183 -0.004887888\n========================================================\nSimulation results ( variance matrix):\n========================================================\nSimulated standard errors\n                  Direct     Indirect       Total\nlog(no2)    0.0492241557 0.0900280932 0.124415128\nlog(POPDEN) 0.0134108898 0.0251068945 0.037465583\nper_mixed   0.0063695185 0.0116938994 0.017821103\nper_asian   0.0005077299 0.0008374614 0.001293851\nper_black   0.0009900108 0.0023352014 0.002705719\nper_other   0.0036219261 0.0062071610 0.009811576\n\nSimulated z-values:\n                 Direct    Indirect       Total\nlog(no2)      9.0899391   8.3558343   9.6427535\nlog(POPDEN)  -4.6603238  -4.2017841  -4.4839244\nper_mixed     3.2763510   3.0212239   3.1534898\nper_asian    -5.1099362  -5.1971745  -5.3691669\nper_black   -14.4607226 -10.3214074 -14.1991203\nper_other    -0.4941047  -0.4993335  -0.4982945\n\nSimulated p-values:\n            Direct     Indirect   Total     \nlog(no2)    &lt; 2.22e-16 &lt; 2.22e-16 &lt; 2.22e-16\nlog(POPDEN) 3.1571e-06 2.6482e-05 7.3283e-06\nper_mixed   0.0010516  0.0025176  0.0016133 \nper_asian   3.2227e-07 2.0234e-07 7.9101e-08\nper_black   &lt; 2.22e-16 &lt; 2.22e-16 &lt; 2.22e-16\nper_other   0.6212322  0.6175444  0.6182765 \n\n\nThe indirect effects in SAR, SAC, and SDM refer to global spillover effects. This means a change of \\(x\\) in the focal units flows through the entire system of neighbours (direct nieightbours, neighbours of neighbours, …) influencing ‘their \\(y\\)’. One can think of this as diffusion or a change in a long-term equilibrium.\nIf Log NO2 increases by one unit, this increases the house price in the focal unit by 0.448 units. Overall, a one unit change in log NO2 increases the house prices in the entire neighbourhood system (direct and higher order neighbours) by 0.754.\nFor SLX models, nothing is gained from computing the impacts, as they equal the coefficients. Again, it’s the effects of direct neighbours only.\n\nprint(impacts(mod_1.slx, listw = queens.lw))\n\nImpact measures (SlX, glht):\n                  Direct     Indirect        Total\nlog(no2)    -0.440727458  0.993602103  0.552874645\nlog(POPDEN) -0.076839828  0.113262218  0.036422390\nper_mixed   -0.033042221  0.126068686  0.093026466\nper_asian   -0.002380698 -0.003828126 -0.006208824\nper_black   -0.016229407 -0.018053503 -0.034282910\nper_other   -0.020391354  0.048139008  0.027747654"
  },
  {
    "objectID": "08_exercise1.html",
    "href": "08_exercise1.html",
    "title": "8  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee (knuth84?) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nblablabla"
  },
  {
    "objectID": "09_comparison.html#general-to-specific-approach",
    "href": "09_comparison.html#general-to-specific-approach",
    "title": "9  Comparing and Selecting Models",
    "section": "9.2 General-to-specific approach",
    "text": "9.2 General-to-specific approach\nThe general-to-specific approach depicts the opposite method of specification search. This approach starts with the most general model and stepwise imposes restrictions on the parameters of this general model.\n\n\n\nHalleck Vega and Elhorst (2015): Nesting of different Spatial Econometric Model Specifications\n\n\nIn theory, we would\n\nstart with a GNS specification and\nsubsequently restrict the model to simplified specifications based on the significance of parameters in the GNS.\n\nThe problem with this strategy is that the GNS is only weakly identified and, thus, is of little help in selecting the correct restrictions .\nThe most intuitive alternative would be to start with one of the two-source models SDM, SDEM, or SAC. This, however, bears the risk of imposing the wrong restriction in the first place (Cook, Hays, and Franzese 2020). Furthermore, Cook, Hays, and Franzese (2020) show that more complicated restrictions are necessary to derive all single-source models from SDEM or SAC specifications."
  },
  {
    "objectID": "09_comparison.html#specific-to-general",
    "href": "09_comparison.html#specific-to-general",
    "title": "9  Comparing and Selecting Models",
    "section": "9.1 Specific-to-general",
    "text": "9.1 Specific-to-general\nThe specific-to-general approach is more common in spatial econometrics. This approach starts with the most basic non-spatial model and tests for possible misspecifications due to omitted autocorrelation in the error term or the dependent variable.\nAnselin et al. (1996) proposed to use Lagrange multiplier (LM) tests for the hypotheses \\(H_0\\): \\(\\lambda=0\\) and \\(H_0\\): \\(\\rho=0\\), which are robust against the alternative source of spatial dependence.\n\n9.1.1 Lagrange Multiplier Test\nWe have earlier talked about methods to detect auto-correlation – visualisation and Moran’s I. Both methodscan tell us that there is spatial autocorrelation. However, both method do not provide any information on why there is autocorrelation. Possible reasons:\n\nInterdependence (\\(\\rho\\))\nClustering on unobservables (\\(\\lambda\\))\nSpillovers in covariates (\\(\\boldsymbol{\\mathbf{\\theta}}\\))\n\nLagrange Multiplier test (Anselin et al. 1996):\n\n(Robust) test for spatial lag dependence \\(LM_\\rho^*\\)\n(Robust) test for spatial error dependence \\(LM_\\lambda^*\\)\n\nRobust test for lag dependence: \\(H_0\\): \\(\\rho=0\\) \\[\n        LM_\\rho^* = G^{-1} \\hat{\\sigma}_\\epsilon^2\n        \\big(\\frac{ \\hat{\\boldsymbol{\\mathbf{\\epsilon}}}^\\intercal \\boldsymbol{\\mathbf{Wy}}}{\\hat{\\sigma}_\\epsilon^2}\n        - \\frac{\\hat{\\boldsymbol{\\mathbf{\\epsilon}}}^\\intercal \\boldsymbol{\\mathbf{W\\hat{\\epsilon}}}}{\\hat{\\sigma}_\\epsilon^2} \\big)^2 \\sim \\chi^2\n\\] \nRobust test for error dependence: \\(H_0\\): \\(\\lambda=0\\)\n\\[\n        LM_\\lambda^* = \\frac{\n        \\big( \\hat{\\boldsymbol{\\mathbf{\\epsilon}}}^\\intercal \\boldsymbol{\\mathbf{W\\hat{\\epsilon}}} / \\hat{\\sigma}_\\epsilon^2        \n        - [T\\hat{\\sigma}_\\epsilon^2(G + T\\hat{\\sigma}_\\epsilon^2)^{-1}]\n         \\hat{\\boldsymbol{\\mathbf{\\epsilon}}}^\\intercal \\boldsymbol{\\mathbf{Wy}} / \\hat{\\sigma}_\\epsilon^2 \\big)^2\n        }{\n        T[1 - \\frac{\\hat{\\sigma}_\\epsilon^2}{G + \\hat{\\sigma}_\\epsilon^2}]\n        } \\sim \\chi^2\n\\] with \\[\n\\begin{split}\n     G &= (\\boldsymbol{\\mathbf{WX\\hat{\\beta}}})^\\intercal (\\boldsymbol{\\mathbf{I}} - \\boldsymbol{\\mathbf{X}} (\\boldsymbol{\\mathbf{X}}^\\intercal\\boldsymbol{\\mathbf{X}})^{-1} \\boldsymbol{\\mathbf{X}}^\\intercal) (\\boldsymbol{\\mathbf{WX\\hat{\\beta}}})   \\\\\n     T &= \\mathrm{tr}[(\\boldsymbol{\\mathbf{W}}^\\intercal + \\boldsymbol{\\mathbf{W}})\\boldsymbol{\\mathbf{W}}],\n\\end{split}     \n\\] where \\(\\mathrm{tr}(\\boldsymbol{\\mathbf{A}})\\) is the sum of the main diagonal of any square matrix \\(\\boldsymbol{\\mathbf{A}}\\).\n\n\n9.1.2 Problem\nThe specific-to-general approach based on the robust LM test offers a good performance in distinguishing between SAR, SEM, and non-spatial OLS (Florax, Folmer, and Rey 2003).\nStill, in their original paper, Anselin et al. (1996) already note the declining power of the robust LM\\(_\\lambda\\) test for spatial error dependence with increasing autocorrelation in the dependent variable (indicating some uncertainty under a SAC-like DGP).\nMur and Angulo (2009) demonstrate strong drawbacks of the specific-to-general approach under non-optimal conditions like heteroscedasticity or endogeneity.\nMoreover, the test disregard the presence of spatial dependence from local spillover effects (\\(\\theta\\) is assumed to be zero), as resulting from an SLX-like process. Cook, Hays, and Franzese (2020), for instance, show theoretically that an SLX-like dependence structure leads to the rejection of both hypotheses \\(H_0\\): \\(\\lambda=0\\) and \\(H_0\\): \\(\\rho=0\\), though no autocorrelation is present (Elhorst and Halleck Vega 2017; Rüttenauer 2022)."
  },
  {
    "objectID": "09_comparison.html#monte-carlo-simulation",
    "href": "09_comparison.html#monte-carlo-simulation",
    "title": "9  Comparing and Selecting Models",
    "section": "9.5 Monte Carlo simulation",
    "text": "9.5 Monte Carlo simulation\nThis section discusses results from Rüttenauer (2022). The aim: how do different spatial models perform under different scenarios?\nThe DGP of the Monte Carlo simulation follows a GNS, where \\({\\boldsymbol{\\mathbf{\\upsilon}}}_k\\) and \\({\\boldsymbol{\\mathbf{\\varepsilon}}}\\) are independent and randomly distributed \\(\\mathcal{N}(0,\\sigma^{2}_\\upsilon)\\) and \\(\\mathcal{N}(0,\\sigma^{2}_\\varepsilon)\\) with a mean of zero, and \\({\\boldsymbol{\\mathbf{x}}}_k\\) is the \\(k\\)th column-vector of \\({\\boldsymbol{\\mathbf{X}}}\\) for \\(k=1,...,K\\) covariates (\\(K\\) is fixed at \\(2\\) in the simulations). The parameter \\(\\rho\\) represents the autocorrelation in the dependent variable, \\(\\lambda\\) the autocorrelation in the disturbances, and \\(\\delta_k\\) the autocorrelation in covariate \\(k\\).\n\\[\n\\begin{split}\n{\\boldsymbol{\\mathbf{y}}}&=\\rho{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{y}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\theta}}}+ {\\boldsymbol{\\mathbf{u}}},\\\\\n{\\boldsymbol{\\mathbf{u}}}&=\\lambda{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{u}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\gamma}}}+{\\boldsymbol{\\mathbf{\\varepsilon}}},\\\\\n{\\boldsymbol{\\mathbf{x}}}_k&=\\delta_k{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{x}}}_k+{\\boldsymbol{\\mathbf{\\upsilon}}}_k.\n\\end{split}\n\\] The parameter-vector \\({\\boldsymbol{\\mathbf{\\gamma}}}\\) specifies the correlation between \\({\\boldsymbol{\\mathbf{x}}}\\) and the disturbance vector \\({\\boldsymbol{\\mathbf{u}}}\\), thereby defining the strength of an omitted variable bias. In reduced form, this DGP can be written as \\[\n\\begin{split}\n{\\boldsymbol{\\mathbf{y}}}=&({\\boldsymbol{\\mathbf{I}}_N}-\\rho {\\boldsymbol{\\mathbf{W}}})^{-1}\\big[({\\boldsymbol{\\mathbf{I}}_N}-\\delta_k {\\boldsymbol{\\mathbf{W}}})^{-1}{\\boldsymbol{\\mathbf{\\upsilon}}_k}\\beta_k \\\\\n&+{\\boldsymbol{\\mathbf{W}}}({\\boldsymbol{\\mathbf{I}}_N}-\\delta_k {\\boldsymbol{\\mathbf{W}}})^{-1}{\\boldsymbol{\\mathbf{\\upsilon}}_k}\\theta_k \\\\\n&+({\\boldsymbol{\\mathbf{I}}_N}-\\lambda {\\boldsymbol{\\mathbf{W}}})^{-1}(({\\boldsymbol{\\mathbf{I}}_N}-\\delta_k {\\boldsymbol{\\mathbf{W}}})^{-1}{\\boldsymbol{\\mathbf{\\upsilon}}_k}\\gamma_k+{\\boldsymbol{\\mathbf{\\varepsilon}}})\\big].\n\\end{split}\n\\]\nThe parameter vector \\({\\boldsymbol{\\mathbf{\\beta}}}\\) was fixed at \\({\\boldsymbol{\\mathbf{\\beta}}}=%\n\\begin{pmatrix}0.2&0.5\\end{pmatrix}\n^\\intercal\\), and the noise parameters were fixed at \\(\\sigma^{2}_\\upsilon\\), \\(\\sigma^{2}_\\varepsilon=1\\) for all trials. All other parameters vary between the following two options for each parameter (vector):\n\n\\(\\rho \\in \\left\\{ 0, 0.5\\right\\}\\),\n\\(\\lambda \\in \\{0, 0.5\\}\\),\n\\({\\boldsymbol{\\mathbf{\\delta}}} \\in \\left\\{ %\n\\begin{pmatrix}0&0\\end{pmatrix}\n^\\intercal, %\n\\begin{pmatrix}0.4&0.7\\end{pmatrix}\n^\\intercal\\right\\}\\),\n\\({\\boldsymbol{\\mathbf{\\theta}}} \\in \\left\\{%\n\\begin{pmatrix}0&0\\end{pmatrix}\n^\\intercal, %\n\\begin{pmatrix}0.1&0.8\\end{pmatrix}\n^\\intercal\\right\\}\\),\n\\({\\boldsymbol{\\mathbf{\\gamma}}} \\in \\left\\{%\n\\begin{pmatrix}0&0\\end{pmatrix}\n^\\intercal, %\n\\begin{pmatrix}0.3&0\\end{pmatrix}\n^\\intercal\\right\\}\\),\n\nleading to a total of 32 distinct combinations. Note that this selection of parameters intentionally violates the common ratio assumption between direct and indirect effects, as this should be a more common case in practical research. All combinations were simulated in 1000 trials, with the same starting seed for each combination. If youre, interested in the simulations, see replication code on Github.\n\n9.5.1 Without omitted variable bias\n\n\n\nBias of impacts and 95% confidence interval of empirical standard deviation without omv: \\({\\boldsymbol{\\mathbf{\\beta}}}=(0.2, 0.5)^\\intercal\\), \\({\\boldsymbol{\\mathbf{\\gamma}}}=(0, 0)^\\intercal\\). \\(\\rho=\\) autocorrelation in the dependent variable (\\(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{y}}\\)); \\(\\boldsymbol{\\mathbf{\\delta}}=\\) autocorrelation in the covariates (\\(\\boldsymbol{\\mathbf{x}}_k = f(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{x}}_k)\\)); \\(\\lambda=\\) autocorrelation in the disturbances (\\(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{u}}\\)); \\(\\boldsymbol{\\mathbf{\\theta}}=\\) spatial spillover effects of covariates (\\(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{X}}\\)); \\(\\boldsymbol{\\mathbf{\\gamma}}=\\) strength of omv.\n\n\nSLX, SDM, and SDEM all provide quite accurate estimates of the direct impacts (most visible in column 2). SAR, SEM, and SAC, in contrast, yield some drawbacks: especially in the presence of local spillover effects, these three specifications are biased (see lower part). Furthermore, SAR and SEM suffer from bias if autocorrelation in the disturbance and autocorrelation in the dependent variable are present simultaneously (see line 6 and 8). Though SLX is downwardly biased in case of autocorrelation in the dependent variable and the covariates (e.g. line 12 and 16), and SDM as well as SDEM yield some bias in case of a GNS-like process (line 14 and 16), those biases are rather moderate. This indicates that SLX, SDM, and SDEM are most robust against misspecification regarding the direct impacts.\nSeveral differences exist regarding the indirect impacts. Most obviously, the often used SAR specification suffers from considerable bias: it overestimates indirect impacts in case of autocorrelation in the disturbances, and offers biased estimates if local spillover effects exist (which are not restricted to a common ratio). The latter also applies to SAC: though SAC offers relatively accurate estimates for \\({\\boldsymbol{\\mathbf{x}}}_2\\), it overestimates indirect impacts for \\({\\boldsymbol{\\mathbf{x}}}_1\\).\nRegarding the remaining three specifications – SLX, SDM, and SDEM – conclusions are less obvious. SDM and SDEM suffer from large bias for high values of \\({\\boldsymbol{\\mathbf{\\theta}}}\\) (see \\(\\boldsymbol{\\mathbf{x}}_2\\)) if the DGP follows a GNS-like process (line 14 and 16): SDM overestimates the indirect impacts, while SDEM underestimates the indirect impacts. In addition, SDM performs badly if the true DGP is SDEM (line 13), and SDEM performs badly if the true DGP is SDM (line 10), whereas the bias increases with higher values of \\(\\theta_k\\) in both cases. Similar to SDEM, SLX underestimates the indirect impacts in presence of global spillovers / autocorrelation in the dependent variable.\n\n\n9.5.2 With omitted variable bias\n\n\n\nBias of impacts and 95% confidence interval of empirical standard deviation with omv: \\({\\boldsymbol{\\mathbf{\\beta}}}=(0.2, 0.5)^\\intercal\\), \\({\\boldsymbol{\\mathbf{\\gamma}}}=(0.3, 0)^\\intercal\\). \\(\\rho=\\) autocorrelation in the dependent variable (\\(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{y}}\\)); \\(\\boldsymbol{\\mathbf{\\delta}}=\\) autocorrelation in the covariates (\\(\\boldsymbol{\\mathbf{x}}_k = f(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{x}}_k)\\)); \\(\\lambda=\\) autocorrelation in the disturbances (\\(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{u}}\\)); \\(\\boldsymbol{\\mathbf{\\theta}}=\\) spatial spillover effects of covariates (\\(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{X}}\\)); \\(\\boldsymbol{\\mathbf{\\gamma}}=\\) strength of omv.\n\n\n\n\n9.5.3 Indirect impacts if DGP = GNS\nBelow an illustration about the indirect impacts, if the spatial process is a combination of\n\nClustering on Unobservables\nInterdependence (in the outcome)\nSpillovers in Covariates\n\n\n\n\nBias of indirect impacts and 95% confidence interval of empirical standard deviation for different strengths of autocorrelation: \\({\\boldsymbol{\\mathbf{\\beta}}}=(0.2, 0.5)^\\intercal\\), \\({\\boldsymbol{\\mathbf{\\gamma}}}=(0, 0)^\\intercal\\), \\({\\boldsymbol{\\mathbf{\\delta}}}=(0, 0)^\\intercal\\), \\({\\boldsymbol{\\mathbf{\\theta}}}=(0.1, 0.8)^\\intercal\\). \\(\\rho=\\) autocorrelation in the dependent variable (\\(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{y}}\\)); \\(\\boldsymbol{\\mathbf{\\delta}}=\\) autocorrelation in the covariates (\\(\\boldsymbol{\\mathbf{x}}_k = f(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{x}}_k)\\)); \\(\\lambda=\\) autocorrelation in the disturbances (\\(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{u}}\\)); \\(\\boldsymbol{\\mathbf{\\theta}}=\\) spatial spillover effects of covariates (\\(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{X}}\\)); \\(\\boldsymbol{\\mathbf{\\gamma}}=\\) strength of omv.\n\n\nFirst, in a GNS-like situation, the bias in SDM grows with increasing autocorrelation in \\({\\boldsymbol{\\mathbf{y}}}\\) (\\(\\rho\\)) and increasing autocorrelation in the disturbances (\\(\\lambda\\)).\nSecond, the bias in SLX and SDEM increases with higher values of \\(\\rho\\), but is unaffected from the strength of \\(\\lambda\\).\nThird, though SLX and SDEM suffer from the same problem, the bias from omitting global autocorrelation is less severe in SLX than in SDEM.\nThus, the SLX outperforms SDEM. Furthermore, SLX outperforms SDM in most situations; only if the autocorrelation in the dependent variable is much stronger than the autocorrelation in the disturbances (\\(\\rho=0.9\\), \\(\\lambda=0.3\\)), SDM yields lower bias than SLX. Note that the SAC yields relatively low biases for the indirect impacts in GNS-like processes, but at the same time produces relative large biases in the direct impacts.\n\n\n\n\n\n\nAnselin, Luc. 1988. Spatial Econometrics: Methods and Models. Studies in Operational Regional Science. Dordrecht: Kluwer.\n\n\n———. 2003. “Spatial Externalities, Spatial Multipliers, and Spatial Econometrics.” International Regional Science Review 26 (2): 153–66. https://doi.org/10.1177/0160017602250972.\n\n\nAnselin, Luc, Anil K. Bera, Raymond Florax, and Mann J. Yoon. 1996. “Simple Diagnostic Tests for Spatial Dependence.” Regional Science and Urban Economics 26 (1): 77–104. https://doi.org/10.1016/0166-0462(95)02111-6.\n\n\nCook, Scott J., Jude C. Hays, and Robert J. Franzese. 2020. “Model Specification and Spatial Interdependence.” In The Sage Handbook of Research Methods in Political Science and International Relations, edited by Luigi Curini and Robert Franzese, 1st ed, 730–47. Thousand Oaks: SAGE Inc.\n\n\nElhorst, J. Paul. 2014. Spatial Econometrics: From Cross-Sectional Data to Spatial Panels. SpringerBriefs in Regional Science. Berlin and Heidelberg: Springer. https://doi.org/10.1007/978-3-642-40340-8.\n\n\nElhorst, J. Paul, and S. Halleck Vega. 2017. “The SLX Model: Extensions and the Sensitivity of Spatial Spillovers to W.” Papeles de Economía Española 152: 34–50.\n\n\nFlorax, Raymond, Hendrik Folmer, and Sergio J. Rey. 2003. “Specification Searches in Spatial Econometrics: The Relevance of Hendry’s Methodology.” Regional Science and Urban Economics 33 (5): 557–79. https://doi.org/10.1016/S0166-0462(03)00002-4.\n\n\nGibbons, Steve, and Henry G. Overman. 2012. “Mostly Pointless Spatial Econometrics?” Journal of Regional Science 52 (2): 172–91. https://doi.org/10.1111/j.1467-9787.2012.00760.x.\n\n\nGibbons, Steve, Henry G. Overman, and Eleonora Patacchini. 2015. “Spatial Methods.” In Handbook of Regional and Urban Economics, edited by Gilles Duranton, J. Vernon Henderson, and William C. Strange, 5:115–68. Amsterdam: Elsevier. https://doi.org/10.1016/B978-0-444-59517-1.00003-9.\n\n\nHalleck Vega, Solmaria, and J. Paul Elhorst. 2015. “The SLX Model.” Journal of Regional Science 55 (3): 339–63. https://doi.org/10.1111/jors.12188.\n\n\nLeSage, James P. 2014. “What Regional Scientists Need to Know about Spatial Econometrics.” The Review of Regional Studies 44 (1): 13–32. https://doi.org/https://dx.doi.org/10.2139/ssrn.2420725.\n\n\nLeSage, James P., and R. Kelley Pace. 2009. Introduction to Spatial Econometrics. Statistics, Textbooks and Monographs. Boca Raton: CRC Press.\n\n\nMur, Jesús, and Ana Angulo. 2009. “Model Selection Strategies in a Spatial Setting: Some Additional Results.” Regional Science and Urban Economics 39 (2): 200–213. https://doi.org/10.1016/j.regsciurbeco.2008.05.018.\n\n\nPinkse, Joris, and Margaret E. Slade. 2010. “The Future of Spatial Econometrics.” Journal of Regional Science 50 (1): 103–17. https://doi.org/10.1111/j.1467-9787.2009.00645.x.\n\n\nRüttenauer, Tobias. 2022. “Spatial Regression Models: A Systematic Comparison of Different Model Specifications Using Monte Carlo Experiments.” Sociological Methods & Research 51 (2): 728–59. https://doi.org/10.1177/0049124119882467."
  },
  {
    "objectID": "10_spatiotemporal.html#spatial-autoregressive-sar-model",
    "href": "10_spatiotemporal.html#spatial-autoregressive-sar-model",
    "title": "10  Spatio-temporal models",
    "section": "11.1 Spatial-Autoregressive (SAR) Model",
    "text": "11.1 Spatial-Autoregressive (SAR) Model\n\\[\n        {\\boldsymbol{\\mathbf{y}}}=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+\\rho{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{y}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{\\varepsilon}}}\n\\]\nSmall example\n\\[\n\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\ny_{3}  \n\\end{bmatrix} =  \\rho\n\\begin{bmatrix}\nw_{11} & w_{12} & w_{13}\\\\\nw_{21} & w_{22} & w_{23}\\\\\nw_{31} & w_{32} & w_{33}\n\\end{bmatrix}\n\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\ny_{3}  \n\\end{bmatrix} + \\boldsymbol{\\mathbf{\\beta}}\n\\begin{bmatrix}\nx_{11} & x_{12} & \\cdots \\\\\nx_{21} & x_{23} & \\cdots \\\\\nx_{31} & x_{32} & \\cdots\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\varepsilon_{1} \\\\\n\\varepsilon_{2} \\\\\n\\varepsilon_{3}  \n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nElhorst, J. Paul. 2014. Spatial Econometrics: From Cross-Sectional Data to Spatial Panels. SpringerBriefs in Regional Science. Berlin and Heidelberg: Springer. https://doi.org/10.1007/978-3-642-40340-8.\n\n\nMillo, Giovanni, and Gianfranco Piras. 2012. “Splm: Spatial Panel Data Models in R.” Journal of Statistical Software 47 (1). https://doi.org/10.18637/jss.v047.i01."
  },
  {
    "objectID": "11_other.html#geographically-weighted-regression",
    "href": "11_other.html#geographically-weighted-regression",
    "title": "11  Other Models",
    "section": "11.1 Geographically weighted regression",
    "text": "11.1 Geographically weighted regression\nDoes the relation between \\(y\\) and \\(x\\) vary depending on the region we are looking at? With geographically weighted regressions (GWR), we can exploit the spatial heterogeneity in relations / coefficients.\nGWR (Brunsdon, Fotheringham, and Charlton 1996; Gollini et al. 2015) is mainly an explorative tool for spatial data analysis in which we estimate an equation at different geographical points. For \\(L\\) given locations across London, we receive \\(L\\) different coefficients.\n\\[\n\\begin{align}\n\\hat{\\boldsymbol{\\mathbf{\\beta}}}_l=& ({\\boldsymbol{\\mathbf{X}}}^\\intercal{\\boldsymbol{\\mathbf{M}}}_l{\\boldsymbol{\\mathbf{X}}})^{-1}{\\boldsymbol{\\mathbf{X}}}^\\intercal{\\boldsymbol{\\mathbf{M}}}_l{\\boldsymbol{\\mathbf{Y}}},\n\\end{align}\n\\]\nThe \\(N \\times N\\) matrix \\({\\boldsymbol{\\mathbf{M}}}_l\\) defines the weights at each local point \\(l\\), assigning higher weights to closer units. The local weights are determined by a kernel density function with a pre-determined bandwidth \\(b\\) around each point (either a fixed distance or an adaptive k nearest neighbours bandwidth). Models are estimated via gwr.basic() or gwr.robust() of the GWmodel package.\n\n# Search for the optimal bandwidth \nset.seed(123)\nhv_1.bw &lt;- bw.gwr(log(med_house_price) ~ log(no2) + log(POPDEN) + pubs_count ,\n                  data = as_Spatial(msoa.spdf),\n                  kernel = \"boxcar\",\n                  adaptive = TRUE) \n\nAdaptive bandwidth: 615 CV score: 117.989 \nAdaptive bandwidth: 388 CV score: 107.5287 \nAdaptive bandwidth: 247 CV score: 89.99347 \nAdaptive bandwidth: 160 CV score: 76.23795 \nAdaptive bandwidth: 106 CV score: 66.39574 \nAdaptive bandwidth: 73 CV score: 62.89816 \nAdaptive bandwidth: 52 CV score: 59.46008 \nAdaptive bandwidth: 39 CV score: 56.70472 \nAdaptive bandwidth: 31 CV score: 54.97107 \nAdaptive bandwidth: 26 CV score: 53.27627 \nAdaptive bandwidth: 23 CV score: 54.23635 \nAdaptive bandwidth: 28 CV score: 54.47944 \nAdaptive bandwidth: 25 CV score: 52.5378 \nAdaptive bandwidth: 24 CV score: 53.74594 \nAdaptive bandwidth: 25 CV score: 52.5378 \n\nhv_1.bw\n\n[1] 25\n\n### GWR \nhv_1.gwr &lt;- gwr.robust(log(med_house_price) ~ log(no2) + log(POPDEN) + pubs_count,\n                      data = as_Spatial(msoa.spdf), \n                      kernel = \"boxcar\", \n                      adaptive = TRUE, \n                      bw = hv_1.bw, \n                      longlat = FALSE)\nprint(hv_1.gwr)\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2023-06-22 11:39:30.243723 \n   Call:\n   gwr.basic(formula = formula, data = data, bw = bw, kernel = kernel, \n    adaptive = adaptive, p = p, theta = theta, longlat = longlat, \n    dMat = dMat, F123.test = F123.test, cv = T, W.vect = W.vect)\n\n   Dependent (y) variable:  med_house_price\n   Independent variables:  no2 POPDEN pubs_count\n   Number of data points: 983\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-0.90930 -0.24801 -0.05018  0.20925  1.49660 \n\n   Coefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept) 10.630835   0.194980  54.523  &lt; 2e-16 ***\n   log(no2)     0.716116   0.074916   9.559  &lt; 2e-16 ***\n   log(POPDEN) -0.111104   0.023101  -4.809 1.75e-06 ***\n   pubs_count   0.008513   0.004209   2.023   0.0434 *  \n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 0.359 on 979 degrees of freedom\n   Multiple R-squared: 0.1177\n   Adjusted R-squared: 0.115 \n   F-statistic: 43.55 on 3 and 979 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 126.1498\n   Sigma(hat): 0.3585988\n   AIC:  781.3976\n   AICc:  781.459\n   BIC:  -142.6963\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: boxcar \n   Adaptive bandwidth: 25 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                     Min.    1st Qu.     Median    3rd Qu.    Max.\n   Intercept   -5.4436033 10.0869061 13.1177690 15.5252567 26.8582\n   log(no2)    -4.0174929 -0.7502331  0.0833824  1.0580475  6.2823\n   log(POPDEN) -0.8610728 -0.3139087 -0.1368702 -0.0200140  0.4031\n   pubs_count  -0.3421595 -0.0252434 -0.0034784  0.0192272  0.2222\n   ************************Diagnostic information*************************\n   Number of data points: 983 \n   Effective number of parameters (2trace(S) - trace(S'S)): 136.1695 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 846.8305 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): -133.0433 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): -316.0801 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): -496.9587 \n   Residual sum of squares: 36.33063 \n   R-square value:  0.7459107 \n   Adjusted R-square value:  0.7050051 \n\n   ***********************************************************************\n   Program stops at: 2023-06-22 11:39:36.521145 \n\n\nThe results give a range of coefficients for different locations. Let’s map those individual coefficients.\n\n# Spatial object\ngwr.spdf &lt;- st_as_sf(hv_1.gwr$SDF)\ngwr.spdf &lt;- st_make_valid(gwr.spdf)\n\n# Map\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\nmp2 &lt;- tm_shape(gwr.spdf) +\n  tm_fill(col = \"log(POPDEN)\", \n          style = \"cont\", \n          # n = 8,\n          title = \"Coefficient\", \n          palette = viridis(100),\n          midpoint = TRUE, stretch.palette = TRUE) +\n  tm_borders(col = \"grey85\") +\n  tm_layout(legend.frame = TRUE, legend.bg.color = TRUE,\n            #legend.position = c(\"right\", \"bottom\"),\n            legend.outside = TRUE,\n            main.title = \"Coefficient of log population density\", \n            main.title.position = \"center\",\n            title.snap.to.legend = TRUE) \n\nmp2 \n\n\n\n\n\n\nJust from looking at the map, there may be a connection with the undergrpund network - the effect of population density on house values seems to be stronger / more positive where underground connection is weaker?!"
  },
  {
    "objectID": "11_other.html#non-linear-models",
    "href": "11_other.html#non-linear-models",
    "title": "11  Other Models",
    "section": "11.2 Non-Linear Models",
    "text": "11.2 Non-Linear Models\nModels with endogenous regressors (SAR)\n\nIn the literature: mostly spatial probit considered\nSpatial logit rather uncommon (non normally distributed errors)\n\nIssues with non-linear spatial models\n\nEstimation: with dependent observations, we need to maximize one \\(n\\)-dimensional (log-)likelihood instead of a product of \\(n\\) independent distributions\nEstimation challenging and computationally intense\nHard to interpret due to non-linear effects in non-linear models\n\n(Elhorst.2017b?), Franzese, Hays, and Cook (2016)\n\n11.2.1 Problem with non-linear models\nSpatial-SAR-Probit \\[\n        {\\boldsymbol{\\mathbf{y}}^\\star}=\\rho{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{y}}^\\star}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{\\varepsilon}}} \\\\     \n        y_i = \\{1 \\text{ if } y_i^\\star &gt; 0; 0 \\text{ if } y_i^\\star \\leq 0 \\} \\nonumber\n\\]\nor in reduced form:\n\\[\n        {\\boldsymbol{\\mathbf{y}}^\\star}=(\\boldsymbol{\\mathbf{I}} - \\rho{\\boldsymbol{\\mathbf{W}}})^{-1}{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}} + \\boldsymbol{\\mathbf{u}} \\text{, } \\boldsymbol{\\mathbf{u}} = (\\boldsymbol{\\mathbf{I}} - \\rho{\\boldsymbol{\\mathbf{W}}})^{-1}{\\boldsymbol{\\mathbf{\\varepsilon}}},\\\\\n        \\text{with } \\boldsymbol{\\mathbf{u}} \\sim MVN(0, (\\boldsymbol{\\mathbf{I}} - \\rho{\\boldsymbol{\\mathbf{W}}})^\\intercal (\\boldsymbol{\\mathbf{I}} - \\rho{\\boldsymbol{\\mathbf{W}}})^{-1}) \\nonumber\n\\]\n\nProbability \\(\\boldsymbol{\\mathbf{y}}^\\star\\) is a latent variable, not observed\nWe only observe binary outcome \\(y_i\\)\n\\(\\mathrm{Cov}(y_i, y_j)\\) is not the same as \\(\\mathrm{Cov}(y_i^\\star, y_j^\\star)\\)\nError term is heteroskedastic and spatially correlated\n\nProbability\n\\[\n        \\mathrm{Prob}[{\\boldsymbol{\\mathbf{y}}^\\star}&gt;0]  =\n        \\mathrm{Prob}[(\\boldsymbol{\\mathbf{I}} - \\rho{\\boldsymbol{\\mathbf{W}}})^{-1}{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}} +\n        (\\boldsymbol{\\mathbf{I}} - \\rho{\\boldsymbol{\\mathbf{W}}})^{-1}{\\boldsymbol{\\mathbf{\\varepsilon}}}] \\\\\n          =  \\mathrm{Prob}[(\\boldsymbol{\\mathbf{I}} - \\rho{\\boldsymbol{\\mathbf{W}}})^{-1}{\\boldsymbol{\\mathbf{\\varepsilon}}} &lt;\n         (\\boldsymbol{\\mathbf{I}} - \\rho{\\boldsymbol{\\mathbf{W}}})^{-1}{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}] \\nonumber\n\\]\nor in using the observed outcome:\n\\[\n        \\mathrm{Prob}[\\boldsymbol{\\mathbf{y}}_i=1 | \\boldsymbol{\\mathbf{X}}]  =\n        \\mathrm{Prob}\\big[u_i &lt;\n         [(\\boldsymbol{\\mathbf{I}} - \\rho{\\boldsymbol{\\mathbf{W}}})^{-1}{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}]_i\\big] \\\\\n          = \\boldsymbol{\\mathbf{\\phi}}\\{[(\\boldsymbol{\\mathbf{I}} - \\rho{\\boldsymbol{\\mathbf{W}}})^{-1}{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}]_i\n         / \\sigma_{ui}\\}\n\\]\n\n\\(\\boldsymbol{\\mathbf{\\phi}}\\{\\}\\) is an n-dimensional cumulative-normal distribution\n\\(\\sigma_{ui}\\) equals \\((\\boldsymbol{\\mathbf{I}} - \\rho{\\boldsymbol{\\mathbf{W}}})^\\intercal (\\boldsymbol{\\mathbf{I}} - \\rho{\\boldsymbol{\\mathbf{W}}})^{-1})_{ii}\\), not constant\nno analytical solution\n\n\n\n11.2.2 Estimation\nEstimation methods for Spatial-SAR Probit / Logit\n\nExpectation Maximization (McMillen 1992).\n(Linearized) Generalized Methods of Moments (Klier and McMillen 2008).\nRecursive Importance Sampling (Beron and Vijverberg 2004).\nMaximum Simulated Likelihood RIS (Franzese, Hays, and Cook 2016)\nBayesian approach with Markov Chain Monte Carlo simulations (LeSage and Pace 2009): R package spatialprobit\n\nNote that it can be hard to interpret the results. As in the linear case, it is necessary to compute the impacts. However, the `marginal’ effects may vary with values of the independent variables and the location (Lacombe and LeSage 2018).\n\n\n11.2.3 Suggestion\nIn case you are not familiar with the econometric estimation methods and spatial regression models, don't use non-linear models with AR term. \n\nIf necessary, I would recommend using `spatialprobit` relying on Bayesian MCMC (set high ndraw and burn-in, e.g. 7500 and 2500).\n\nSo far, no `best practice’ guide\nNo systematic comparison of estimation methods\nIn R: Only spatialprobit provides impact measures?\nHard to interpret results\n\nWork-around: If the specification is theoretical plausible, using SLX probit / logit might be a practical solution!\n\n\n\n\n\n\nBeron, Kurt J., and Wim P. M. Vijverberg. 2004. “Probit in a Spatial Context: A Monte Carlo Analysis.” In Advances in Spatial Econometrics: Methodology, Tools and Applications, edited by Luc Anselin, Florax, Raymond J. G. M, and Sergio J. Rey, 169–95. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-05617-2‗ 8.\n\n\nBrunsdon, Chris, A. Stewart Fotheringham, and Martin E. Charlton. 1996. “Geographically Weighted Regression: A Method for Exploring Spatial Nonstationarity.” Geographical Analysis 28 (4): 281–98. https://doi.org/10.1111/j.1538-4632.1996.tb00936.x.\n\n\nFranzese, Robert J., Jude C. Hays, and Scott J. Cook. 2016. “Spatial- and Spatiotemporal-Autoregressive Probit Models of Interdependent Binary Outcomes.” Political Science Research and Methods 4 (01): 151–73. https://doi.org/10.1017/psrm.2015.14.\n\n\nGollini, Isabella, Binbin Lu, Martin Charlton, Christopher Brunsdon, and Paul Harris. 2015. “GWmodel : An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models.” Journal of Statistical Software 63 (17). https://doi.org/10.18637/jss.v063.i17.\n\n\nKlier, Thomas, and Daniel P. McMillen. 2008. “Clustering of Auto Supplier Plants in the United States: Generalized Method of Moments Spatial Logit for Large Samples.” Journal of Business & Economic Statistics 26 (4): 460–71.\n\n\nLacombe, Donald J., and James P. LeSage. 2018. “Use and Interpretation of Spatial Autoregressive Probit Models.” The Annals of Regional Science 60 (1): 1–24. https://doi.org/10.1007/s00168-015-0705-x.\n\n\nLeSage, James P., and R. Kelley Pace. 2009. Introduction to Spatial Econometrics. Statistics, Textbooks and Monographs. Boca Raton: CRC Press.\n\n\nMcMillen, Daniel P. 1992. “Probit with Spatial Autocorrelation.” Journal of Regional Science 32 (3): 335–48. https://doi.org/10.1111/j.1467-9787.1992.tb00190.x."
  },
  {
    "objectID": "12_exercise2.html",
    "href": "12_exercise2.html",
    "title": "12  Exercise II",
    "section": "",
    "text": "Required packages\n\npkgs &lt;- c(\"sf\", \"mapview\", \"spdep\", \"spatialreg\", \"tmap\", \"viridisLite\", \n          \"plm\", \"splm\", \"SDPDmod\")\nlapply(pkgs, require, character.only = TRUE)\n\n\n\nSession info\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] SDPDmod_0.0.3     splm_1.6-2        plm_2.6-3        \n [4] viridisLite_0.4.2 tmap_3.3-3        spatialreg_1.2-9 \n [7] Matrix_1.5-4.1    spdep_1.2-8       spData_2.2.2     \n[10] mapview_2.11.0    sf_1.0-13        \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.0   dplyr_1.1.2        fastmap_1.1.1     \n [4] leaflet_2.1.2      TH.data_1.1-2      dotCall64_1.0-2   \n [7] XML_3.99-0.14      digest_0.6.31      lifecycle_1.0.3   \n[10] LearnBayes_2.15.1  survival_3.5-5     terra_1.7-29      \n[13] magrittr_2.0.3     compiler_4.3.1     rlang_1.1.1       \n[16] tools_4.3.1        utf8_1.2.3         collapse_1.9.6    \n[19] knitr_1.43         htmlwidgets_1.6.2  sp_1.6-1          \n[22] classInt_0.4-9     RColorBrewer_1.1-3 multcomp_1.4-24   \n[25] abind_1.4-5        KernSmooth_2.23-21 expm_0.999-7      \n[28] leafsync_0.1.0     grid_4.3.1         stats4_4.3.1      \n[31] fansi_1.0.4        xtable_1.8-4       lfe_2.9-0         \n[34] e1071_1.7-13       leafem_0.2.0       colorspace_2.1-0  \n[37] scales_1.2.1       MASS_7.3-60        dichromat_2.0-0.1 \n[40] cli_3.6.1          mvtnorm_1.2-2      rmarkdown_2.22    \n[43] miscTools_0.6-28   generics_0.1.3     RSpectra_0.16-1   \n[46] rstudioapi_0.14    tmaptools_3.1-1    bdsmatrix_1.3-6   \n[49] DBI_1.1.3          proxy_0.4-27       ibdreg_0.3.8      \n[52] splines_4.3.1      stars_0.6-1        parallel_4.3.1    \n[55] s2_1.1.4           base64enc_0.1-3    vctrs_0.6.3       \n[58] boot_1.3-28.1      webshot_0.5.4      sandwich_3.0-2    \n[61] jsonlite_1.8.5     Formula_1.2-5      crosstalk_1.2.0   \n[64] units_0.8-2        spam_2.9-1         glue_1.6.2        \n[67] lwgeom_0.2-13      codetools_0.2-19   deldir_1.0-9      \n[70] raster_3.6-20      lmtest_0.9-40      munsell_0.5.0     \n[73] tibble_3.2.1       pillar_1.9.0       htmltools_0.5.5   \n[76] satellite_1.0.4    R6_2.5.1           maxLik_1.5-2      \n[79] wk_0.7.3           Rdpack_2.4         evaluate_0.21     \n[82] lattice_0.21-8     rbibutils_2.2.13   png_0.1-8         \n[85] class_7.3-22       Rcpp_1.0.10        coda_0.19-4       \n[88] nlme_3.1-162       xfun_0.39          zoo_1.8-12        \n[91] pkgconfig_2.0.3   \n\n\n\n\n13 Inkar data\nBelow, we read and transform some characteristics of the INKAR data on German counties.\n\ndi &lt;- c(\"_data/\")\n\n# Define the downloaded filed\nj &lt;- c(\"inkar.csv\")\nc &lt;- 1\n\nfor(k in j){\n  header &lt;- as.vector(t(read.table(paste0(di, k), nrows = 1, sep = \";\")[1,]))\n  # Clean header\n  header &lt;- stringi::stri_replace_all_fixed(\n    header, \n    c(\"ä\", \"ö\", \"ü\", \"Ä\", \"Ö\", \"Ü\"), \n    c(\"ae\", \"oe\", \"ue\", \"Ae\", \"Oe\", \"Ue\"), \n    vectorize_all = FALSE\n  )\n  header &lt;- gsub(\" \", \"\", header)\n  header &lt;- gsub(\"\\\\.\", \"\", header)\n  header &lt;- iconv(header, \"latin1\", \"ASCII\", sub = \"\")\n  \n  # Combine with second row header (year)\n  header2 &lt;- as.vector(t(read.table(paste0(di, k), skip = 1, nrows = 1, sep = \";\")[1,]))\n  header3 &lt;- paste(header, header2, sep = \"_\")\n  header3 &lt;- gsub(\"_NA\", \"\", header3)\n  \n  # Input and rename data\n  data &lt;- read.csv(paste0(di, k), skip = 2, header = FALSE, sep = \";\", quote = \"\\\"\", dec = \",\", stringsAsFactors = F)\n  names(data) &lt;- header3\n  data1 &lt;- data\n  \n  # Correct character vars (containing thousands separator)\n  vars &lt;- which(sapply(data1, is.character))\n  vars &lt;- vars[-which(vars %in% c(1:3))]\n  for(l in vars){\n    data1[,l] &lt;- gsub(\"\\\\.\", \"\", data1[,l])\n    data1[,l] &lt;- gsub(\"\\\\,\", \".\", data1[,l])\n    data1[,l] &lt;- as.numeric(data1[,l])\n  }\n  \n  \n  # #Save\n  # l &lt;- paste(\"bearb\", k, sep = \"_\")\n  # write.table(data1, file = l, row.names = FALSE, sep = \";\", dec = \".\", na = \".\")\n  \n  #Reshape\n  helpvar1 &lt;- unique(header[4:length(header)])\n  helpvar2 &lt;-  sort(unique(header2[!is.na(header2)]))\n  n_vars &lt;- length(helpvar1)\n  n_times &lt;- length(helpvar2)\n  helpvar1 &lt;- sort(rep(helpvar1, times = n_times))\n  helpvar2 &lt;- rep(helpvar2, times = n_vars)\n  helpvar3 &lt;- paste(helpvar1, helpvar2, sep = \"_\")\n  count &lt;- ncol(data1)+1\n  for(v in helpvar3) {\n    if(v %in% names(data1)) {}\n    else{\n      data1[,count] &lt;- NA\n      colnames(data1)[count] &lt;- v\n      count &lt;- count+1\n    }\n  }\n  data1 &lt;- data1[c(colnames(data1)[1:3], sort(helpvar3))]\n  \n  data1 &lt;- reshape(data1, direction = \"long\", varying = 4:ncol(data1), \n                   sep = \"_\")\n  colnames(data1) &lt;- substr(colnames(data1), 1, 30)\n  \n  if(c == 1){\n    data.long &lt;- data1\n  }else{\n    data.long &lt;- merge(data.long, data1, all.x = TRUE, all.y = TRUE)\n  }\n  \n  c &lt;- c+1\n  \n}\n\n### Rename\n\nnames(data.long) &lt;- c(\"Kennziffer\"               ,      \"Raumeinheit\",                   \n\"Aggregat\"                 ,      \"time\"                          ,\n\"out_commute\"               ,      \"employment_rate\"           ,\n\"population\"               ,      \"GDP\",\n\"in_commute\"               ,      \"households_with_kids\"           ,\n\"debts\"                     ,      \"life_expectancy\"               ,\n\"car_density\"               ,      \"traffic_accidents\"        ,\n\"id\" )\n\nVariables are\n\n\n\n\n\n\n\nVariable\nINkar name\n\n\n\n\n“Kennziffer”\n\n\n\n“Raumeinheit”\n\n\n\n“Aggregat”\n\n\n\n“time”\n\n\n\n“out_commute”\nAuspendler - Out-commuters\n\n\n“employment_rate”\nBeschäftigtenquote - Employment rate\n\n\n“population”\nBevoelkerung gesamt - Total population\n\n\n“GDP”\nBruttoinlandsprodukt in 1000 Euro - Gross Domestic Product in 1000 euros\n\n\n“in_commute”\nEinpendler - In-commuters\n\n\n“households_with_kids”\nHaushalte mit Kindern - Households with children\n\n\n“debts”\nKommunale Schulden - Municipal debts\n\n\n“life_expectancy”\nLebenserwartung - Life expectancy\n\n\n“car_density”\nPkw-Dichte - Car density\n\n\n“traffic_accidents”\nStraßenverkehrsunfälle - Road traffic accidents\n\n\n“id”"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "13  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anselin, Luc. 1988. Spatial Econometrics:\nMethods and Models. Studies in\nOperational Regional Science. Dordrecht:\nKluwer.\n\n\n———. 1995. “Local Indicators of Spatial\nAssociation-LISA.” Geographical Analysis 27 (2):\n93–115. https://doi.org/10.1111/j.1538-4632.1995.tb00338.x.\n\n\n———. 2003. “Spatial Externalities, Spatial\nMultipliers, and Spatial Econometrics.”\nInternational Regional Science Review 26 (2): 153–66. https://doi.org/10.1177/0160017602250972.\n\n\nAnselin, Luc, and Anil K. Bera. 1998. “Spatial\nDependence in Linear Regression Models with an\nIntroduction to Spatial Econometrics.”\nIn Handbook of Applied Economic Statistics, edited\nby Aman Ullah and David E. A. Giles, 237–89. New York:\nDekker.\n\n\nAnselin, Luc, Anil K. Bera, Raymond Florax, and Mann J. Yoon. 1996.\n“Simple Diagnostic Tests for Spatial\nDependence.” Regional Science and Urban Economics\n26 (1): 77–104. https://doi.org/10.1016/0166-0462(95)02111-6.\n\n\nBeron, Kurt J., and Wim P. M. Vijverberg. 2004. “Probit in a\nSpatial Context: A Monte Carlo\nAnalysis.” In Advances in Spatial\nEconometrics: Methodology, Tools and\nApplications, edited by Luc Anselin, Florax, Raymond\nJ. G. M, and Sergio J. Rey, 169–95. Berlin, Heidelberg:\nSpringer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-05617-2‗\n8.\n\n\nBetz, Timm, Scott J. Cook, and Florian M. Hollenbach. 2020.\n“Spatial Interdependence and Instrumental Variable Models.”\nPolitical Science Research and Methods 8 (4): 646–61. https://doi.org/10.1017/psrm.2018.61.\n\n\nBivand, Roger S., and Colin Rudel. 2018. “Rgeos:\nInterface to Geometry Engine - Open\nSource (’GEOS’).”\n\n\nBivand, Roger, Giovanni Millo, and Gianfranco Piras. 2021. “A\nReview of Software for Spatial\nEconometrics in R.” Mathematics 9\n(11): 1276. https://doi.org/10.3390/math9111276.\n\n\nBivand, Roger, and David W. S. Wong. 2018. “Comparing\nImplementations of Global and Local Indicators of Spatial\nAssociation.” TEST 27 (3): 716–48. https://doi.org/10.1007/s11749-018-0599-x.\n\n\nBrunsdon, Chris, A. Stewart Fotheringham, and Martin E. Charlton. 1996.\n“Geographically Weighted Regression: A\nMethod for Exploring Spatial\nNonstationarity.” Geographical Analysis 28 (4):\n281–98. https://doi.org/10.1111/j.1538-4632.1996.tb00936.x.\n\n\nCliff, Andrew, and Keith Ord. 1972. “Testing for Spatial\nAutocorrelation Among Regression Residuals.”\nGeographical Analysis 4 (3): 267–84. https://doi.org/10.1111/j.1538-4632.1972.tb00475.x.\n\n\nCook, Scott J., Jude C. Hays, and Robert J. Franzese. 2020. “Model\nSpecification and Spatial\nInterdependence.” In The Sage Handbook of\nResearch Methods in Political Science and International Relations,\nedited by Luigi Curini and Robert Franzese, 1st ed, 730–47.\nThousand Oaks: SAGE Inc.\n\n\nDrukker, David M., Peter Egger, and Ingmar R. Prucha. 2013. “On\nTwo-Step Estimation of a Spatial Autoregressive\nModel with Autoregressive Disturbances and\nEndogenous Regressors.” Econometric Reviews\n32 (5-6): 686–733. https://doi.org/10.1080/07474938.2013.741020.\n\n\nFranzese, Robert J., and Jude C. Hays. 2007. “Spatial\nEconometric Models of Cross-Sectional\nInterdependence in Political Science Panel and\nTime-Series-Cross-Section Data.” Political\nAnalysis 15 (2): 140–64. https://doi.org/10.1093/pan/mpm005.\n\n\nFranzese, Robert J., Jude C. Hays, and Scott J. Cook. 2016.\n“Spatial- and Spatiotemporal-Autoregressive Probit\nModels of Interdependent Binary Outcomes.”\nPolitical Science Research and Methods 4 (01): 151–73. https://doi.org/10.1017/psrm.2015.14.\n\n\nGibbons, Steve, and Henry G. Overman. 2012. “Mostly\nPointless Spatial Econometrics?” Journal of\nRegional Science 52 (2): 172–91. https://doi.org/10.1111/j.1467-9787.2012.00760.x.\n\n\nGollini, Isabella, Binbin Lu, Martin Charlton, Christopher Brunsdon, and\nPaul Harris. 2015. “GWmodel : An R\nPackage for Exploring Spatial Heterogeneity Using\nGeographically Weighted Models.” Journal of\nStatistical Software 63 (17). https://doi.org/10.18637/jss.v063.i17.\n\n\nHalleck Vega, Solmaria, and J. Paul Elhorst. 2015. “The SLX\nModel.” Journal of Regional Science 55 (3):\n339–63. https://doi.org/10.1111/jors.12188.\n\n\nKelejian, Harry H., and Gianfranco Piras. 2017. Spatial\nEconometrics. Elsevier. https://doi.org/10.1016/C2016-0-04332-2.\n\n\nKelejian, Harry H., and Ingmar R. Prucha. 1998. “A\nGeneralized Spatial Two-Stage Least Squares Procedure for\nEstimating a Spatial Autoregressive Model with\nAutoregressive Disturbances.” The Journal of\nReal Estate Finance and Economics 17 (1): 99–121. https://doi.org/10.1023/A:1007707430416.\n\n\n———. 1999. “A Generalized Moments Estimator for the\nAutoregressive Parameter in a Spatial\nModel.” International Economic Review 40 (2):\n509–33. https://doi.org/10.1111/1468-2354.00027.\n\n\n———. 2010. “Specification and Estimation of\nSpatial Autoregressive Models with\nAutoregressive and Heteroskedastic\nDisturbances.” Journal of Econometrics 157 (1):\n53–67. https://doi.org/10.1016/j.jeconom.2009.10.025.\n\n\nKelejian, Harry H., Ingmar R. Prucha, and Yevgeny Yuzefovich. 2004.\n“Instrumental Variable Estimation of a Spatial\nAutoregressive Model with Autoregressive\nDisturbances: Large and Small Sample\nResults.” In Spatial and Spatiotemporal\nEconometrics, edited by James P. LeSage and R. Kelley Pace,\n163–98. Advances in Econometrics. Amsterdam and\nBoston: Elsevier.\n\n\nKlier, Thomas, and Daniel P. McMillen. 2008. “Clustering of\nAuto Supplier Plants in the United States:\nGeneralized Method of Moments Spatial Logit\nfor Large Samples.” Journal of Business &\nEconomic Statistics 26 (4): 460–71.\n\n\nLacombe, Donald J., and James P. LeSage. 2018. “Use and\nInterpretation of Spatial Autoregressive Probit Models.” The\nAnnals of Regional Science 60 (1): 1–24. https://doi.org/10.1007/s00168-015-0705-x.\n\n\nLee, Lung-fei. 2004. “Asymptotic Distributions of\nQuasi-Maximum Likelihood Estimators for Spatial\nAutoregressive Models.” Econometrica 72 (6):\n1899–1925.\n\n\nLeSage, James P. 2014. “What Regional Scientists Need\nto Know about Spatial Econometrics.”\nThe Review of Regional Studies 44 (1): 13–32.\nhttps://doi.org/https://dx.doi.org/10.2139/ssrn.2420725.\n\n\nLeSage, James P., and R. Kelley Pace. 2009. Introduction to\nSpatial Econometrics. Statistics,\nTextbooks and Monographs. Boca\nRaton: CRC Press.\n\n\n———. 2014. “The Biggest Myth in Spatial\nEconometrics.” Econometrics 2 (4): 217–49. https://doi.org/10.3390/econometrics2040217.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019.\nGeocomputation with R. 1st ed. Chapman &\nHall/CRC the R Series. Boca\nRaton: Chapman & Hall/CRC.\n\n\nMcMillen, Daniel P. 1992. “Probit with Spatial\nAutocorrelation.” Journal of Regional Science 32\n(3): 335–48. https://doi.org/10.1111/j.1467-9787.1992.tb00190.x.\n\n\nMoran, P. A. P. 1950. “Notes on Continuous Stochastic\nPhenomena.” Biometrika 37 (1/2): 17. https://doi.org/10.2307/2332142.\n\n\nNeumayer, Eric, and Thomas Plümper. 2016. “W.”\nPolitical Science Research and Methods 4 (01): 175–93. https://doi.org/10.1017/psrm.2014.40.\n\n\nOrd, John Keith. 1975. “Estimation Methods for\nModels of Spatial Interaction.”\nJournal of the American Statistical Association 70 (349):\n120–26. https://doi.org/10.2307/2285387.\n\n\nPace, R. Kelley, and James P. LeSage. 2010. “Omitted\nVariable Biases of OLS and Spatial Lag\nModels.” In Progress in Spatial\nAnalysis, edited by Antonio Páez, Julie Gallo, Ron N.\nBuliung, and Sandy Dall’erba, 17–28. Berlin and Heidelberg:\nSpringer.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data\nScience: With Applications in R.\nFirst. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nRüttenauer, Tobias. 2022. “Spatial Regression Models:\nA Systematic Comparison of Different Model\nSpecifications Using Monte Carlo Experiments.”\nSociological Methods & Research 51 (2): 728–59. https://doi.org/10.1177/0049124119882467.\n\n\nSarrias, Mauricio. 2023. Intermediate Spatial\nEconometrics with Applications in\nR.\n\n\nTobler, Waldo R. 1970. “A Computer Movie Simulating Urban\nGrowth in the Detroit Region.” Economic\nGeography 46: 234–40. https://doi.org/10.2307/143141.\n\n\nWimpy, Cameron, Guy D. Whitten, and Laron K. Williams. 2021. “X\nMarks the Spot: Unlocking the\nTreasure of Spatial-X Models.” The\nJournal of Politics 83 (2): 722–39. https://doi.org/10.1086/710089.\n\n\nWooldridge, Jeffrey M. 2010. Econometric Analysis of\nCross Section and Panel Data.\nCambridge, Mass.: MIT Press."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Data Analysis",
    "section": "",
    "text": "Introduction\nThis course material is designed for a 1-day GESIS workshop on spatial data analysis. Rüttenauer (2024) is a handbook chapter accompanying these workshop materials.\nIn recent years, more and more spatial data has become available, providing the possibility to combine otherwise unrelated data, such as social, economic, and environmental data. This also opens up the possibility of analyzing spatial patterns and processes (e.g., spillover effects or diffusion).\nMany social science research questions are spatially dependent such as voting outcomes, housing prices, labour markets, protest behavior, or migration decisions. Observing an event in one region or neighborhood increases the likelihood that we observe similar processes in proximate areas. As Tobler’s first law of geography puts it: “Everything is related to everything else, but near things are more related than distant things”. This dependence can stem from spatial contagion, spatial spillovers, or common confounders. Therefore, basic assumptions of standard regression models are violated when analyzing spatial data. However, more imoprtantly, spatial processes are interesting for their own sake. Spatial regression models can detect spatial dependence and explicitly model spatial relations, identifying spatial clustering, spillovers or diffusion processes.\nThe main objective of the course is the theoretical understanding and practical application of spatial regression models. This course will first give an overview on how to perform common spatial operations using spatial information, such as aggregating spatial units, calculating distances, merging spatial data as well as visualizing them. The course will further focus on the analysis of geographic data and the application of spatial regression techniques to model and analyze spatial processes, and furthermore, the course addresses several methods for defining spatial relationships, detecting and diagnosing spatial dependence and autocorrelation. Finally, we will discuss various spatial regression techniques to model processes, clarify the assumptions of these models, and show how they differ in their applications and interpretations.\nThe field has developed very quickly over the past few years, and R now provides a rich set of packages for various spatial data operations. For a more in-depth introduction into spatial data analysis in R, have a look into the materials references below.\nThe material introduces the use of geographical information to connect and analyze different spatial data sources very briefly. This introduction is limited to the fundamentals of using geographical information in R. Stefan Jünger & Anne-Kathrin Stroppe have provided a comprehensive GESIS workshop on geospatial techniques in R. The focus of this workshop will be on techniques for spatial data analysis, such as spatial regression models."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Spatial Data Analysis",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nSession\n\n\n\n\n09:00 – 09:30\nRefresher on R for spatial data\n\n\n09:30 – 11:00\nSpatial Relationships (W) and Spatial Dependence\n\n\n11:15 – 12:00\nPractical exercise\n\n\nLunch break\n\n\n\n13:00 – 14:30\nSpatial Regression Models (SLX, Error, lagged DV)\n\n\n14:45 – 16:00\nInterpreting Results: Spatial Impacts\n\n\n16:15 – 18:00\nPractical exercise"
  },
  {
    "objectID": "index.html#some-useful-packages",
    "href": "index.html#some-useful-packages",
    "title": "Spatial Data Analysis",
    "section": "Some useful packages",
    "text": "Some useful packages\nBy now, R provides a lot of functionalities for GIS applications and spatial econometrics, and further extensions. There are lots of packages providing a huge variety of spatial functionalities and methods (see e.g. R. Bivand, Millo, and Piras 2021). Important packages for fundamental spatial operations are:\n\nSpatial data workhorses: sf (Pebesma.2018?) and terra\nVisualization: mapview (Appelhans.2021?) and tmap (Tennekes.2018?)\nSpatial weights and other relations: spdep (R. S. Bivand and Rudel 2018)\nSpatial interpolation and kriging: gstat (Graler.2016?)\nSpatial regression models: spatialreg and sphet (R. Bivand and Piras 2015)\nThe packages have constantly developed over the past years, and older packages such as rgdal, rgeos, and sp are currently retiring (Blog post)"
  },
  {
    "objectID": "index.html#further-readings",
    "href": "index.html#further-readings",
    "title": "Spatial Data Analysis",
    "section": "Further Readings",
    "text": "Further Readings\n\nGreat up-to-date introduction to spatial R: Lovelace, Nowosad, and Muenchow (2019), updated version available online\nGreat open-science book on Spatial Data Science Pebesma and Bivand (2023)\nComprehensive introduction to spatial econometrics: LeSage and Pace (2009)\nRelative intuitive introduction to spatial econometrics: Ward and Gleditsch (2008)\nArticle-length introductions to spatial econometrics: Elhorst (2012), Halleck Vega and Elhorst (2015), LeSage (2014), Rüttenauer (2024), and Rüttenauer (2022)"
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Spatial Data Analysis",
    "section": "Course materials",
    "text": "Course materials\n\nI highly recommend the great Introduction to Geospatial Techniques for Social Scientists in R including, see Stefan Jünger & Anne-Kathrin Stroppe’s GESIS workshop materials. Nice materials on GIS, spatial operations and spatial data visualisation!\nFor those looking for a more in-depth introduction, I highly recommend Roger Bivand’s course on Spatial Data Analysis: Youtube recordings, Course Materials\nI’ve learned most of what I know about spatial econometrics from Scott J. Cook and his workshop on Spatial Econometrics at the Essex Summer school.\n\n\n\n\n\n\n\nBivand, Roger S., and Colin Rudel. 2018. “Rgeos: Interface to Geometry Engine - Open Source (’GEOS’).”\n\n\nBivand, Roger, Giovanni Millo, and Gianfranco Piras. 2021. “A Review of Software for Spatial Econometrics in R.” Mathematics 9 (11): 1276. https://doi.org/10.3390/math9111276.\n\n\nBivand, Roger, and Gianfranco Piras. 2015. “Comparing Implementations of Estimation Methods for Spatial Econometrics.” Journal of Statistical Software 63 (18): 1–36. https://doi.org/10.18637/jss.v063.i18.\n\n\nElhorst, J. Paul. 2012. “Dynamic Spatial Panels: Models, Methods, and Inferences.” Journal of Geographical Systems 14 (1): 5–28. https://doi.org/10.1007/s10109-011-0158-4.\n\n\nHalleck Vega, Solmaria, and J. Paul Elhorst. 2015. “The SLX Model.” Journal of Regional Science 55 (3): 339–63. https://doi.org/10.1111/jors.12188.\n\n\nLeSage, James P. 2014. “What Regional Scientists Need to Know about Spatial Econometrics.” The Review of Regional Studies 44 (1): 13–32. https://doi.org/https://dx.doi.org/10.2139/ssrn.2420725.\n\n\nLeSage, James P., and R. Kelley Pace. 2009. Introduction to Spatial Econometrics. Statistics, Textbooks and Monographs. Boca Raton: CRC Press.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. 1st ed. Chapman & Hall/CRC the R Series. Boca Raton: Chapman & Hall/CRC.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With Applications in R. First. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nRüttenauer, Tobias. 2022. “Spatial Regression Models: A Systematic Comparison of Different Model Specifications Using Monte Carlo Experiments.” Sociological Methods & Research 51 (2): 728–59. https://doi.org/10.1177/0049124119882467.\n\n\n———. 2024. “Spatial Data Analysis.” arXiv. https://arxiv.org/abs/2402.09895.\n\n\nWard, Michael Don, and Kristian Skrede Gleditsch. 2008. Spatial Regression Models. Vol. 155. Quantitative Applications in the Social Sciences. Thousand Oaks: Sage."
  },
  {
    "objectID": "08_exercise1.html#environmental-inequality",
    "href": "08_exercise1.html#environmental-inequality",
    "title": "8  Exercise I",
    "section": "8.1 Environmental inequality",
    "text": "8.1 Environmental inequality\nHow would you investigate the following descriptive research question: Are ethnic (and immigrant) minorities in London exposed to higher levels of pollution? Also consider the spatial structure. What’s your dependent and whats your independent variable?\n\n1) Define a neigbours weights object of your choice\nAssume a typical neighbourhood would be 1.5km in diameter\n\ncoords &lt;- st_centroid(msoa.spdf)\n\nWarning: st_centroid assumes attributes are constant over\ngeometries\n\n# Neighbours within 3km distance\ndist_15.nb &lt;- dnearneigh(coords, d1 = 0, d2 = 2500)\n\nsummary(dist_15.nb)\n\nNeighbour list object:\nNumber of regions: 983 \nNumber of nonzero links: 15266 \nPercentage nonzero weights: 1.579859 \nAverage number of links: 15.53001 \n4 regions with no links:\n158 463 478 505\nLink number distribution:\n\n 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 \n 4  5  9 23 19 26 36 31 53 39 61 63 59 48 42 35 24 31 28 30 27 26 \n22 23 24 25 26 27 28 29 30 31 32 33 34 \n25 19 38 29 32 38 26 16 20 10  8  1  2 \n5 least connected regions:\n160 469 474 597 959 with 1 link\n2 most connected regions:\n565 567 with 34 links\n\n# There are some mpty one. Lets impute with the nearest neighbour\nk2.nb &lt;- knearneigh(coords, k = 1)\n\n# Replace zero\nnolink_ids &lt;- which(card(dist_15.nb) == 0)\ndist_15.nb[card(dist_15.nb) == 0] &lt;- k2.nb$nn[nolink_ids, ]\n\nsummary(dist_15.nb)\n\nNeighbour list object:\nNumber of regions: 983 \nNumber of nonzero links: 15270 \nPercentage nonzero weights: 1.580273 \nAverage number of links: 15.53408 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 \n 9  9 23 19 26 36 31 53 39 61 63 59 48 42 35 24 31 28 30 27 26 25 \n23 24 25 26 27 28 29 30 31 32 33 34 \n19 38 29 32 38 26 16 20 10  8  1  2 \n9 least connected regions:\n158 160 463 469 474 478 505 597 959 with 1 link\n2 most connected regions:\n565 567 with 34 links\n\n# listw object with row-normalization\ndist_15.lw &lt;- nb2listw(dist_15.nb, style = \"W\")\n\n\n\n2) Estimate the extent of spatial auto-correlation\n\nmoran.test(msoa.spdf$no2, listw = dist_15.lw)\n\n\n    Moran I test under randomisation\n\ndata:  msoa.spdf$no2  \nweights: dist_15.lw    \n\nMoran I statistic standard deviate = 65.197, p-value &lt;\n2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.891520698      -0.001018330       0.000187411 \n\n\n\n\n3) Estimate a spatial SAR regression model\n\nEstimate a spatial autoregressive SAR model\n\n\nmod_1.sar &lt;- lagsarlm(log(no2) ~ per_mixed + per_asian + per_black + per_other\n                      + per_nonUK_EU + per_nonEU  + log(POPDEN),  \n                      data = msoa.spdf, \n                      listw = dist_15.lw,\n                      Durbin = FALSE) # we could here extend to SDM\nsummary(mod_1.sar)\n\n\nCall:\nlagsarlm(formula = log(no2) ~ per_mixed + per_asian + per_black + \n    per_other + per_nonUK_EU + per_nonEU + log(POPDEN), data = msoa.spdf, \n    listw = dist_15.lw, Durbin = FALSE)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.2140485 -0.0267085 -0.0021421  0.0238337  0.3505513 \n\nType: lag \nCoefficients: (asymptotic standard errors) \n                Estimate  Std. Error z value  Pr(&gt;|z|)\n(Intercept)  -1.7004e-02  1.8122e-02 -0.9383  0.348110\nper_mixed     3.4376e-04  1.4758e-03  0.2329  0.815810\nper_asian    -8.5205e-05  1.1494e-04 -0.7413  0.458507\nper_black    -4.2754e-04  2.3468e-04 -1.8218  0.068484\nper_other     1.9693e-03  7.4939e-04  2.6279  0.008591\nper_nonUK_EU  8.9027e-04  3.9638e-04  2.2460  0.024703\nper_nonEU     1.8460e-03  3.5159e-04  5.2506 1.516e-07\nlog(POPDEN)   1.8650e-02  2.7852e-03  6.6963 2.138e-11\n\nRho: 0.9684, LR test value: 2002.5, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.0063124\n    z-value: 153.41, p-value: &lt; 2.22e-16\nWald statistic: 23535, p-value: &lt; 2.22e-16\n\nLog likelihood: 1562.401 for lag model\nML residual variance (sigma squared): 0.0020568, (sigma: 0.045352)\nNumber of observations: 983 \nNumber of parameters estimated: 10 \nAIC: -3104.8, (AIC for lm: -1104.3)\nLM test for residual autocorrelation\ntest value: 108.97, p-value: &lt; 2.22e-16\n\n\n\nHave a look into the true multiplier matrix \\(({\\boldsymbol{\\mathbf{I}}_N}-\\rho {\\boldsymbol{\\mathbf{W}}})^{-1}\\beta_k\\)\n\n\nW &lt;- listw2mat(dist_15.lw)\nI &lt;- diag(dim(W)[1])\n\nrho &lt;- unname(mod_1.sar$rho)\n\nM &lt;- solve(I - rho*W)\n\nM[1:10, 1:10]\n\n                1           2           3           4           5\n [1,] 1.164650997 0.002433319 0.004089559 0.004034508 0.006545994\n [2,] 0.010706605 1.407336301 0.643881932 0.370049927 0.464794934\n [3,] 0.011246286 0.402426207 1.474021599 0.429011868 0.641526285\n [4,] 0.008875918 0.185024963 0.343209495 1.684533322 0.614086824\n [5,] 0.012000989 0.193664556 0.427684190 0.511739020 1.560840834\n [6,] 0.010741524 0.192552594 0.452940016 0.631452476 0.672787841\n [7,] 0.012779708 0.141953871 0.299247377 0.418234186 0.616895800\n [8,] 0.014769006 0.125781189 0.253122442 0.295553039 0.500919513\n [9,] 0.011708131 0.147549264 0.309080773 0.568442619 0.629156269\n[10,] 0.009937859 0.152900148 0.306652041 0.727001926 0.553973310\n                6           7          8           9          10\n [1,] 0.004882511 0.005808958 0.00872714 0.005854065 0.003613767\n [2,] 0.385105188 0.283907742 0.32703109 0.324608380 0.244640236\n [3,] 0.566175019 0.374059222 0.41132397 0.424986063 0.306652041\n [4,] 0.631452476 0.418234186 0.38421895 0.625286881 0.581601541\n [5,] 0.560656534 0.514079833 0.54266281 0.576726579 0.369315540\n [6,] 1.571175245 0.558170218 0.46513922 0.661184961 0.543820047\n [7,] 0.558170218 1.475511568 0.58520461 0.614170880 0.463886540\n [8,] 0.357799398 0.450157392 1.46638195 0.474994894 0.272339890\n [9,] 0.601077237 0.558337164 0.56135760 1.581077095 0.517983092\n[10,] 0.679775059 0.579858174 0.44255232 0.712226751 1.560083138\n\n\n\nCreate an \\(N \\times N\\) effects matrix. What is the effect of unit 6 on unit 10?\n\n\n# For beta 1\n\nbeta &lt;- mod_1.sar$coefficients\n\neffM &lt;- beta[2] * M\n\neffM[1:10, 1:10]\n\n                 1            2            3            4\n [1,] 4.003610e-04 8.364789e-07 1.405829e-06 1.386904e-06\n [2,] 3.680507e-06 4.837866e-04 2.213411e-04 1.272085e-04\n [3,] 3.866028e-06 1.383382e-04 5.067103e-04 1.474773e-04\n [4,] 3.051190e-06 6.360427e-05 1.179819e-04 5.790759e-04\n [5,] 4.125465e-06 6.657422e-05 1.470209e-04 1.759156e-04\n [6,] 3.692511e-06 6.619197e-05 1.557029e-04 2.170684e-04\n [7,] 4.393158e-06 4.879813e-05 1.028694e-04 1.437724e-04\n [8,] 5.077000e-06 4.323860e-05 8.701349e-05 1.015994e-04\n [9,] 4.024792e-06 5.072160e-05 1.062497e-04 1.954081e-04\n[10,] 3.416243e-06 5.256102e-05 1.054148e-04 2.499145e-04\n                 5            6            7            8\n [1,] 2.250254e-06 1.678414e-06 1.996890e-06 3.000045e-06\n [2,] 1.597781e-04 1.323839e-04 9.759625e-05 1.124204e-04\n [3,] 2.205314e-04 1.946286e-04 1.285868e-04 1.413969e-04\n [4,] 2.110988e-04 2.170684e-04 1.437724e-04 1.320793e-04\n [5,] 5.365554e-04 1.927315e-04 1.767203e-04 1.865460e-04\n [6,] 2.312779e-04 5.401079e-04 1.918768e-04 1.598965e-04\n [7,] 2.120644e-04 1.918768e-04 5.072225e-04 2.011702e-04\n [8,] 1.721963e-04 1.229973e-04 1.547463e-04 5.040841e-04\n [9,] 2.162790e-04 2.066266e-04 1.919342e-04 1.929725e-04\n[10,] 1.904341e-04 2.336798e-04 1.993323e-04 1.521320e-04\n                 9           10\n [1,] 2.012396e-06 1.242270e-06\n [2,] 1.115875e-04 8.409764e-05\n [3,] 1.460934e-04 1.054148e-04\n [4,] 2.149489e-04 1.999316e-04\n [5,] 1.982558e-04 1.269561e-04\n [6,] 2.272892e-04 1.869438e-04\n [7,] 2.111277e-04 1.594658e-04\n [8,] 1.632845e-04 9.361968e-05\n [9,] 5.435118e-04 1.780621e-04\n[10,] 2.448354e-04 5.362949e-04\n\n# \"Effect\" of unit 6 on unit 10\neffM[10, 6]\n\n           6 \n0.0002336798 \n\n\n\nCalculate and interpret the summary impact measures.\n\n\nmod_1.sar.imp &lt;- impacts(mod_1.sar, listw = dist_15.lw, R = 300)\nsummary(mod_1.sar.imp)\n\nImpact measures (lag, exact):\n                    Direct     Indirect        Total\nper_mixed     0.0004939013  0.010385844  0.010879745\nper_asian    -0.0001224192 -0.002574253 -0.002696672\nper_black    -0.0006142789 -0.012917166 -0.013531445\nper_other     0.0028294759  0.059498722  0.062328198\nper_nonUK_EU  0.0012791011  0.026897166  0.028176267\nper_nonEU     0.0026523198  0.055773451  0.058425770\nlog(POPDEN)   0.0267960076  0.563471199  0.590267206\n========================================================\nSimulation results ( variance matrix):\nDirect:\n\nIterations = 1:300\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 300 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                   Mean        SD  Naive SE Time-series SE\nper_mixed     0.0006040 0.0020920 1.208e-04      1.208e-04\nper_asian    -0.0001103 0.0001560 9.007e-06      1.035e-05\nper_black    -0.0006271 0.0003298 1.904e-05      1.904e-05\nper_other     0.0027658 0.0010967 6.332e-05      6.332e-05\nper_nonUK_EU  0.0012667 0.0005646 3.260e-05      3.260e-05\nper_nonEU     0.0026330 0.0004942 2.853e-05      3.103e-05\nlog(POPDEN)   0.0268165 0.0037907 2.189e-04      2.653e-04\n\n2. Quantiles for each variable:\n\n                   2.5%        25%        50%        75%     97.5%\nper_mixed    -3.555e-03 -0.0006939  0.0006818  2.010e-03 4.614e-03\nper_asian    -4.052e-04 -0.0002075 -0.0001140 -1.541e-05 1.825e-04\nper_black    -1.223e-03 -0.0008542 -0.0006202 -4.248e-04 4.855e-07\nper_other     5.201e-04  0.0020690  0.0027611  3.533e-03 4.875e-03\nper_nonUK_EU  6.608e-05  0.0008741  0.0012604  1.684e-03 2.267e-03\nper_nonEU     1.644e-03  0.0023112  0.0026410  2.932e-03 3.589e-03\nlog(POPDEN)   2.006e-02  0.0242190  0.0268230  2.966e-02 3.408e-02\n\n========================================================\nIndirect:\n\nIterations = 1:300\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 300 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                  Mean       SD  Naive SE Time-series SE\nper_mixed     0.012078 0.045516 0.0026279      0.0026279\nper_asian    -0.002418 0.003570 0.0002061      0.0002369\nper_black    -0.013473 0.007693 0.0004441      0.0004602\nper_other     0.059114 0.025766 0.0014876      0.0014876\nper_nonUK_EU  0.027127 0.013340 0.0007702      0.0007702\nper_nonEU     0.056823 0.016381 0.0009458      0.0010557\nlog(POPDEN)   0.573386 0.120109 0.0069345      0.0080970\n\n2. Quantiles for each variable:\n\n                  2.5%       25%       50%        75%     97.5%\nper_mixed    -0.075534 -0.014592  0.013192  0.0405609 1.008e-01\nper_asian    -0.010291 -0.004422 -0.002205 -0.0003546 3.968e-03\nper_black    -0.030384 -0.018381 -0.012752 -0.0086602 1.556e-05\nper_other     0.012396  0.043711  0.057869  0.0716045 1.180e-01\nper_nonUK_EU  0.001391  0.017889  0.027007  0.0347052 5.276e-02\nper_nonEU     0.033511  0.046229  0.053537  0.0655623 9.040e-02\nlog(POPDEN)   0.394650  0.485552  0.557876  0.6467093 8.672e-01\n\n========================================================\nTotal:\n\nIterations = 1:300\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 300 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                  Mean       SD  Naive SE Time-series SE\nper_mixed     0.012682 0.047576 0.0027468      0.0027468\nper_asian    -0.002529 0.003722 0.0002149      0.0002471\nper_black    -0.014100 0.007999 0.0004618      0.0004778\nper_other     0.061880 0.026737 0.0015437      0.0015437\nper_nonUK_EU  0.028394 0.013849 0.0007996      0.0007996\nper_nonEU     0.059456 0.016742 0.0009666      0.0010819\nlog(POPDEN)   0.600202 0.121985 0.0070428      0.0082708\n\n2. Quantiles for each variable:\n\n                  2.5%      25%       50%       75%     97.5%\nper_mixed    -0.079021 -0.01537  0.013886  0.042492 1.060e-01\nper_asian    -0.010702 -0.00465 -0.002321 -0.000369 4.168e-03\nper_black    -0.031483 -0.01929 -0.013426 -0.009016 1.604e-05\nper_other     0.012907  0.04572  0.060653  0.074784 1.228e-01\nper_nonUK_EU  0.001457  0.01881  0.028109  0.036437 5.446e-02\nper_nonEU     0.035310  0.04858  0.056231  0.068118 9.358e-02\nlog(POPDEN)   0.416004  0.51273  0.584256  0.672491 9.010e-01\n\n\n\n\n4) Is SAR the right model choice or would you rather estimate a different model?\n\nHow do results change once you specify a spatial Durbin model?\n\nI am using a spatial Durbin error model here.\n\nmod_1.durb &lt;- errorsarlm(log(no2) ~ per_mixed + per_asian + per_black + per_other\n                      + per_nonUK_EU + per_nonEU + log(POPDEN),  \n                      data = msoa.spdf, \n                      listw = dist_15.lw,\n                      Durbin = TRUE)\n\nsummary(mod_1.durb)\n\n\nCall:\nerrorsarlm(formula = log(no2) ~ per_mixed + per_asian + per_black + \n    per_other + per_nonUK_EU + per_nonEU + log(POPDEN), data = msoa.spdf, \n    listw = dist_15.lw, Durbin = TRUE)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.1839285 -0.0254426 -0.0027042  0.0216084  0.2944840 \n\nType: error \nCoefficients: (asymptotic standard errors) \n                    Estimate  Std. Error z value  Pr(&gt;|z|)\n(Intercept)       2.64939215  0.24748370 10.7053 &lt; 2.2e-16\nper_mixed         0.00553333  0.00223688  2.4737  0.013373\nper_asian        -0.00017156  0.00024183 -0.7094  0.478062\nper_black        -0.00057947  0.00034426 -1.6832  0.092334\nper_other         0.00203392  0.00112534  1.8074  0.070701\nper_nonUK_EU      0.00086254  0.00058902  1.4644  0.143091\nper_nonEU         0.00135822  0.00053480  2.5397  0.011096\nlog(POPDEN)       0.02716824  0.00354239  7.6695 1.732e-14\nlag.per_mixed     0.00107140  0.00819322  0.1308  0.895960\nlag.per_asian    -0.00060616  0.00070080 -0.8649  0.387069\nlag.per_black    -0.00191733  0.00130997 -1.4636  0.143291\nlag.per_other     0.01014125  0.00496979  2.0406  0.041293\nlag.per_nonUK_EU  0.00925620  0.00217624  4.2533 2.106e-05\nlag.per_nonEU     0.00563564  0.00185541  3.0374  0.002386\nlag.log(POPDEN)  -0.01370891  0.01128957 -1.2143  0.224634\n\nLambda: 0.99424, LR test value: 1527.8, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.0024921\n    z-value: 398.96, p-value: &lt; 2.22e-16\nWald statistic: 159170, p-value: &lt; 2.22e-16\n\nLog likelihood: 1572.051 for error model\nML residual variance (sigma squared): 0.0019546, (sigma: 0.044211)\nNumber of observations: 983 \nNumber of parameters estimated: 17 \nAIC: NA (not available for weighted model), (AIC for lm: -1584.3)\n\nmod_1.durb.imp &lt;- impacts(mod_1.durb, listw = dist_15.lw, R = 300)\nsummary(mod_1.durb.imp, zstats = TRUE, short = TRUE)\n\nImpact measures (SDEM, glht, n):\n                    Direct      Indirect         Total\nper_mixed     0.0055333298  0.0010713981  0.0066047279\nper_asian    -0.0001715584 -0.0006061567 -0.0007777151\nper_black    -0.0005794704 -0.0019173261 -0.0024967965\nper_other     0.0020339226  0.0101412504  0.0121751729\nper_nonUK_EU  0.0008625423  0.0092561971  0.0101187394\nper_nonEU     0.0013582217  0.0056356426  0.0069938643\nlog(POPDEN)   0.0271682392 -0.0137089088  0.0134593305\n========================================================\nStandard errors:\n                   Direct     Indirect        Total\nper_mixed    0.0022368774 0.0081932151 0.0089412446\nper_asian    0.0002418282 0.0007008041 0.0007540247\nper_black    0.0003442649 0.0013099673 0.0013639632\nper_other    0.0011253352 0.0049697880 0.0051074709\nper_nonUK_EU 0.0005890159 0.0021762395 0.0022231333\nper_nonEU    0.0005348024 0.0018554128 0.0020196122\nlog(POPDEN)  0.0035423870 0.0112895670 0.0132006518\n========================================================\nZ-values:\n                 Direct   Indirect     Total\nper_mixed     2.4736849  0.1307665  0.738681\nper_asian    -0.7094226 -0.8649446 -1.031419\nper_black    -1.6832108 -1.4636442 -1.830545\nper_other     1.8073926  2.0405801  2.383797\nper_nonUK_EU  1.4643785  4.2532989  4.551567\nper_nonEU     2.5396703  3.0374063  3.462974\nlog(POPDEN)   7.6694724 -1.2142989  1.019596\n\np-values:\n             Direct     Indirect   Total     \nper_mixed    0.013373   0.8959600  0.46010070\nper_asian    0.478062   0.3870692  0.30234457\nper_black    0.092334   0.1432912  0.06716843\nper_other    0.070701   0.0412926  0.01713506\nper_nonUK_EU 0.143091   2.1064e-05 5.3248e-06\nper_nonEU    0.011096   0.0023862  0.00053424\nlog(POPDEN)  1.7319e-14 0.2246336  0.30792015"
  },
  {
    "objectID": "09_comparison.html#general-advice",
    "href": "09_comparison.html#general-advice",
    "title": "9  Comparing and Selecting Models",
    "section": "9.3 General advice?",
    "text": "9.3 General advice?\nLeSage and Pace (2009), LeSage (2014), Elhorst (2014) argue that there are strong analytical reasons to restrict the model specifications to a subset, as the SDM subsumes the SLX and SAR model, and the SDEM subsumes SLX and SEM.\nIt is easily observed that SDM reduces to SLX if \\(\\rho=0\\) and to SAR if \\({\\boldsymbol{\\mathbf{\\theta}}}=0\\), while the SDEM reduces to SLX if \\(\\lambda=0\\) and to SEM if \\({\\boldsymbol{\\mathbf{\\theta}}}=0\\). Less intuitively, (Anselin 1988) has also shown that the SDM subsumes the SEM. Therefore, we can express the reduced form and rearrange terms:\n\\[\n\\begin{split}\n{\\boldsymbol{\\mathbf{y}}}&= {\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}} + ({\\boldsymbol{\\mathbf{I}}_N}-\\lambda {\\boldsymbol{\\mathbf{W}}})^{-1}{\\boldsymbol{\\mathbf{\\varepsilon}}} \\\\\n({\\boldsymbol{\\mathbf{I}}_N}-\\lambda {\\boldsymbol{\\mathbf{W}}}){\\boldsymbol{\\mathbf{y}}}&= ({\\boldsymbol{\\mathbf{I}}_N}-\\lambda {\\boldsymbol{\\mathbf{W}}}){\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}} + {\\boldsymbol{\\mathbf{\\varepsilon}}} \\\\\n({\\boldsymbol{\\mathbf{I}}_N}-\\lambda {\\boldsymbol{\\mathbf{W}}}){\\boldsymbol{\\mathbf{y}}}&={\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}} -\\lambda{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}} + {\\boldsymbol{\\mathbf{\\varepsilon}}} \\\\\n{\\boldsymbol{\\mathbf{y}}}&=({\\boldsymbol{\\mathbf{I}}_N}-\\lambda {\\boldsymbol{\\mathbf{W}}})^{-1}({\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}} + {\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\theta}}} + {\\boldsymbol{\\mathbf{\\varepsilon}}}).\n\\end{split}\n\\]\nThus, the SEM constitutes a special case of an SDM with the relative simple restriction \\({\\boldsymbol{\\mathbf{\\theta}}}=-\\lambda{\\boldsymbol{\\mathbf{\\beta}}}\\), meaning direct and indirect effects are constrained to a common factor (Anselin 1988, 2003).\nThe fact that SDM subsumes SAR, SLX, and SEM leads to the conclusion that applied research should only consider SDM and SDEM as model specifications (LeSage 2014). Especially in the case of a likely omitted variable bias, (LeSage and Pace 2009, ~68) argue in favour of using the SDM.\nNonetheless, others propose to use the SLX specification as point of departure (Gibbons and Overman 2012; Halleck Vega and Elhorst 2015). First, scholars have argued that SAC and SDM models are only weakly identified in practice (Gibbons and Overman 2012; Pinkse and Slade 2010). Second, the global spillover specification in SAR, SAC, and SDM often seems to be theoretically implausible.\nAnd finally:\n\n\n\n“I will use spatial lags of X, not spatial lags of Y”, J. Wooldridge on twitter"
  },
  {
    "objectID": "09_comparison.html#design-and-theory",
    "href": "09_comparison.html#design-and-theory",
    "title": "9  Comparing and Selecting Models",
    "section": "9.4 Design and Theory",
    "text": "9.4 Design and Theory\nSome argue that the best way of choosing the appropriate model specification is to exclude one or more sources of spatial dependence – autocorrelation in the dependent variable, autocorrelation in the disturbances, or spatial spillover effects of the covariates – by design Gibbons, Overman, and Patacchini (2015).\nNatural experiments are probably the best way of making one or more sources of spatial dependence unlikely, thereby restricting the model alternatives to a subset of all available models. However, the opportunities to use natural experiments are restricted in social sciences, making it a favourable but often impractical way of model selection.\nCook, Hays, and Franzese (2020) and Rüttenauer (2022) argue that theoretical considerations should guide the model selection.\n\nRule out some sources of spatial dependence by theory, and thus restrict the specifications to a subset ( Where does the spatial dependence come from? ),\nTheoretical mechanisms may guide the choice of either global or local spillover effects."
  },
  {
    "objectID": "10_spatiotemporal.html",
    "href": "10_spatiotemporal.html",
    "title": "10  Spatio-temporal models",
    "section": "",
    "text": "11 Comparing and Selecting Models"
  },
  {
    "objectID": "06_regression-estimation.html#simulataneity-bias",
    "href": "06_regression-estimation.html#simulataneity-bias",
    "title": "6  Spatial Regression Models: Estimation",
    "section": "6.1 Simulataneity bias",
    "text": "6.1 Simulataneity bias\nRemember what is happening when we estimate a spatial auto-regressive model.\n\nNote the circular process here: My \\(X\\) influences my \\(Y\\), which then influences your \\(Y\\), which then influences my \\(Y\\) again. We write this as\n\\[\n    \\begin{equation}\n        {\\boldsymbol{\\mathbf{y}}}=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+\\rho{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{y}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{\\varepsilon}}}.\n        \\end{equation}  \n\\]\nIf we ignore \\({\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}\\) and write the pure auto-regressive term in its reduce form, we get:\n\\[\n\\boldsymbol{\\mathbf{y}} =\\left(\\boldsymbol{\\mathbf{I}}_n - \\rho\\boldsymbol{\\mathbf{W}}\\right)^{-1}\\varepsilon,\n\\]\nand the spatial lag term is\n\\[\n\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{y}} =\\boldsymbol{\\mathbf{W}}\\left(\\boldsymbol{\\mathbf{I}}_n - \\rho\\boldsymbol{\\mathbf{W}}\\right)^{-1}\\varepsilon.\n\\]\nThe OLS estimator for the spatial lag term then is\n\\[\n\\hat{\\rho}_{OLS} = \\left[\\underbrace{\\left(\\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{y}} \\right)^\\top}_{(1\\times n)}\\underbrace{\\left(\\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{y}} \\right)}_{(n\\times 1)}\\right]^{-1}\\underbrace{\\left(\\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{y}} \\right)^\\top}_{(1\\times n)}\\underbrace{\\boldsymbol{\\mathbf{y}}}_{(n\\times 1)}.\n\\]\nIt can then be shown that the OLS estimators equals\n\\[\n          \\hat{\\rho}_{OLS} = \\rho + \\left[\\left(\\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{y}} \\right)^\\top\\left(\\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{y}} \\right)\\right]^{-1}\\left(\\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{y}} \\right)^\\top\\varepsilon \\\\\n                                = \\rho + \\left(\\sum_{i = 1}^n \\boldsymbol{\\mathbf{y}}_{Li}^2\\right)^{-1}\\left(\\sum_{i = 1}^{n}\\boldsymbol{\\mathbf{y}}_{Li}\\epsilon_i\\right),\n\\]\nwith \\(\\boldsymbol{\\mathbf{y}}_{Li}\\) defined as the \\(i\\)th element of the spatial lag operator \\(\\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{y}} = \\boldsymbol{\\mathbf{y}}_L\\). It can further be shown that the second part of the equation \\(\\neq 0\\), which demonstrates that OLS gives a biased estimate of \\(\\rho\\) (Franzese and Hays 2007; Sarrias 2023).\n\n\n\n\n\n\nWarning\n\n\n\nDo not estimate spatial lags of the dependent variable in OLS. It will suffer from simultaneity bias."
  },
  {
    "objectID": "06_regression-estimation.html#instrumental-variable",
    "href": "06_regression-estimation.html#instrumental-variable",
    "title": "6  Spatial Regression Models: Estimation",
    "section": "6.2 Instrumental variable",
    "text": "6.2 Instrumental variable\nA potential way of estimating spatial lag /SAR models is 2SLS (Kelejian and Prucha 1998).\nWe start with our standard model\n\\[\n        {\\boldsymbol{\\mathbf{y}}}=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+\\rho{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{y}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{\\varepsilon}}}.\n\\]\nAs we have seen above, there is a problem of simultaneity: the “covariate” \\({\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{y}}}\\) is endogenous. One way of dealing with this endogeneity problem is the Instrumental Variable approach.\nSo, the question is what are good instruments \\(\\boldsymbol{\\mathbf{H}}\\) for \\({\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{y}}}\\)? As we have specified the mode, we are sure that \\({\\boldsymbol{\\mathbf{X}}}\\) determines \\({\\boldsymbol{\\mathbf{y}}}\\). Thus, it must be true that \\({\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{X}}}\\) and \\({\\boldsymbol{\\mathbf{W}}}^2{\\boldsymbol{\\mathbf{X}}},\\ldots, {\\boldsymbol{\\mathbf{W}}}^l{\\boldsymbol{\\mathbf{X}}}\\) determines \\({\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{y}}}\\).\nNote that \\({\\boldsymbol{\\mathbf{W}}}^l\\) denotes higher orders of \\({\\boldsymbol{\\mathbf{W}}}\\). So \\({\\boldsymbol{\\mathbf{W}}}^2\\) are the second order neighbours (neighbours of neighbours), and \\({\\boldsymbol{\\mathbf{W}}}^3\\) are the third order neighbours (the neighbours of my neighbour’s neighbours), and so on…\nWe will discuss this in more detail later, but note for now that the reduced form of the SAR always contains a series of higher order neighbours.\n\\[\n({\\boldsymbol{\\mathbf{I}}_N}-\\rho {\\boldsymbol{\\mathbf{W}}})^{-1}\\beta_k\n=({\\boldsymbol{\\mathbf{I}}_N} + \\rho{\\boldsymbol{\\mathbf{W}}} + \\rho^2{\\boldsymbol{\\mathbf{W}}}^2 + \\rho^3{\\boldsymbol{\\mathbf{W}}}^3 + ...)\\beta_k\n= ({\\boldsymbol{\\mathbf{I}}_N} + \\sum_{h=1}^\\infty \\rho^h{\\boldsymbol{\\mathbf{W}}}^h)\\beta_k .\n\\]\nThus, Kelejian and Prucha (1998) suggested to use a set of lagged covariates as instruments for \\(\\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{Y}}\\):\n\\[\n\\boldsymbol{\\mathbf{H}} = \\boldsymbol{\\mathbf{X}}, \\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{X}}, \\boldsymbol{\\mathbf{W}}^2\\boldsymbol{\\mathbf{X}}, ... , \\boldsymbol{\\mathbf{W}}^l\\boldsymbol{\\mathbf{X}},\n\\]\nwhere \\(l\\) is a pre-defined number for the higher order neighbours included. In practice, \\(l\\) is usually restricted to \\(l=2\\).\nThis has further been developed by, for instance, using a (truncated) power series as instruments (Kelejian, Prucha, and Yuzefovich 2004):\n\\[\n\\boldsymbol{\\mathbf{H}} =\\left[\\boldsymbol{\\mathbf{X}}, \\boldsymbol{\\mathbf{W}}\\left(\\sum_{l = 1}^{\\infty}\\rho^{l}\\boldsymbol{\\mathbf{W}}^l\\right)\\boldsymbol{\\mathbf{X}} \\boldsymbol{\\mathbf{\\beta}}\\right].\n\\]\nWe can estimate this using the pacakge spatialreg with the function stsls(),\n\nmod_1.sls &lt;- stsls(log(med_house_price) ~ log(no2) + log(POPDEN) + \n                     per_mixed + per_asian + per_black + per_other,  \n                   data = msoa.spdf, \n                   listw = queens.lw,\n                   robust = TRUE, #  heteroskedasticity robust SEs\n                   W2X = TRUE) # Second order neighbours are included as instruments (else only first)\nsummary(mod_1.sls)\n\n\nCall:\nstsls(formula = log(med_house_price) ~ log(no2) + log(POPDEN) + \n    per_mixed + per_asian + per_black + per_other, data = msoa.spdf, \n    listw = queens.lw, robust = TRUE, W2X = TRUE)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.5464924 -0.1238002 -0.0052299  0.0989150  1.0793093 \n\nCoefficients: \n               Estimate HC0 std. Error z value  Pr(&gt;|z|)\nRho          0.71004211     0.04678235 15.1776 &lt; 2.2e-16\n(Intercept)  2.73582523     0.50997823  5.3646 8.113e-08\nlog(no2)     0.37752751     0.04920257  7.6729 1.688e-14\nlog(POPDEN) -0.05710992     0.01684036 -3.3913 0.0006957\nper_mixed    0.01634307     0.00588488  2.7771 0.0054842\nper_asian   -0.00205426     0.00045905 -4.4750 7.640e-06\nper_black   -0.01166456     0.00128557 -9.0734 &lt; 2.2e-16\nper_other   -0.00280423     0.00332302 -0.8439 0.3987377\n\nResidual variance (sigma squared): 0.035213, (sigma: 0.18765)"
  },
  {
    "objectID": "06_regression-estimation.html#generalized-method-of-moments",
    "href": "06_regression-estimation.html#generalized-method-of-moments",
    "title": "6  Spatial Regression Models: Estimation",
    "section": "6.3 Generalized Method of Moments",
    "text": "6.3 Generalized Method of Moments\nGeneralized Method of Moments (GMM) provides a way of estimating spatial error / SEM models. A motivation for GMM was that Maximum Likelihood was unfeasible for large samples and its consistent could not be shown. Kelejian and Prucha (1999) thus proposed a Moments estimator for SEM.\nWe start with the model\n\\[\n        \\begin{equation}\n        \\begin{split}\n        {\\boldsymbol{\\mathbf{y}}}&=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+{\\boldsymbol{\\mathbf{u}}},\\\\\n        {\\boldsymbol{\\mathbf{u}}}&=\\lambda{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{u}}}+{\\boldsymbol{\\mathbf{\\varepsilon}}}\n        \\end{split}\n        \\end{equation}\n\\]\nThe key issue here is to find a consistent estimator for \\(\\lambda\\). However, we usually do not want to draw inference about \\(\\lambda\\) itself, but only need it to consistently estimate \\(\\boldsymbol{\\mathbf{\\beta}}\\). Kelejian and Prucha (1999) thus treat \\(\\lambda\\) as pure nuisance paramter.\nIn essence, the GMM works as follows (Sarrias 2023):\n\nFirst of all obtain a consistent estimate of \\(\\boldsymbol{\\mathbf{\\beta}}\\), say \\(\\widetilde{\\boldsymbol{\\mathbf{\\beta}}}\\) using either OLS or non-linear least squares (NLS).\nUse this estimate to obtain an estimate of \\(\\boldsymbol{\\mathbf{u}}\\), say \\(\\widehat{\\boldsymbol{\\mathbf{u}}}\\),\nUse \\(\\widehat{\\boldsymbol{\\mathbf{u}}}\\), to estimate \\(\\lambda\\), say \\(\\widehat{\\lambda}\\), using\n\n\\[\n  (\\widehat{\\lambda}_{NLS, n}, \\widehat{\\sigma}^2_{NLS, N}) = \\mathrm{argmin} \\left\\lbrace \\boldsymbol{\\mathbf{\\upsilon}}_n(\\lambda, \\sigma^2)^\\top\\boldsymbol{\\mathbf{\\upsilon}}_n(\\lambda, \\sigma^2): \\rho \\in [-a, a], \\sigma^2\\in [0, b]\\right\\rbrace,\n\\]\n\nEstimate \\(\\boldsymbol{\\mathbf{\\beta}}\\) using Equation\n\n\\[\n\\begin{split}\n\\boldsymbol{\\mathbf{\\beta}}_{FGLS}(\\lambda) &=\\left[\\boldsymbol{\\mathbf{X}}^\\top\\boldsymbol{\\mathbf{\\Omega}}(\\widehat{\\lambda})^{-1}\\boldsymbol{\\mathbf{X}}\\right]^{-1}\\boldsymbol{\\mathbf{X}}^\\top\\boldsymbol{\\mathbf{\\Omega}}(\\widehat{\\lambda})^{-1}\\boldsymbol{\\mathbf{y}}.\\\\\n\\boldsymbol{\\mathbf{\\Omega}}(\\lambda) &= (\\boldsymbol{\\mathbf{I}} - \\lambda\\boldsymbol{\\mathbf{W}})^{-1}(\\boldsymbol{\\mathbf{I}} - \\lambda\\boldsymbol{\\mathbf{W}}^\\top)^{-1}\n\\end{split}\n\\]\nFor more, see for instance Kelejian and Piras (2017), chapter 2.2.4 or Sarrias (2023).\nWe can calculate the estimator using GMerrorsar() from spatialreg.\n\nmod_1.gmm &lt;- GMerrorsar(log(med_house_price) ~ log(no2) + log(POPDEN) + \n                     per_mixed + per_asian + per_black + per_other,  \n                   data = msoa.spdf, \n                   listw = queens.lw,\n                   se.lambda = TRUE) # Provide standard error for lambda\nsummary(mod_1.gmm)\n\n\nCall:\nGMerrorsar(formula = log(med_house_price) ~ log(no2) + log(POPDEN) + \n    per_mixed + per_asian + per_black + per_other, data = msoa.spdf, \n    listw = queens.lw, se.lambda = TRUE)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.8275979 -0.1840855 -0.0096616  0.1610019  1.2270026 \n\nType: GM SAR estimator\nCoefficients: (GM standard errors) \n               Estimate  Std. Error  z value  Pr(&gt;|z|)\n(Intercept) 11.07612114  0.26596129  41.6456 &lt; 2.2e-16\nlog(no2)     0.67758095  0.08620995   7.8597 3.775e-15\nlog(POPDEN) -0.08006377  0.01464953  -5.4653 4.622e-08\nper_mixed   -0.01307831  0.00894766  -1.4616    0.1438\nper_asian   -0.00521983  0.00090937  -5.7400 9.465e-09\nper_black   -0.01957288  0.00134527 -14.5494 &lt; 2.2e-16\nper_other   -0.00521695  0.00489760  -1.0652    0.2868\n\nLambda: 0.69344 (standard error): 0.071248 (z-value): 9.7328\nResidual variance (sigma squared): 0.037126, (sigma: 0.19268)\nGM argmin sigma squared: 0.037239\nNumber of observations: 983 \nNumber of parameters estimated: 9"
  },
  {
    "objectID": "06_regression-estimation.html#maximum-likelihood-estimation",
    "href": "06_regression-estimation.html#maximum-likelihood-estimation",
    "title": "6  Spatial Regression Models: Estimation",
    "section": "6.4 Maximum likelihood estimation",
    "text": "6.4 Maximum likelihood estimation\n\n6.4.1 ML SAR\nMaximum Likelihood estimation of spatial models is the most common way of estimation. The procedure to estimate Sar models via ML is based on Ord (1975) and Anselin (1988).\nStarting with\n\\[\n\\begin{split}\n    \\boldsymbol{\\mathbf{y}}  = \\rho \\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{y}} + \\boldsymbol{\\mathbf{X}}\\boldsymbol{\\mathbf{\\beta }}+ \\varepsilon, \\\\\n     \\varepsilon  \\sim \\mathcal{N}(\\boldsymbol{\\mathbf{0}}_n , \\sigma^2\\boldsymbol{\\mathbf{I}}_n),\n\\end{split}     \n\\]\nand its reduced form\n\\[\n\\begin{split}\n{\\boldsymbol{\\mathbf{y}}} =({\\boldsymbol{\\mathbf{I}}_N}-\\rho {\\boldsymbol{\\mathbf{W}}})^{-1}({\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{\\varepsilon}}}), \\\\\n{\\boldsymbol{\\mathbf{y}}} =\\boldsymbol{\\mathbf{A}}^{-1}({\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{\\varepsilon}}}),\n\\end{split}\n\\]\nwhere \\(\\boldsymbol{\\mathbf{A}} = ({\\boldsymbol{\\mathbf{I}}_N}-\\rho {\\boldsymbol{\\mathbf{W}}})\\).\nThe ML estimator then choses the parameters \\(\\hat\\rho\\), \\(\\hat{\\boldsymbol{\\mathbf{\\beta}}}\\), and \\(\\hat\\sigma\\) to maximize the probability of fitting the observed sample based on the Likelihood function\n\\[\n\\begin{split}\n\\mathcal{L} (\\boldsymbol{\\mathbf{\\theta}}) &= \\log\\left| \\boldsymbol{\\mathbf{A}}\\right| - \\frac{n\\log(2\\pi)}{2} - \\frac{n\\log(\\sigma^2)}{2} - \\frac{1}{2\\sigma^2}(\\boldsymbol{\\mathbf{A}}\\boldsymbol{\\mathbf{y}}-\\boldsymbol{\\mathbf{X}}\\boldsymbol{\\mathbf{\\beta}})^\\top (\\boldsymbol{\\mathbf{A}}\\boldsymbol{\\mathbf{y}}-\\boldsymbol{\\mathbf{X}}\\boldsymbol{\\mathbf{\\beta}}) \\\\\n&= \\log\\left| \\boldsymbol{\\mathbf{A}}\\right| - \\frac{n\\log(2\\pi)}{2} - \\frac{n\\log(\\sigma^2)}{2} - \\frac{1}{2\\sigma^2}\\left[\\boldsymbol{\\mathbf{y}}^\\top \\boldsymbol{\\mathbf{A}}^\\top\\boldsymbol{\\mathbf{A}}\\boldsymbol{\\mathbf{y}} - 2\\left(\\boldsymbol{\\mathbf{A}}\\boldsymbol{\\mathbf{y}}\\right)^\\top\\boldsymbol{\\mathbf{X}}\\boldsymbol{\\mathbf{\\beta }}+ \\boldsymbol{\\mathbf{\\beta}}^\\top\\boldsymbol{\\mathbf{X}}^\\top\\boldsymbol{\\mathbf{X}}\\boldsymbol{\\mathbf{\\beta}}\\right],\n\\end{split}\n\\]\nML estimation of the SAR works as follows Sarrias (2023):\n\nPerform the two auxiliary regression of \\(\\boldsymbol{\\mathbf{y}}\\) and \\(\\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{y}}\\) on \\(\\boldsymbol{\\mathbf{X}}\\) to obtain the estimators \\(\\widehat{\\boldsymbol{\\mathbf{\\beta}}}_O\\) and \\(\\widehat{\\boldsymbol{\\mathbf{\\beta}}}_L\\) as in Equation \\[\n\\begin{split}\n\\widehat{\\boldsymbol{\\mathbf{\\beta}}}_{ML}(\\rho) &= \\left(\\boldsymbol{\\mathbf{X}}^\\top\\boldsymbol{\\mathbf{X}}\\right)^{-1}\\boldsymbol{\\mathbf{X}}^\\top\\boldsymbol{\\mathbf{y}} - \\rho\\left(\\boldsymbol{\\mathbf{X}}^\\top\\boldsymbol{\\mathbf{X}}\\right)^{-1}\\boldsymbol{\\mathbf{X}}^\\top\\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{y}}, \\\\\n&= \\widehat{\\boldsymbol{\\mathbf{\\beta}}}_O -\\rho \\widehat{\\boldsymbol{\\mathbf{\\beta}}}_L.\n\\end{split}\n\\]\nUse \\(\\widehat{\\boldsymbol{\\mathbf{\\beta}}}_O\\) and \\(\\widehat{\\boldsymbol{\\mathbf{\\beta}}}_L\\) to compute the residuals in Equation \\[\n\\varepsilon_O = \\boldsymbol{\\mathbf{y}} - \\boldsymbol{\\mathbf{X}}\\widehat{\\boldsymbol{\\mathbf{\\beta}}}_0\\,\\,\\mbox{and} \\;\\; \\varepsilon_L = \\boldsymbol{\\mathbf{W}}\\boldsymbol{\\mathbf{y}} - \\boldsymbol{\\mathbf{X}}\\widehat{\\boldsymbol{\\mathbf{\\beta}}_L}.\n\\]\nBy numerical optimization to obtain an estimate of \\(\\rho\\), maximize the concentrated likelihood given in\n\n\\[\n\\ell(\\rho)=-\\frac{n}{2}-\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log\\left[\\frac{\\left(\\varepsilon_O - \\rho\\varepsilon_L\\right)^\\top\\left(\\varepsilon_O - \\rho\\varepsilon_L\\right)}{n}\\right] + \\log\\left|\\boldsymbol{\\mathbf{I}}_n - \\rho\\boldsymbol{\\mathbf{W}}\\right|,\n\\]\n\nUse the estimate of \\(\\widehat{\\rho}\\) to plug it back in to the expression for \\(\\boldsymbol{\\mathbf{\\beta}}\\) and \\(\\sigma^2\\)\n\n\\[\n\\begin{split}\n\\widehat{\\boldsymbol{\\mathbf{\\beta}}}_{ML}(\\rho) = \\left(\\boldsymbol{\\mathbf{X}}^\\top\\boldsymbol{\\mathbf{X}}\\right)^{-1}\\boldsymbol{\\mathbf{X}}^\\top\\boldsymbol{\\mathbf{A}}\\boldsymbol{\\mathbf{y}}\n\\widehat{\\sigma}^2_{ML}(\\rho) =\\\\\n\\frac{\\left(\\boldsymbol{\\mathbf{A}}\\boldsymbol{\\mathbf{y}} - \\boldsymbol{\\mathbf{X}}\\boldsymbol{\\mathbf{\\beta}}_{ML}\\right)^\\top\\left(\\boldsymbol{\\mathbf{A}}\\boldsymbol{\\mathbf{y}} - \\boldsymbol{\\mathbf{X}}\\boldsymbol{\\mathbf{\\beta}}_{ML}\\right)}{n}\n\\end{split}\n\\]\n\n\n6.4.2 ML SEM\nWe can also use ML to estimate the spatial error / SEM model of the form\n\\[\n        \\begin{equation}\n        \\begin{split}\n        {\\boldsymbol{\\mathbf{y}}}&=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+{\\boldsymbol{\\mathbf{u}}},\\\\\n        {\\boldsymbol{\\mathbf{u}}}&=\\lambda{\\boldsymbol{\\mathbf{W}}}{\\boldsymbol{\\mathbf{u}}}+{\\boldsymbol{\\mathbf{\\varepsilon}}}\\\\\n        \\varepsilon  &\\sim \\mathcal{N}(\\boldsymbol{\\mathbf{0}}_n , \\sigma^2\\boldsymbol{\\mathbf{I}}_n)\n        \\end{split}\n        \\end{equation}\n\\] Its reduce for is given by\n\\[\n        \\begin{equation}\n        \\begin{split}\n        {\\boldsymbol{\\mathbf{y}}}&=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+({\\boldsymbol{\\mathbf{I}}_N}-\\lambda {\\boldsymbol{\\mathbf{W}}})^{-1}{\\boldsymbol{\\mathbf{\\varepsilon}}}.\\\\\n        {\\boldsymbol{\\mathbf{y}}}&=\\alpha{\\boldsymbol{\\mathbf{\\iota}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+\\boldsymbol{\\mathbf{B}}^{-1}{\\boldsymbol{\\mathbf{\\varepsilon}}}.\n        \\end{split}\n        \\end{equation}\n\\]\nwhere \\(\\boldsymbol{\\mathbf{B}} = ({\\boldsymbol{\\mathbf{I}}_N}-\\lambda {\\boldsymbol{\\mathbf{W}}})\\).\nNote that the OLS estimate of the SEM model are unbiased – if there is no omitted variable bias! However, even in that case, they are inefficient if \\(\\lambda \\neq 0\\).\nThe log-likelihood function is given by\n\\[\n\\begin{split}\n\\ell(\\boldsymbol{\\mathbf{\\theta}}) = - \\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2)-\\frac{(\\boldsymbol{\\mathbf{y}} - \\boldsymbol{\\mathbf{X}}\\boldsymbol{\\mathbf{\\beta}})^\\top \\boldsymbol{\\mathbf{\\Omega}}(\\lambda) (\\boldsymbol{\\mathbf{y}} - \\boldsymbol{\\mathbf{X}}\\boldsymbol{\\mathbf{\\beta}})}{2\\sigma^2} + \\log\\left|\\boldsymbol{\\mathbf{I}}_n - \\lambda \\boldsymbol{\\mathbf{W}}\\right|, \\\\\n\\boldsymbol{\\mathbf{\\Omega}}(\\lambda) = \\boldsymbol{\\mathbf{B}}^\\top \\boldsymbol{\\mathbf{B}} = \\left(\\boldsymbol{\\mathbf{I}}_n-\\lambda\\boldsymbol{\\mathbf{W}}\\right)^\\top \\left(\\boldsymbol{\\mathbf{I}}_n-\\lambda\\boldsymbol{\\mathbf{W}}\\right)\n\\end{split}\n\\]\nBased on Anselin and Bera (1998), the ML estimation of SEM follow the procedure (Sarrias 2023):\n\nCarry out an OLS of \\(\\boldsymbol{\\mathbf{B}}\\boldsymbol{\\mathbf{X}}\\) on \\(\\boldsymbol{\\mathbf{B}}\\boldsymbol{\\mathbf{y}}\\); get \\(\\widehat{\\boldsymbol{\\mathbf{\\beta}}}_{OLS}\\)\nCompute initial set of residuals \\(\\widehat{\\epsilon}_{OLS} = \\boldsymbol{\\mathbf{B}}\\boldsymbol{\\mathbf{y}} - \\boldsymbol{\\mathbf{B}}\\boldsymbol{\\mathbf{X}}\\widehat{\\boldsymbol{\\mathbf{\\beta}}}_{OLS}\\)\nGiven $_{OLS} $, find \\(\\widehat{\\lambda}\\) that maximizes the concentrated likelihood\n\n\\[\n\\ell(\\lambda)= \\mbox{const} + \\frac{n}{2}\\log\\left[\\frac{1}{n}\\widehat{\\boldsymbol{\\mathbf{\\varepsilon}}}^\\top\\boldsymbol{\\mathbf{B}}^\\top\\boldsymbol{\\mathbf{B}} \\widehat{\\boldsymbol{\\mathbf{\\varepsilon}}}\\right] + \\log\\left|\\boldsymbol{\\mathbf{B}}\\right|.\n\\]\n\nIf the convergence criterion is met, proceed, otherwise repeat steps 1, 2 and 3.\nGiven \\(\\widehat{\\lambda}\\), estimate \\(\\widehat{\\boldsymbol{\\mathbf{\\beta}}}(\\lambda)\\) by GLS and obtain a new vector of residuals, \\(\\widehat{\\boldsymbol{\\mathbf{\\varepsilon}}}(\\lambda)\\)\nGiven \\(\\widehat{\\boldsymbol{\\mathbf{\\varepsilon}}}(\\lambda)\\) and \\(\\widehat{\\lambda}\\), estimate \\(\\widehat{\\sigma}(\\lambda)\\).\n\nThe package spatialreg Pebesma and Bivand (2023) provides a series of functions to calculate the ML estimator for all spatial models we have considered.\nTable from Pebesma and Bivand (2023):\n\n\n\n\n\n\n\n\nmodel\nmodel name\nmaximum likelihood estimation function\n\n\n\n\nSEM\nspatial error\nerrorsarlm(..., Durbin=FALSE)\n\n\nSEM\nspatial error\nspautolm(..., family=\"SAR\")\n\n\nSDEM\nspatial Durbin error\nerrorsarlm(..., Durbin=TRUE)\n\n\nSLM\nspatial lag\nlagsarlm(..., Durbin=FALSE)\n\n\nSDM\nspatial Durbin\nlagsarlm(..., Durbin=TRUE)\n\n\nSAC\nspatial autoregressive combined\nsacsarlm(..., Durbin=FALSE)\n\n\nGNM\ngeneral nested\nsacsarlm(..., Durbin=TRUE)\n\n\n\nML SAR\n\nmod_1.sar &lt;- lagsarlm(log(med_house_price) ~ log(no2) + log(POPDEN) + \n                        per_mixed + per_asian + per_black + per_other,  \n                      data = msoa.spdf, \n                      listw = queens.lw,\n                      Durbin = FALSE) # we could here extend to SDM\nsummary(mod_1.sar)\n\n\nCall:\nlagsarlm(formula = log(med_house_price) ~ log(no2) + log(POPDEN) + \n    per_mixed + per_asian + per_black + per_other, data = msoa.spdf, \n    listw = queens.lw, Durbin = FALSE)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.5281789 -0.1220524 -0.0099245  0.0992203  1.0936745 \n\nType: lag \nCoefficients: (asymptotic standard errors) \n               Estimate  Std. Error  z value  Pr(&gt;|z|)\n(Intercept)  3.17383180  0.29041604  10.9286 &lt; 2.2e-16\nlog(no2)     0.39705423  0.04452880   8.9168 &lt; 2.2e-16\nlog(POPDEN) -0.05583014  0.01242876  -4.4920 7.055e-06\nper_mixed    0.01851577  0.00579832   3.1933  0.001407\nper_asian   -0.00228346  0.00045876  -4.9775 6.442e-07\nper_black   -0.01263650  0.00100282 -12.6009 &lt; 2.2e-16\nper_other   -0.00161419  0.00289082  -0.5584  0.576582\n\nRho: 0.66976, LR test value: 473.23, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.025311\n    z-value: 26.461, p-value: &lt; 2.22e-16\nWald statistic: 700.19, p-value: &lt; 2.22e-16\n\nLog likelihood: 196.7203 for lag model\nML residual variance (sigma squared): 0.035402, (sigma: 0.18815)\nNumber of observations: 983 \nNumber of parameters estimated: 9 \nAIC: -375.44, (AIC for lm: 95.786)\nLM test for residual autocorrelation\ntest value: 8.609, p-value: 0.0033451\n\n\nML SEM\n\nmod_1.sem &lt;- errorsarlm(log(med_house_price) ~ log(no2) + log(POPDEN) +\n                          per_mixed + per_asian + per_black + per_other,  \n                        data = msoa.spdf, \n                        listw = queens.lw,\n                        Durbin = FALSE) # we could here extend to SDEM\nsummary(mod_1.sem)\n\n\nCall:\nerrorsarlm(formula = log(med_house_price) ~ log(no2) + log(POPDEN) + \n    per_mixed + per_asian + per_black + per_other, data = msoa.spdf, \n    listw = queens.lw, Durbin = FALSE)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.581785 -0.105218 -0.012758  0.094430  0.913425 \n\nType: error \nCoefficients: (asymptotic standard errors) \n               Estimate  Std. Error  z value  Pr(&gt;|z|)\n(Intercept) 12.92801104  0.35239139  36.6865 &lt; 2.2e-16\nlog(no2)     0.15735296  0.10880727   1.4462 0.1481317\nlog(POPDEN) -0.08316270  0.01254315  -6.6301 3.354e-11\nper_mixed   -0.03377962  0.00811054  -4.1649 3.115e-05\nper_asian   -0.00413115  0.00096849  -4.2656 1.994e-05\nper_black   -0.01653816  0.00126741 -13.0488 &lt; 2.2e-16\nper_other   -0.01693012  0.00462999  -3.6566 0.0002556\n\nLambda: 0.88605, LR test value: 623.55, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.015803\n    z-value: 56.068, p-value: &lt; 2.22e-16\nWald statistic: 3143.6, p-value: &lt; 2.22e-16\n\nLog likelihood: 271.8839 for error model\nML residual variance (sigma squared): 0.026911, (sigma: 0.16405)\nNumber of observations: 983 \nNumber of parameters estimated: 9 \nAIC: NA (not available for weighted model), (AIC for lm: 95.786)\n\n\n\n\n\n\n\n\nAnselin, Luc. 1988. Spatial Econometrics: Methods and Models. Studies in Operational Regional Science. Dordrecht: Kluwer.\n\n\nAnselin, Luc, and Anil K. Bera. 1998. “Spatial Dependence in Linear Regression Models with an Introduction to Spatial Econometrics.” In Handbook of Applied Economic Statistics, edited by Aman Ullah and David E. A. Giles, 237–89. New York: Dekker.\n\n\nBivand, Roger, Giovanni Millo, and Gianfranco Piras. 2021. “A Review of Software for Spatial Econometrics in R.” Mathematics 9 (11): 1276. https://doi.org/10.3390/math9111276.\n\n\nDrukker, David M., Peter Egger, and Ingmar R. Prucha. 2013. “On Two-Step Estimation of a Spatial Autoregressive Model with Autoregressive Disturbances and Endogenous Regressors.” Econometric Reviews 32 (5-6): 686–733. https://doi.org/10.1080/07474938.2013.741020.\n\n\nFranzese, Robert J., and Jude C. Hays. 2007. “Spatial Econometric Models of Cross-Sectional Interdependence in Political Science Panel and Time-Series-Cross-Section Data.” Political Analysis 15 (2): 140–64. https://doi.org/10.1093/pan/mpm005.\n\n\nKelejian, Harry H., and Gianfranco Piras. 2017. Spatial Econometrics. Elsevier. https://doi.org/10.1016/C2016-0-04332-2.\n\n\nKelejian, Harry H., and Ingmar R. Prucha. 1998. “A Generalized Spatial Two-Stage Least Squares Procedure for Estimating a Spatial Autoregressive Model with Autoregressive Disturbances.” The Journal of Real Estate Finance and Economics 17 (1): 99–121. https://doi.org/10.1023/A:1007707430416.\n\n\n———. 1999. “A Generalized Moments Estimator for the Autoregressive Parameter in a Spatial Model.” International Economic Review 40 (2): 509–33. https://doi.org/10.1111/1468-2354.00027.\n\n\n———. 2010. “Specification and Estimation of Spatial Autoregressive Models with Autoregressive and Heteroskedastic Disturbances.” Journal of Econometrics 157 (1): 53–67. https://doi.org/10.1016/j.jeconom.2009.10.025.\n\n\nKelejian, Harry H., Ingmar R. Prucha, and Yevgeny Yuzefovich. 2004. “Instrumental Variable Estimation of a Spatial Autoregressive Model with Autoregressive Disturbances: Large and Small Sample Results.” In Spatial and Spatiotemporal Econometrics, edited by James P. LeSage and R. Kelley Pace, 163–98. Advances in Econometrics. Amsterdam and Boston: Elsevier.\n\n\nLee, Lung-fei. 2004. “Asymptotic Distributions of Quasi-Maximum Likelihood Estimators for Spatial Autoregressive Models.” Econometrica 72 (6): 1899–1925.\n\n\nOrd, John Keith. 1975. “Estimation Methods for Models of Spatial Interaction.” Journal of the American Statistical Association 70 (349): 120–26. https://doi.org/10.2307/2285387.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With Applications in R. First. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nSarrias, Mauricio. 2023. Intermediate Spatial Econometrics with Applications in R."
  },
  {
    "objectID": "10_spatiotemporal.html#static-panel-data-models",
    "href": "10_spatiotemporal.html#static-panel-data-models",
    "title": "10  Spatio-temporal models",
    "section": "10.1 Static panel data models",
    "text": "10.1 Static panel data models\nThe idea behind a static panel data with auto-regressive term is similar to the cross sectional situation (Millo and Piras 2012).\n\\[\n        {\\boldsymbol{\\mathbf{y}}}= \\rho(\\boldsymbol{\\mathbf{I}}_T\\otimes {\\boldsymbol{\\mathbf{W}}_N}){\\boldsymbol{\\mathbf{y}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{u}}}.\n\\]\nwhere \\(\\otimes\\) is the Kronecker product (block-wise multiplication).\n\\[\n\\begin{split}\n\\underbrace{\\underbrace{\\boldsymbol{\\mathbf{I}}_T}_{T \\times T} \\otimes \\underbrace{\\boldsymbol{\\mathbf{W}}_N}_{N \\times N}}_{NT \\times NT}=\n\\begin{pmatrix}\n      1 & 0 & \\cdots & 0  \\\\\n      0 & 1 & \\cdots & 0  \\\\\n      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n      0 & 0 & \\cdots & 1\n\\end{pmatrix}\n\\left[\\begin{array}{cccc}\nv_{1} w_{1} & v_{1} w_{2} & \\cdots & v_{1} w_{m} \\\\\nv_{2} w_{1} & v_{2} w_{2} & \\cdots & v_{2} w_{m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nv_{n} w_{1} & v_{n} w_{2} & \\cdots & v_{n} w_{m}\n\\end{array}\\right] =\\\\\n\\begin{pmatrix}\n      \\left[\\begin{array}{cccc}\nv_{1} w_{1} & v_{1} w_{2} & \\cdots & v_{1} w_{m} \\\\\nv_{2} w_{1} & v_{2} w_{2} & \\cdots & v_{2} w_{m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nv_{n} w_{1} & v_{n} w_{2} & \\cdots & v_{n} w_{m}\n\\end{array}\\right] & 0 & \\cdots & 0  \\\\\n      0 & \\left[\\begin{array}{cccc}\nv_{1} w_{1} & v_{1} w_{2} & \\cdots & v_{1} w_{m} \\\\\nv_{2} w_{1} & v_{2} w_{2} & \\cdots & v_{2} w_{m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nv_{n} w_{1} & v_{n} w_{2} & \\cdots & v_{n} w_{m}\n\\end{array}\\right] & \\cdots & 0  \\\\\n      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n      0 & 0 & \\cdots & \\left[\\begin{array}{cccc}\nv_{1} w_{1} & v_{1} w_{2} & \\cdots & v_{1} w_{m} \\\\\nv_{2} w_{1} & v_{2} w_{2} & \\cdots & v_{2} w_{m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nv_{n} w_{1} & v_{n} w_{2} & \\cdots & v_{n} w_{m}\n\\end{array}\\right]\n\\end{pmatrix}\n\\end{split}\n\\]\nHere we model only spatial dependence within each cross-section and multiply the same spatial weights matrix \\(T\\) times. Off block-diagonal elements are all zero. So there is no spatial dependence that goes across time.\nThe error term can be decomposed into two parts:\n\\[\n        {\\boldsymbol{\\mathbf{u}}}= (\\boldsymbol{\\mathbf{\\iota}}_T \\otimes {\\boldsymbol{\\mathbf{I}}_N})\\boldsymbol{\\mathbf{\\mu}}+ {\\boldsymbol{\\mathbf{\\nu}}},\n\\]\nwhere \\(\\boldsymbol{\\mathbf{\\iota}}_T\\) is a \\(T \\times 1\\) vector of ones, \\({\\boldsymbol{\\mathbf{I}}_N}\\) an \\(N \\times N\\) identity matrix, \\(\\boldsymbol{\\mathbf{\\mu}}\\) is a vector of time-invariant individual specific effects (not spatially autocorrelated).\nWe could obviously extent the specification to allow for error correlation by specifying\n\\[\n        {\\boldsymbol{\\mathbf{\\nu}}}= \\lambda(\\boldsymbol{\\mathbf{I}}_T \\otimes {\\boldsymbol{\\mathbf{W}}_N})\\boldsymbol{\\mathbf{\\nu }}+ {\\boldsymbol{\\mathbf{\\varepsilon}}}.\n\\]\nThe individual effects can be treated as fixed or random.\nFixed Effects\nIn the FE model, the individual specific effects are treated as fixed. If we re-write the equation above, we derive at the well-know fixed effects formula with an additional spatial autoregressive term:\n\\[\n        {y_{it}}= \\rho\\sum_{j=1}^Nw_{ij}y_{jt} + \\boldsymbol{\\mathbf{x}}_{it}\\boldsymbol{\\mathbf{\\beta }}+ \\mu_i + \\nu_{it},\n\\] where \\(\\mu_i\\) denote the individual-specific fixed effects.\nAs with the standard spatial lag model, we cannot rely on the OLS estimator because of the simultaneity problem. The coefficients are thus estimated by maximum likelihood (Elhorst 2014).\nRandom Effects\nIn the RE model, the individual specific effects are treated as components of the error \\(\\mu \\sim \\mathrm{IID}(o, \\sigma_\\mu^2)\\). The model can then be written as\n\\[\n\\begin{split}\n        {\\boldsymbol{\\mathbf{y}}}= \\rho(\\boldsymbol{\\mathbf{I}}_T\\otimes {\\boldsymbol{\\mathbf{W}}_N}){\\boldsymbol{\\mathbf{y}}}+{\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ {\\boldsymbol{\\mathbf{u}}}, \\\\\n        {\\boldsymbol{\\mathbf{u}}}= (\\boldsymbol{\\mathbf{\\iota}}_T \\otimes {\\boldsymbol{\\mathbf{I}}_N})\\boldsymbol{\\mathbf{\\mu}}+ [\\boldsymbol{\\mathbf{I}}_T \\otimes (\\boldsymbol{\\mathbf{I}}_N -  \\lambda{\\boldsymbol{\\mathbf{W}}_N})]^{-1} {\\boldsymbol{\\mathbf{\\varepsilon}}}.\n\\end{split}     \n\\]\nAs with the conventional random effects model, we make the strong assumption that the unobserved individual effects are uncorrelated with the covariates \\(\\boldsymbol{\\mathbf{X}}\\) in the model."
  },
  {
    "objectID": "10_spatiotemporal.html#dynamic-panel-data-models",
    "href": "10_spatiotemporal.html#dynamic-panel-data-models",
    "title": "10  Spatio-temporal models",
    "section": "10.2 Dynamic panel data models",
    "text": "10.2 Dynamic panel data models\nRelying on panel data and repeated measures over time, comes with an additional layer of dependence / autocorrelation between units. We have spatial dependence (with its three potential sources), and we have temporal/serial dependence within each unit over time.\nA general dynamic model would account for all sources of potential dependence, including combinations (Elhorst 2012). The most general model can be written as:\n\\[\n\\begin{split}\n        {\\boldsymbol{\\mathbf{y}}_t}=& \\tau \\boldsymbol{\\mathbf{y}}_{t-1} + \\rho(\\boldsymbol{\\mathbf{I}}_T\\otimes {\\boldsymbol{\\mathbf{W}}_N}){\\boldsymbol{\\mathbf{y}}}_t\n        + \\gamma(\\boldsymbol{\\mathbf{I}}_T\\otimes {\\boldsymbol{\\mathbf{W}}_N}){\\boldsymbol{\\mathbf{y}}_{t-1}}\\\\\n        &~+ {\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\beta}}}+ (\\boldsymbol{\\mathbf{I}}_T\\otimes {\\boldsymbol{\\mathbf{W}}_N}){\\boldsymbol{\\mathbf{X}}}{\\boldsymbol{\\mathbf{\\theta}}}+ {\\boldsymbol{\\mathbf{u}}}_t,\\\\\n        {\\boldsymbol{\\mathbf{u}}_t}=&  + (\\boldsymbol{\\mathbf{\\iota}}_T \\otimes {\\boldsymbol{\\mathbf{I}}_N})\\boldsymbol{\\mathbf{\\mu}}+ {\\boldsymbol{\\mathbf{\\nu}}_t},\\\\\n        {\\boldsymbol{\\mathbf{\\nu}}_t}=& \\psi{\\boldsymbol{\\mathbf{\\nu}}}_{t-1} + \\lambda(\\boldsymbol{\\mathbf{I}}_T \\otimes {\\boldsymbol{\\mathbf{W}}_N})\\boldsymbol{\\mathbf{\\nu }}+ {\\boldsymbol{\\mathbf{\\varepsilon}}},\n\\end{split}     \n\\]\nWhere \\({\\boldsymbol{\\mathbf{X}}}\\) could further contain time-lagged covariates. Compared to the static spatial panel model, we have introduced temporal dependency in the outcome \\(\\tau \\boldsymbol{\\mathbf{y}}_{t-1}\\) and the spatially lagged outcome \\(\\gamma(\\boldsymbol{\\mathbf{I}}_T\\otimes {\\boldsymbol{\\mathbf{W}}_N}){\\boldsymbol{\\mathbf{y}}_{t-1}}\\), and in the error term \\(\\psi{\\boldsymbol{\\mathbf{\\nu}}}_{t-1}\\).\n\n10.2.1 Impacts in spatial panel models\nNote that similar to the distinction between local and global spillovers, we now have to distinguish between short-term and long-term effects. A change in \\(\\boldsymbol{\\mathbf{X}}_t\\) no influences focal \\(Y\\) and neighbour’s \\(Y\\) but also contemporaneous \\(Y\\) and future \\(Y\\).\nWhile the short-term effects are the known impacts\n\\[\n\\frac{\\partial {\\boldsymbol{\\mathbf{y}}}}{\\partial {\\boldsymbol{\\mathbf{x}}}_k} = ({\\boldsymbol{\\mathbf{I}}}-\\rho{\\boldsymbol{\\mathbf{W}}_{NT}})^{-1}\\left[\\beta_k+{\\boldsymbol{\\mathbf{W}}_{NT}}\\theta_k\\right].\n\\]\nThe long-term impacts, by contrast, additionally account for the effect multiplying through time\n\\[\n\\frac{\\partial {\\boldsymbol{\\mathbf{y}}}}{\\partial {\\boldsymbol{\\mathbf{x}}}_k} = [(1-\\tau){\\boldsymbol{\\mathbf{I}}}-(\\rho+\\gamma){\\boldsymbol{\\mathbf{W}}_{NT}}]^{-1}\\left[\\beta_k+{\\boldsymbol{\\mathbf{W}}_{NT}}\\theta_k\\right].\n\\]\nFor more information see Elhorst (2012).\n\n\n\nSummary impact measures in dynamic spatial panel models (Elhorst 2012)"
  },
  {
    "objectID": "10_spatiotemporal.html#example-industrial-facilities-in-municipal-income",
    "href": "10_spatiotemporal.html#example-industrial-facilities-in-municipal-income",
    "title": "10  Spatio-temporal models",
    "section": "11.3 Example: Industrial facilities in municipal income",
    "text": "11.3 Example: Industrial facilities in municipal income\nRüttenauer and Best (2021): Environmental Inequality and Residential Sorting in Germany: A Spatial Time-Series Analysis of the Demographic Consequences of Industrial Sites. Demography, 58(6), 2243–2263. https://doi.org/10.1215/00703370-9563077\n\n\n\nSpatial distribution of industrial facilities and income tax revenue per municipality for 2015.\n\n\n\n\n\n\n\n\n\nCroissant, Yves, and Givanni Millo, eds. 2018. “Spatial Panels.” In Panel Data Econometrics with R, 245–84. Chichester, UK: John Wiley & Sons, Ltd. https://doi.org/10.1002/9781119504641.ch10.\n\n\nElhorst, J. Paul. 2012. “Dynamic Spatial Panels: Models, Methods, and Inferences.” Journal of Geographical Systems 14 (1): 5–28. https://doi.org/10.1007/s10109-011-0158-4.\n\n\n———. 2014. Spatial Econometrics: From Cross-Sectional Data to Spatial Panels. SpringerBriefs in Regional Science. Berlin and Heidelberg: Springer. https://doi.org/10.1007/978-3-642-40340-8.\n\n\nLee, Lung-fei, and Jihai Yu. 2010. “Estimation of Spatial Autoregressive Panel Data Models with Fixed Effects.” Journal of Econometrics 154 (2): 165–85. https://doi.org/10.1016/j.jeconom.2009.08.001.\n\n\nLeSage, James P. 2014. “Spatial Econometric Panel Data Model Specification: A Bayesian Approach.” Spatial Statistics 9 (August): 122–45. https://doi.org/10.1016/j.spasta.2014.02.002.\n\n\nMillo, Giovanni, and Gianfranco Piras. 2012. “Splm: Spatial Panel Data Models in R.” Journal of Statistical Software 47 (1). https://doi.org/10.18637/jss.v047.i01.\n\n\nRüttenauer, Tobias, and Henning Best. 2021. “Environmental Inequality and Residential Sorting in Germany: A Spatial Time-Series Analysis of the Demographic Consequences of Industrial Sites.” Demography 58 (6): 2243–63. https://doi.org/10.1215/00703370-9563077."
  },
  {
    "objectID": "10_spatiotemporal.html#example-industrial-facilities-and-municipal-income",
    "href": "10_spatiotemporal.html#example-industrial-facilities-and-municipal-income",
    "title": "10  Spatio-temporal models",
    "section": "10.5 Example: Industrial facilities and municipal income",
    "text": "10.5 Example: Industrial facilities and municipal income\nRüttenauer and Best (2021): Environmental Inequality and Residential Sorting in Germany: A Spatial Time-Series Analysis of the Demographic Consequences of Industrial Sites. Demography, 58(6), 2243–2263. https://doi.org/10.1215/00703370-9563077\n\n\n\nSpatial distribution of industrial facilities and income tax revenue per municipality for 2015.\n\n\n\n\n\n\n\n\n\nCroissant, Yves, and Givanni Millo, eds. 2018. “Spatial Panels.” In Panel Data Econometrics with R, 245–84. Chichester, UK: John Wiley & Sons, Ltd. https://doi.org/10.1002/9781119504641.ch10.\n\n\nElhorst, J. Paul. 2012. “Dynamic Spatial Panels: Models, Methods, and Inferences.” Journal of Geographical Systems 14 (1): 5–28. https://doi.org/10.1007/s10109-011-0158-4.\n\n\n———. 2014. Spatial Econometrics: From Cross-Sectional Data to Spatial Panels. SpringerBriefs in Regional Science. Berlin and Heidelberg: Springer. https://doi.org/10.1007/978-3-642-40340-8.\n\n\nFingleton, Bernard, Daniel Olner, and Gwilym Pryce. 2020. “Estimating the Local Employment Impacts of Immigration: A Dynamic Spatial Panel Model.” Urban Studies 57 (13): 2646–62. https://doi.org/10.1177/0042098019887916.\n\n\nLee, Lung-fei, and Jihai Yu. 2010. “Estimation of Spatial Autoregressive Panel Data Models with Fixed Effects.” Journal of Econometrics 154 (2): 165–85. https://doi.org/10.1016/j.jeconom.2009.08.001.\n\n\nLeSage, James P. 2014. “Spatial Econometric Panel Data Model Specification: A Bayesian Approach.” Spatial Statistics 9 (August): 122–45. https://doi.org/10.1016/j.spasta.2014.02.002.\n\n\nMillo, Giovanni, and Gianfranco Piras. 2012. “Splm: Spatial Panel Data Models in R.” Journal of Statistical Software 47 (1). https://doi.org/10.18637/jss.v047.i01.\n\n\nRüttenauer, Tobias, and Henning Best. 2021. “Environmental Inequality and Residential Sorting in Germany: A Spatial Time-Series Analysis of the Demographic Consequences of Industrial Sites.” Demography 58 (6): 2243–63. https://doi.org/10.1215/00703370-9563077."
  },
  {
    "objectID": "05_regression-theory.html#examples",
    "href": "05_regression-theory.html#examples",
    "title": "5  Spatial Regression Models",
    "section": "5.5 Examples",
    "text": "5.5 Examples\n__(Boillat.2022__?)\nThe paper investigates the effects of protected areas and various land tenure regimes on deforestation and possible spillover effects in Bolivia, a global tropical deforestation hotspot.\n\nProtected areas – which in Bolivia are all based on co-management schemes - also protect forests in adjacent areas, showing an indirect protective spillover effect. Indigenous lands however only have direct forest protection effects.\n__(Fischer.2009__?)\nThe focus of this paper is on the role of human capital in explaining labor productivity variation among 198 European regions within a regression framework.\n\nA ceteris paribus increase in the level of human capital is found to have a significant and positive direct impact. But this positive direct impact is offset by a significant and negative indirect (spillover) impact leading to a total impact that is not significantly different from zero.\nThe intuition here arises from the notion that it is relative regional advantages in human capital that matter most for labor productivity, so changing human capital across all regions should have little or no total impact on (average) labor productivity levels.\n__(Ruttenauer.2018a__?)\nThis study investigates the presence of environmental inequality in Germany - the connection between the presence of foreign-minority population and objectively measured industrial pollution.\n\nResults reveal that the share of minorities within a census cell indeed positively correlates with the exposure to industrial pollution. Furthermore, spatial spillover effects are highly relevant: the characteristics of the neighbouring spatial units matter in predicting the amount of pollution. Especially within urban areas, clusters of high minority neighbourhoods are affected by high levels of environmental pollution.\n\n\n\n\n\n\nBetz, Timm, Scott J. Cook, and Florian M. Hollenbach. 2020. “Spatial Interdependence and Instrumental Variable Models.” Political Science Research and Methods 8 (4): 646–61. https://doi.org/10.1017/psrm.2018.61.\n\n\nCook, Scott J., Jude C. Hays, and Robert J. Franzese. 2020. “Model Specification and Spatial Interdependence.” In The Sage Handbook of Research Methods in Political Science and International Relations, edited by Luigi Curini and Robert Franzese, 1st ed, 730–47. Thousand Oaks: SAGE Inc.\n\n\nFranzese, Robert J., and Jude C. Hays. 2007. “Spatial Econometric Models of Cross-Sectional Interdependence in Political Science Panel and Time-Series-Cross-Section Data.” Political Analysis 15 (2): 140–64. https://doi.org/10.1093/pan/mpm005.\n\n\nGibbons, Steve, and Henry G. Overman. 2012. “Mostly Pointless Spatial Econometrics?” Journal of Regional Science 52 (2): 172–91. https://doi.org/10.1111/j.1467-9787.2012.00760.x.\n\n\nHalleck Vega, Solmaria, and J. Paul Elhorst. 2015. “The SLX Model.” Journal of Regional Science 55 (3): 339–63. https://doi.org/10.1111/jors.12188.\n\n\nKelejian, Harry H., and Gianfranco Piras. 2017. Spatial Econometrics. Elsevier. https://doi.org/10.1016/C2016-0-04332-2.\n\n\nLeSage, James P. 2014. “What Regional Scientists Need to Know about Spatial Econometrics.” The Review of Regional Studies 44 (1): 13–32. https://doi.org/https://dx.doi.org/10.2139/ssrn.2420725.\n\n\nLeSage, James P., and R. Kelley Pace. 2009. Introduction to Spatial Econometrics. Statistics, Textbooks and Monographs. Boca Raton: CRC Press.\n\n\nPace, R. Kelley, and James P. LeSage. 2010. “Omitted Variable Biases of OLS and Spatial Lag Models.” In Progress in Spatial Analysis, edited by Antonio Páez, Julie Gallo, Ron N. Buliung, and Sandy Dall’erba, 17–28. Berlin and Heidelberg: Springer.\n\n\nRüttenauer, Tobias. 2022. “Spatial Regression Models: A Systematic Comparison of Different Model Specifications Using Monte Carlo Experiments.” Sociological Methods & Research 51 (2): 728–59. https://doi.org/10.1177/0049124119882467.\n\n\nWimpy, Cameron, Guy D. Whitten, and Laron K. Williams. 2021. “X Marks the Spot: Unlocking the Treasure of Spatial-X Models.” The Journal of Politics 83 (2): 722–39. https://doi.org/10.1086/710089.\n\n\nWooldridge, Jeffrey M. 2010. Econometric Analysis of Cross Section and Panel Data. Cambridge, Mass.: MIT Press."
  },
  {
    "objectID": "10_spatiotemporal.html#example",
    "href": "10_spatiotemporal.html#example",
    "title": "10  Spatio-temporal models",
    "section": "11.3 Example:",
    "text": "11.3 Example:\nFingleton, Olner, and Pryce (2020): Estimating the local employment impacts of immigration: A dynamic spatial panel model. Urban Studies, 57(13), 2646–2662. https://doi.org/10.1177/0042098019887916\nThis paper highlights a number of important gaps in the UK evidence base on the employment impacts of immigration, namely: (1) the lack of research on the local impacts of immigration – existing studies only estimate the impact for the country as a whole; (2) the absence of long-term estimates – research has focused on relatively short time spans – there are no estimates of the impact over several decades, for example; (3) the tendency to ignore spatial dependence of employment which can bias the results and distort inference – there are no robust spatial econometric estimates we are aware of.\nWe illustrate our approach with an application to London and find that no migrant group has a statistically significant long-term negative effect on employment. EU migrants, however, are found to have a significant positive impact, which may have important implications for the Brexit debate. Our approach opens up a new avenue of inquiry into subnational variations in the impacts of immigration on employment.\n\n\n\nImpacts on employment, Fingleton, Olner, and Pryce (2020)"
  },
  {
    "objectID": "10_spatiotemporal.html#example-local-employment-impacts-of-immigration",
    "href": "10_spatiotemporal.html#example-local-employment-impacts-of-immigration",
    "title": "10  Spatio-temporal models",
    "section": "10.3 Example: Local employment impacts of immigration",
    "text": "10.3 Example: Local employment impacts of immigration\nFingleton, Olner, and Pryce (2020): Estimating the local employment impacts of immigration: A dynamic spatial panel model. Urban Studies, 57(13), 2646–2662. https://doi.org/10.1177/0042098019887916\nThis paper highlights a number of important gaps in the UK evidence base on the employment impacts of immigration, namely: (1) the lack of research on the local impacts of immigration – existing studies only estimate the impact for the country as a whole; (2) the absence of long-term estimates – research has focused on relatively short time spans – there are no estimates of the impact over several decades, for example; (3) the tendency to ignore spatial dependence of employment which can bias the results and distort inference – there are no robust spatial econometric estimates we are aware of.\nWe illustrate our approach with an application to London and find that no migrant group has a statistically significant long-term negative effect on employment. EU migrants, however, are found to have a significant positive impact, which may have important implications for the Brexit debate. Our approach opens up a new avenue of inquiry into subnational variations in the impacts of immigration on employment.\n\n\n\nImpacts on employment, Fingleton, Olner, and Pryce (2020)"
  },
  {
    "objectID": "07_impacts.html#examples",
    "href": "07_impacts.html#examples",
    "title": "\n7  Spatial Impacts\n",
    "section": "\n7.4 Examples",
    "text": "7.4 Examples\nBoillat, Ceddia, and Bottazzi (2022)\nThe paper investigates the effects of protected areas and various land tenure regimes on deforestation and possible spillover effects in Bolivia, a global tropical deforestation hotspot.\n\nProtected areas – which in Bolivia are all based on co-management schemes - also protect forests in adjacent areas, showing an indirect protective spillover effect. Indigenous lands however only have direct forest protection effects.\nFischer et al. (2009)\nThe focus of this paper is on the role of human capital in explaining labor productivity variation among 198 European regions within a regression framework.\n\nA ceteris paribus increase in the level of human capital is found to have a significant and positive direct impact. But this positive direct impact is offset by a significant and negative indirect (spillover) impact leading to a total impact that is not significantly different from zero.\nThe intuition here arises from the notion that it is relative regional advantages in human capital that matter most for labor productivity, so changing human capital across all regions should have little or no total impact on (average) labor productivity levels.\nRüttenauer (2018)\nThis study investigates the presence of environmental inequality in Germany - the connection between the presence of foreign-minority population and objectively measured industrial pollution.\n\nResults reveal that the share of minorities within a census cell indeed positively correlates with the exposure to industrial pollution. Furthermore, spatial spillover effects are highly relevant: the characteristics of the neighbouring spatial units matter in predicting the amount of pollution. Especially within urban areas, clusters of high minority neighbourhoods are affected by high levels of environmental pollution.\n\n\n\n\n\n\nAnselin, Luc. 2003. “Spatial Externalities, Spatial Multipliers, and Spatial Econometrics.” International Regional Science Review 26 (2): 153–66. https://doi.org/10.1177/0160017602250972.\n\n\nBoillat, Sébastien, M. Graziano Ceddia, and Patrick Bottazzi. 2022. “The Role of Protected Areas and Land Tenure Regimes on Forest Loss in Bolivia: Accounting for Spatial Spillovers.” Global Environmental Change 76 (September): 102571. https://doi.org/10.1016/j.gloenvcha.2022.102571.\n\n\nFischer, Manfred M., Monika Bartkowska, Aleksandra Riedl, Sascha Sardadvar, and Andrea Kunnert. 2009. “The Impact of Human Capital on Regional Labor Productivity in Europe.” Letters in Spatial and Resource Sciences 2 (2-3): 97–108. https://doi.org/10.1007/s12076-009-0027-7.\n\n\nHalleck Vega, Solmaria, and J. Paul Elhorst. 2015. “The SLX Model.” Journal of Regional Science 55 (3): 339–63. https://doi.org/10.1111/jors.12188.\n\n\nLeSage, James P. 2014. “What Regional Scientists Need to Know about Spatial Econometrics.” The Review of Regional Studies 44 (1): 13–32. https://doi.org/https://dx.doi.org/10.2139/ssrn.2420725.\n\n\nLeSage, James P., and R. Kelley Pace. 2009. Introduction to Spatial Econometrics. Statistics, Textbooks and Monographs. Boca Raton: CRC Press.\n\n\nRüttenauer, Tobias. 2018. “Neighbours Matter: A Nation-wide Small-area Assessment of Environmental Inequality in Germany.” Social Science Research 70: 198–211. https://doi.org/10.1016/j.ssresearch.2017.11.009."
  },
  {
    "objectID": "10_spatiotemporal.html#estimation-in-r",
    "href": "10_spatiotemporal.html#estimation-in-r",
    "title": "10  Spatio-temporal models",
    "section": "10.4 Estimation in R",
    "text": "10.4 Estimation in R\nTo estimate spatial panel models in R, we can use the splm package of Millo and Piras (2012).\nWe use a standard example with longitudinal data from the plm package here.\n\ndata(Produc, package = \"plm\")\ndata(usaww)\n\nhead(Produc)\n\n    state year region     pcap     hwy   water    util       pc\n1 ALABAMA 1970      6 15032.67 7325.80 1655.68 6051.20 35793.80\n2 ALABAMA 1971      6 15501.94 7525.94 1721.02 6254.98 37299.91\n3 ALABAMA 1972      6 15972.41 7765.42 1764.75 6442.23 38670.30\n4 ALABAMA 1973      6 16406.26 7907.66 1742.41 6756.19 40084.01\n5 ALABAMA 1974      6 16762.67 8025.52 1734.85 7002.29 42057.31\n6 ALABAMA 1975      6 17316.26 8158.23 1752.27 7405.76 43971.71\n    gsp    emp unemp\n1 28418 1010.5   4.7\n2 29375 1021.9   5.2\n3 31303 1072.3   4.7\n4 33430 1135.5   3.9\n5 33749 1169.8   5.5\n6 33604 1155.4   7.7\n\nusaww[1:10, 1:10]\n\n            ALABAMA   ARIZONA ARKANSAS CALIFORNIA COLORADO\nALABAMA         0.0 0.0000000        0        0.0      0.0\nARIZONA         0.0 0.0000000        0        0.2      0.2\nARKANSAS        0.0 0.0000000        0        0.0      0.0\nCALIFORNIA      0.0 0.3333333        0        0.0      0.0\nCOLORADO        0.0 0.1428571        0        0.0      0.0\nCONNECTICUT     0.0 0.0000000        0        0.0      0.0\nDELAWARE        0.0 0.0000000        0        0.0      0.0\nFLORIDA         0.5 0.0000000        0        0.0      0.0\nGEORGIA         0.2 0.0000000        0        0.0      0.0\nIDAHO           0.0 0.0000000        0        0.0      0.0\n            CONNECTICUT DELAWARE FLORIDA GEORGIA IDAHO\nALABAMA               0        0    0.25    0.25     0\nARIZONA               0        0    0.00    0.00     0\nARKANSAS              0        0    0.00    0.00     0\nCALIFORNIA            0        0    0.00    0.00     0\nCOLORADO              0        0    0.00    0.00     0\nCONNECTICUT           0        0    0.00    0.00     0\nDELAWARE              0        0    0.00    0.00     0\nFLORIDA               0        0    0.00    0.50     0\nGEORGIA               0        0    0.20    0.00     0\nIDAHO                 0        0    0.00    0.00     0\n\n\nProduc contains data on US States Production - a panel of 48 observations from 1970 to 1986. usaww is a spatial weights matrix of the 48 continental US States based on the queen contiguity relation.\nLet start with an FE SEM model, using function spml() for maximum likelihood estimation of static spatial panel models.\n\n# Gen listw object\nusalw &lt;- mat2listw(usaww, style = \"W\")\n\n# Spec formula\nfm &lt;- log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp\n\n### Esimate FE SEM model\nsemfe.mod &lt;- spml(formula = fm, data = Produc, \n                  index = c(\"state\", \"year\"),  # ID column\n                  listw = usalw,          # listw\n                  model = \"within\",       # one of c(\"within\", \"random\", \"pooling\").\n                  effect = \"individual\",  # type of fixed effects\n                  lag = FALSE,            # spatila lg of Y\n                  spatial.error = \"b\",    # \"b\" (Baltagi), \"kkp\" (Kapoor, Kelejian and Prucha)\n                  method = \"eigen\",       # estimation method, for large data e.g. (\"spam\", \"Matrix\" or \"LU\")\n                  na.action = na.fail,    # handling of missings\n                  zero.policy = NULL)     # handling of missings\n\nsummary(semfe.mod)\n\nSpatial panel fixed effects error model\n \n\nCall:\nspml(formula = fm, data = Produc, index = c(\"state\", \"year\"), \n    listw = usalw, na.action = na.fail, model = \"within\", effect = \"individual\", \n    lag = FALSE, spatial.error = \"b\", method = \"eigen\", zero.policy = NULL)\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-0.1246945 -0.0237699 -0.0034993  0.0170886  0.1882224 \n\nSpatial error parameter:\n    Estimate Std. Error t-value  Pr(&gt;|t|)    \nrho 0.557401   0.033075  16.853 &lt; 2.2e-16 ***\n\nCoefficients:\n            Estimate Std. Error t-value Pr(&gt;|t|)    \nlog(pcap)  0.0051438  0.0250109  0.2057  0.83705    \nlog(pc)    0.2053026  0.0231427  8.8712  &lt; 2e-16 ***\nlog(emp)   0.7822540  0.0278057 28.1328  &lt; 2e-16 ***\nunemp     -0.0022317  0.0010709 -2.0839  0.03717 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA RE SAR model, by contrast, can be estimated using the following options:\n\n### Estimate an RE SAR model\nsarre.mod &lt;- spml(formula = fm, data = Produc, \n                  index = c(\"state\", \"year\"),  # ID column\n                  listw = usalw,          # listw\n                  model = \"random\",       # one of c(\"within\", \"random\", \"pooling\").\n                  effect = \"individual\",  # type of fixed effects\n                  lag = TRUE,             # spatila lg of Y\n                  spatial.error = \"none\", # \"b\" (Baltagi), \"kkp\" (Kapoor, Kelejian and Prucha)\n                  method = \"eigen\",       # estimation method, for large data e.g. (\"spam\", \"Matrix\" or \"LU\")\n                  na.action = na.fail,    # handling of missings\n                  zero.policy = NULL)     # handling of missings\n\nsummary(sarre.mod)\n\nML panel with spatial lag, random effects \n\nCall:\nspreml(formula = formula, data = data, index = index, w = listw2mat(listw), \n    w2 = listw2mat(listw2), lag = lag, errors = errors, cl = cl, \n    method = \"eigen\", zero.policy = ..2)\n\nResiduals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.38    1.57    1.70    1.70    1.80    2.13 \n\nError variance parameters:\n    Estimate Std. Error t-value Pr(&gt;|t|)  \nphi  21.3175     8.2929  2.5706  0.01015 *\n\nSpatial autoregressive coefficient:\n       Estimate Std. Error t-value  Pr(&gt;|t|)    \nlambda 0.161615   0.029042  5.5648 2.625e-08 ***\n\nCoefficients:\n               Estimate  Std. Error t-value  Pr(&gt;|t|)    \n(Intercept)  1.65814987  0.15071855 11.0016 &lt; 2.2e-16 ***\nlog(pcap)    0.01294505  0.02493997  0.5190    0.6037    \nlog(pc)      0.22555375  0.02163422 10.4258 &lt; 2.2e-16 ***\nlog(emp)     0.67081074  0.02642113 25.3892 &lt; 2.2e-16 ***\nunemp       -0.00579716  0.00089175 -6.5009 7.984e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that Millo and Piras (2012) use a different notation, namely \\(\\lambda\\) for lag dependence, and \\(\\rho\\) for error dependence….\nAgain, we have to use an additional step to get impacts for SAR-like models.\n\n# Number of years\nT &lt;- length(unique(Produc$year))\n\n# impacts\nsarre.mod.imp &lt;- impacts(sarre.mod,\n                         listw = usalw,\n                         time = T)\nsummary(sarre.mod.imp)                         \n\nImpact measures (lag, trace):\n                Direct     Indirect        Total\nlog(pcap)  0.013028574  0.002411880  0.015440454\nlog(pc)    0.227009032  0.042024438  0.269033470\nlog(emp)   0.675138835  0.124983264  0.800122098\nunemp     -0.005834562 -0.001080108 -0.006914669\n========================================================\nSimulation results ( variance matrix):\nDirect:\n\nIterations = 1:200\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 200 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n               Mean        SD  Naive SE Time-series SE\nlog(pcap)  0.014018 0.0269148 1.903e-03      1.903e-03\nlog(pc)    0.229110 0.0217812 1.540e-03      1.540e-03\nlog(emp)   0.672171 0.0279965 1.980e-03      1.980e-03\nunemp     -0.005876 0.0009191 6.499e-05      6.499e-05\n\n2. Quantiles for each variable:\n\n               2.5%       25%       50%       75%     97.5%\nlog(pcap) -0.040735 -0.002110  0.014639  0.031085  0.064264\nlog(pc)    0.189454  0.213645  0.228699  0.243676  0.274472\nlog(emp)   0.617315  0.653580  0.671957  0.692865  0.728716\nunemp     -0.007725 -0.006416 -0.005892 -0.005249 -0.004214\n\n========================================================\nIndirect:\n\nIterations = 1:200\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 200 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n               Mean        SD  Naive SE Time-series SE\nlog(pcap)  0.002625 0.0051289 3.627e-04      3.627e-04\nlog(pc)    0.043037 0.0098790 6.986e-04      6.986e-04\nlog(emp)   0.125961 0.0247641 1.751e-03      1.751e-03\nunemp     -0.001102 0.0002789 1.972e-05      2.191e-05\n\n2. Quantiles for each variable:\n\n               2.5%        25%       50%       75%      97.5%\nlog(pcap) -0.007588 -0.0003474  0.002949  0.005978  0.0120732\nlog(pc)    0.026912  0.0358375  0.042576  0.049154  0.0618856\nlog(emp)   0.078987  0.1074866  0.125531  0.142507  0.1744827\nunemp     -0.001636 -0.0012761 -0.001077 -0.000920 -0.0006084\n\n========================================================\nTotal:\n\nIterations = 1:200\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 200 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n               Mean       SD  Naive SE Time-series SE\nlog(pcap)  0.016644 0.031958 2.260e-03      2.260e-03\nlog(pc)    0.272147 0.028217 1.995e-03      1.995e-03\nlog(emp)   0.798132 0.040933 2.894e-03      2.894e-03\nunemp     -0.006978 0.001114 7.879e-05      7.879e-05\n\n2. Quantiles for each variable:\n\n              2.5%       25%       50%       75%     97.5%\nlog(pcap) -0.04982 -0.002483  0.017731  0.036710  0.077099\nlog(pc)    0.22333  0.251301  0.270158  0.291200  0.334917\nlog(emp)   0.71769  0.768460  0.796217  0.826540  0.875376\nunemp     -0.00910 -0.007736 -0.006924 -0.006279 -0.005004\n\n\nThere is an alternative by using the package SDPDmod by Rozeta Simonovska (see vignette).\n\n### FE SAR model\nsarfe.mod2 &lt;- SDPDm(formula = fm, \n                    data = Produc, \n                    W = usaww,                 \n                    index = c(\"state\",\"year\"), # ID\n                    model = \"sar\",             # on of c(\"sar\",\"sdm\"),\n                    effect = \"individual\",     # FE structure\n                    dynamic = FALSE,           # time lags of the dependet variable\n                    LYtrans = TRUE)            # Lee-Yu transformation (bias correction)\n\nsummary(sarfe.mod2)\n\nsar panel model with individual fixed effects\n\nCall:\nSDPDm(formula = fm, data = Produc, W = usaww, index = c(\"state\", \n    \"year\"), model = \"sar\", effect = \"individual\", dynamic = FALSE, \n    LYtrans = TRUE)\n\nSpatial autoregressive coefficient:\n    Estimate Std. Error t-value  Pr(&gt;|t|)    \nrho 0.278598   0.023999  11.609 &lt; 2.2e-16 ***\n\nCoefficients:\n            Estimate Std. Error t-value  Pr(&gt;|t|)    \nlog(pcap) -0.0468727  0.0262162 -1.7879   0.07379 .  \nlog(pc)    0.1859441  0.0237251  7.8374 4.598e-15 ***\nlog(emp)   0.6230539  0.0305554 20.3910 &lt; 2.2e-16 ***\nunemp     -0.0044700  0.0008917 -5.0129 5.362e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd subsequently, we can calculate the impacts of the model.\n\n# Impats\nsarfe.mod2.imp &lt;- impactsSDPDm(sarfe.mod2, \n                               NSIM = 200, # N simulations\n                               sd = 12345) # seed\n\nsummary(sarfe.mod2.imp)\n\n\nImpact estimates for spatial (static) model\n========================================================\n\nDirect:\n             Estimate  Std. Error t-value  Pr(&gt;|t|)    \nlog(pcap) -0.04549036  0.02599130 -1.7502   0.08008 .  \nlog(pc)    0.18809827  0.02383588  7.8914 2.988e-15 ***\nlog(emp)   0.63773076  0.03027063 21.0676 &lt; 2.2e-16 ***\nunemp     -0.00459914  0.00089696 -5.1275 2.936e-07 ***\n\nIndirect:\n             Estimate  Std. Error t-value  Pr(&gt;|t|)    \nlog(pcap) -0.01635880  0.00940186 -1.7400   0.08187 .  \nlog(pc)    0.06724675  0.00991019  6.7856 1.156e-11 ***\nlog(emp)   0.22820595  0.02236869 10.2020 &lt; 2.2e-16 ***\nunemp     -0.00164908  0.00037108 -4.4440 8.829e-06 ***\n\nTotal:\n            Estimate Std. Error t-value  Pr(&gt;|t|)    \nlog(pcap) -0.0618492  0.0352540 -1.7544   0.07936 .  \nlog(pc)    0.2553450  0.0313984  8.1324 4.208e-16 ***\nlog(emp)   0.8659367  0.0371925 23.2825 &lt; 2.2e-16 ***\nunemp     -0.0062482  0.0012311 -5.0751 3.873e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote: I did not manage to estimate a dynamic panel model with SDPDm."
  },
  {
    "objectID": "03_weights.html#boldsymbolmathbfw-connectivity-between-units",
    "href": "03_weights.html#boldsymbolmathbfw-connectivity-between-units",
    "title": "3  Spatial Relationships W",
    "section": "3.2 \\(\\boldsymbol{\\mathbf{W}}\\): Connectivity between units",
    "text": "3.2 \\(\\boldsymbol{\\mathbf{W}}\\): Connectivity between units\nThe connectivity between units is usually represented in a matrix \\(\\boldsymbol{\\mathbf{W}}\\). There is an ongoing debate about the importance of spatial weights for spatial econometrics and about the right way to specify weights matrices (LeSage and Pace 2014; Neumayer and Plümper 2016). The following graph shows some possible options in how to define connectivity between units.\n\n\n\nFigure: Different measures of connectivity, Source: Bivand and Rudel (2018)\n\n\nIn spatial econometrics, the spatial connectivity (as shown above) is usually represented by a spatial weights matrix \\({\\boldsymbol{\\mathbf{W}}}\\): \\[\n\\begin{equation}\n        \\boldsymbol{\\mathbf{W}} = \\begin{bmatrix}\n            w_{11} & w_{12} & \\dots & w_{1n} \\\\\n            w_{21} & w_{22} & \\dots & w_{2n} \\\\\n            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n            w_{n1} & w_{n2} & \\dots     & w_{nn}\n            \\end{bmatrix}\n        \\end{equation}\n\\] The spatial weights matrix \\(\\boldsymbol{\\mathbf{W}}\\) is an \\(N \\times N\\) dimensional matrix with elements \\(w_{ij}\\) specifying the relation or connectivity between each pair of units \\(i\\) and \\(j\\).\nNote: The diagonal elements \\(w_{i,i}= w_{1,1}, w_{2,2}, \\dots, w_{n,n}\\) of \\(\\boldsymbol{\\mathbf{W}}\\) are always zero. No unit is a neighbour of itself. This is not true for spatial multiplier matrices (as we will see later).\n\n3.2.1 Contiguity weights\nA very common type of spatial weights. Binary specification, taking the value 1 for neighbouring units (queens: sharing a common edge; rook: sharing a common border), and 0 otherwise.\nContiguity weights \\(w_{i,j}\\), where\n\\[\\begin{equation}\n  w_{i,j} =\n    \\begin{cases}\n      1 & \\text{if $i$ and $j$ neighbours}\\\\\n      0 & \\text{otherwise}\n    \\end{cases}       \n\\end{equation}\\]\nA contiguity weights matrix with three units, where unit 1 and unit 3 are neighbours, while unit 2 has no neighbours would look like this:\n\\[\n        \\begin{equation}\n        \\boldsymbol{\\mathbf{W}}  = \\begin{bmatrix}\n            0 & 0 & 1  \\\\\n            0 & 0 & 0  \\\\\n            1 & 0 & 0  \n            \\end{bmatrix}   \\nonumber\n        \\end{equation}\n\\]\n\nSparse matrices\nProblem of `island’: units without neighbours (if I calculate an average of their neigbours, would that be zero, or NA, or a mean?)\n\nLets create a contiguity weights matrix (Queens neighbours) for the London MSOAs: we create a neighbours list (nb) using poly2nb(), which is an efficient way of storing \\({\\boldsymbol{\\mathbf{W}}}\\). A snap of 1 meter accounts for potential lacks of accuracy between lines and points.\n\n# Contiguity (Queens) neighbours weights\nqueens.nb &lt;- poly2nb(msoa.spdf, \n                     queen = TRUE, # a single shared boundary point meets the contiguity condition\n                     snap = 1) # we consider points in 1m distance as 'touching'\nsummary(queens.nb)\n\nNeighbour list object:\nNumber of regions: 983 \nNumber of nonzero links: 5648 \nPercentage nonzero weights: 0.5845042 \nAverage number of links: 5.745677 \nLink number distribution:\n\n  2   3   4   5   6   7   8   9  10  11  12  13 \n  9  39 130 264 273 169  66  19   5   6   2   1 \n9 least connected regions:\n160 270 475 490 597 729 755 778 861 with 2 links\n1 most connected region:\n946 with 13 links\n\n# Lets plot that\nplot(st_geometry(msoa.spdf), border = \"grey60\")\nplot(queens.nb, st_centroid(st_geometry(msoa.spdf)), \n     add = TRUE, pch = 19, cex = 0.6)\n\n\n\n# We can also transform this into a matrix W\nW &lt;- nb2mat(queens.nb, style = \"B\")\nprint(W[1:10, 1:10])\n\n   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n1     0    0    0    0    0    0    0    0    0     0\n2     0    0    1    0    0    0    0    0    0     0\n3     0    1    0    0    1    0    0    0    0     0\n4     0    0    0    0    0    1    0    0    0     1\n5     0    0    1    0    0    1    1    0    0     0\n6     0    0    0    1    1    0    1    0    1     1\n7     0    0    0    0    1    1    0    1    1     0\n8     0    0    0    0    0    0    1    0    0     0\n9     0    0    0    0    0    1    1    0    0     1\n10    0    0    0    1    0    1    0    0    1     0\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAmong those first 10 units that you see above, which are the neighbours of unit number 6?\nWhy is the diagonal of this matrix all zero?\n\n\nOverall, the matrix W has dimensions \\(N \\times N\\), a row and a column for each observation. The value in a cell shows how units \\(i\\) (row number) and \\(j\\) (column number) are related to each other.\n\ndim(W)\n\n[1] 983 983\n\n\nThe row and column sums indicate the number of neighbours of each observation.\n\nrowSums(W)[1:10]\n\n 1  2  3  4  5  6  7  8  9 10 \n11  6  7  5  5  6  6  6  6  5 \n\ncolSums(W)[1:10]\n\n [1] 11  6  7  5  5  6  6  6  6  5\n\n\nAdjacency or graph-based neighbour’s weights matrices are usually symmetric. If unit 1 is a neighbour of unit 55, then unit 55 is also a neighbour of unit 1.\n\n\n\n\n\n\nHigher Order Neighbours\n\n\n\nYour neighbours have neighbours too, and they are called higher (second) order neighbours. The neighbours of your neighbour’s neighbours are third order neighbours.\nYou can use nblag() to calculate higher order neighbour relations.\n\n\n\n\n3.2.2 Distance based weights\nAnother common type uses the distance \\(d_{ij}\\) between each unit \\(i\\) and \\(j\\).\n\nInverse distance weights \\(w_{i,j} = \\frac{1}{d_{ij}^\\alpha}\\), where \\(\\alpha\\) define the strength of the spatial decay.\n\n\\[\n        \\begin{equation}\n        \\boldsymbol{\\mathbf{W}} = \\begin{bmatrix}\n            0 & \\frac{1}{d_{ij}^\\alpha} & \\frac{1}{d_{ij}^\\alpha}  \\\\\n            \\frac{1}{d_{ij}^\\alpha} & 0 & \\frac{1}{d_{ij}^\\alpha}  \\\\\n            \\frac{1}{d_{ij}^\\alpha} & \\frac{1}{d_{ij}^\\alpha} & 0  \n            \\end{bmatrix}   \\nonumber\n        \\end{equation}\n\\]\n\nDense matrices\nSpecifying thresholds may be useful (to get rid of very small non-zero weights)\n\nFor now, we will just specify a neighbours list with a distance threshold of 3km using dnearneigh(). An alternative would be k nearest neighbours using knearneigh(). We will do the inverse weighting later.\n\n# Crease centroids\ncoords &lt;- st_geometry(st_centroid(msoa.spdf))\n\nWarning: st_centroid assumes attributes are constant over\ngeometries\n\n# Neighbours within 3km distance\ndist_3.nb &lt;- dnearneigh(coords, d1 = 0, d2 = 3000)\nsummary(dist_3.nb)\n\nNeighbour list object:\nNumber of regions: 983 \nNumber of nonzero links: 22086 \nPercentage nonzero weights: 2.285652 \nAverage number of links: 22.46796 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 \n 4  3  7 13 11 14 14 17 26 22 26 30 33 34 46 34 59 43 38 30 25 19 \n23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 \n22 15 21 14 23 17 17 23 28 19 26 24 29 24 27 25 22 18  8 10 12  5 \n45 46 47 \n 3  2  1 \n4 least connected regions:\n158 160 463 959 with 1 link\n1 most connected region:\n545 with 47 links\n\n# Lets plot that\nplot(st_geometry(msoa.spdf), border = \"grey60\")\nplot(dist_3.nb, coords, \n     add = TRUE, pch = 19, cex = 0.6)\n\n\n\n\nAnd you can see that the matrix is not so sparse anymore:\n\nW2 &lt;- nb2mat(dist_3.nb, style = \"B\")\nW2[1:10, 1:10]\n\n   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n1     0    0    0    0    0    0    0    0    0     0\n2     0    0    1    0    1    0    0    0    0     0\n3     0    1    0    0    1    1    1    0    0     0\n4     0    0    0    0    1    1    1    0    1     1\n5     0    1    1    1    0    1    1    1    1     1\n6     0    0    1    1    1    0    1    1    1     1\n7     0    0    1    1    1    1    0    1    1     1\n8     0    0    0    0    1    1    1    0    1     0\n9     0    0    0    1    1    1    1    1    0     1\n10    0    0    0    1    1    1    1    0    1     0"
  },
  {
    "objectID": "03_weights.html#islands-missings",
    "href": "03_weights.html#islands-missings",
    "title": "3  Spatial Relationships W",
    "section": "3.4 Islands / missings",
    "text": "3.4 Islands / missings\nIn practice, we often have a problem with islands. If we use contiguity based or distance based neighbour definitions, some units may end up with empty neighbours sets: they just do not touch any other unit and do not have a neighbour within a specific distance. This however creates a problem: what is the value in the neighbouring units?\nThe zero.policy option in spdep allows to proceed with empty neighbours sets. However, many further functions may run into problems and return errors. It often makes sense to either drop islands, to choose weights which always have neighbours (e.g. k nearest), or impute empty neighbours sets by using the nearest neighbours.\n\n\n\n\n\n\nBivand, Roger S., and Colin Rudel. 2018. “Rgeos: Interface to Geometry Engine - Open Source (’GEOS’).”\n\n\nLeSage, James P., and R. Kelley Pace. 2014. “The Biggest Myth in Spatial Econometrics.” Econometrics 2 (4): 217–49. https://doi.org/10.3390/econometrics2040217.\n\n\nNeumayer, Eric, and Thomas Plümper. 2016. “W.” Political Science Research and Methods 4 (01): 175–93. https://doi.org/10.1017/psrm.2014.40.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With Applications in R. First. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nTobler, Waldo R. 1970. “A Computer Movie Simulating Urban Growth in the Detroit Region.” Economic Geography 46: 234–40. https://doi.org/10.2307/143141."
  },
  {
    "objectID": "04_dependence.html#example",
    "href": "04_dependence.html#example",
    "title": "4  Detecting Spatial Dependence",
    "section": "4.4 Example",
    "text": "4.4 Example\n\nTate.2021\nThis study explores the geography of flood exposure and social vulnerability in the conterminous United States based on spatial analysis of fluvial and pluvial flood extent, land cover, and social vulnerability.\nMobile homes and racial minorities are most overrepresented in hotspots compared to elsewhere. The results identify priority locations where interventions can mitigate both physical and social aspects of flood vulnerability."
  },
  {
    "objectID": "12_exercise2.html#diffusion-of-political-regimes",
    "href": "12_exercise2.html#diffusion-of-political-regimes",
    "title": "12  Exercise II",
    "section": "12.1 Diffusion of political regimes",
    "text": "12.1 Diffusion of political regimes\nSee for instance Gleditsch and Ward (2006) for an example for the diffusion of democratization.\n\n\n\n\n\n\nGleditsch, Kristian Skrede, and Michael D. Ward. 2006. “Diffusion and the International Context of Democratization.” International Organization 60 (4): 911–33."
  },
  {
    "objectID": "12_exercise2.html#inkar-data",
    "href": "12_exercise2.html#inkar-data",
    "title": "12  Exercise II",
    "section": "12.1 Inkar data",
    "text": "12.1 Inkar data\nBelow, we read and transform some characteristics of the INKAR data on German counties.\n\ndi &lt;- c(\"_data/\")\n\n# Define the downloaded filed\nj &lt;- c(\"inkar.csv\")\nc &lt;- 1\n\nfor(k in j){\n  header &lt;- as.vector(t(read.table(paste0(di, k), nrows = 1, sep = \";\")[1,]))\n  # Clean header\n  header &lt;- stringi::stri_replace_all_fixed(\n    header, \n    c(\"ä\", \"ö\", \"ü\", \"Ä\", \"Ö\", \"Ü\"), \n    c(\"ae\", \"oe\", \"ue\", \"Ae\", \"Oe\", \"Ue\"), \n    vectorize_all = FALSE\n  )\n  header &lt;- gsub(\" \", \"\", header)\n  header &lt;- gsub(\"\\\\.\", \"\", header)\n  header &lt;- iconv(header, \"latin1\", \"ASCII\", sub = \"\")\n  \n  # Combine with second row header (year)\n  header2 &lt;- as.vector(t(read.table(paste0(di, k), skip = 1, nrows = 1, sep = \";\")[1,]))\n  header3 &lt;- paste(header, header2, sep = \"_\")\n  header3 &lt;- gsub(\"_NA\", \"\", header3)\n  \n  nc &lt;- length(header3)\n  # Input and rename data\n  data &lt;- read.csv(paste0(di, k), skip = 2, header = FALSE, sep = \";\", \n                   quote = \"\\\"\", dec = \",\", stringsAsFactors = F,\n                   colClasses = \"character\")\n  names(data) &lt;- header3\n  data1 &lt;- data\n  \n  # Correct character vars (containing thousands separator)\n  vars &lt;- which(sapply(data1, is.character))\n  vars &lt;- vars[-which(vars %in% c(1:3))]\n  for(l in vars){\n    data1[,l] &lt;- gsub(\"\\\\.\", \"\", data1[,l])\n    data1[,l] &lt;- gsub(\"\\\\,\", \".\", data1[,l])\n    data1[,l] &lt;- as.numeric(data1[,l])\n  }\n  \n  \n  # #Save\n  # l &lt;- paste(\"bearb\", k, sep = \"_\")\n  # write.table(data1, file = l, row.names = FALSE, sep = \";\", dec = \".\", na = \".\")\n  \n  # #Reshape\n  # helpvar1 &lt;- unique(header[4:length(header)])\n  # helpvar2 &lt;-  sort(unique(header2[!is.na(header2)]))\n  # n_vars &lt;- length(helpvar1)\n  # n_times &lt;- length(helpvar2)\n  # helpvar1 &lt;- sort(rep(helpvar1, times = n_times))\n  # helpvar2 &lt;- rep(helpvar2, times = n_vars)\n  # helpvar3 &lt;- paste(helpvar1, helpvar2, sep = \"_\")\n  # count &lt;- ncol(data1)+1\n  # for(v in helpvar3) {\n  #   if(v %in% names(data1)) {}\n  #   else{\n  #     data1[,count] &lt;- NA\n  #     colnames(data1)[count] &lt;- v\n  #     count &lt;- count+1\n  #   }\n  # }\n  # data1 &lt;- data1[c(colnames(data1)[1:3], sort(helpvar3))]\n  # \n  # data1 &lt;- reshape(data1, direction = \"long\", varying = 4:ncol(data1), \n  #                  sep = \"_\")\n  data.long &lt;- tidyr::pivot_longer(data1, \n                                  cols = 4:ncol(data1),\n                                  names_to = c(\".value\", \"year\"),\n                                  names_pattern = \"(.*)_(.*)\")\n  \n  \n  colnames(data.long) &lt;- substr(colnames(data.long), 1, 30)\n  \n  if(c == 1){\n    inkar.df &lt;- data.long\n  }else{\n    inkar.df &lt;- merge(inkar.df, data.long, all.x = TRUE, all.y = TRUE)\n  }\n  \n  c &lt;- c+1\n  \n}\n\n\n\ninkar.df$year &lt;- as.numeric(inkar.df$year)\n\nnames(inkar.df)[which(names(inkar.df) == \"Pkw-Dichte\")] &lt;- \"pkw_dichte\"\n\nsave(inkar.df, file = \"_data/inkar.Rdata\")\n\nVariables are\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\n“Kennziffer”\nID\n\n\n“Raumeinheit”\nName\n\n\n“Aggregat”\nLevel\n\n\n“year”\nYear\n\n\n“Bruttoinlandsproduktin1000Euro”\nGross Domestic Product in 1000 euros\n\n\n“HaushaltemitKindern”\nHouseholds with children\n\n\n“Lebenserwartung”\nLife expectancy\n\n\n“KommunaleSchulden”\nMunicipal debts\n\n\n“Pkw-Dichte”\nCar density\n\n\n“Straenverkehrsunfaelle”\nRoad traffic accidents\n\n\n“Einpendler”\nIn-commuters\n\n\n“Auspendler”\nOut-commuters\n\n\n“Beschaeftigtenquote”\nEmployment rate\n\n\n“Bevoelkerunggesamt”\nPopulation\n\n\n“Krankenhausbetten”\nHospital beds per 1,000 inhabitants\n\n\n“Gesamtwanderungssaldo”\nNet migration balance"
  },
  {
    "objectID": "12_exercise2.html#county-shapes",
    "href": "12_exercise2.html#county-shapes",
    "title": "12  Exercise II",
    "section": "12.2 County shapes",
    "text": "12.2 County shapes\n\nkreise.spdf &lt;- st_read(dsn = \"_data/Kreisgrenzen_2020_mit_Einwohnerzahl\",\n                       layer = \"KRS_ew_20\")\n\nReading layer `KRS_ew_20' from data source \n  `C:\\work\\Lehre\\Geodata_Spatial_Regression\\_data\\Kreisgrenzen_2020_mit_Einwohnerzahl' \n  using driver `ESRI Shapefile'\nSimple feature collection with 401 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 5.86625 ymin: 47.27012 xmax: 15.04182 ymax: 55.05878\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "12_exercise2.html#exercises",
    "href": "12_exercise2.html#exercises",
    "title": "12  Exercise II",
    "section": "12.3 Exercises",
    "text": "12.3 Exercises\n\n12.3.1 Please map the life expectancy across Germany\n\n\n12.3.2 Test the effect of regional characteristics on life expectancy\n\nChose some variables that could predict life expectancy\n\nMerge data (as with conventional data)\n\n# Merge\ninkar_2015.spdf &lt;- merge(kreise.spdf, inkar.df[inkar.df$year == 2015, ], \n                         by.x = \"ags\", by.y = \"Kennziffer\")\n\nPlot it\n\ncols &lt;- viridis(n = 100, direction = -1, option = \"G\")\n\nmp1 &lt;-  tm_shape(inkar_2015.spdf) + \n  tm_fill(col = \"Lebenserwartung\", \n          style = \"cont\", # algorithm to def cut points\n          palette = cols, # colours\n          stretch.palette = TRUE,\n          title = \"in years\"\n          ) +\n  tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"Life expectancy\", \n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8)\n\nmp1\n\n\n\n\n\nChose between:\n\nGenerate impacts\n\n# nb &lt;- poly2nb(kreise.spdf, row.names = \"ags\", queen = TRUE)\nknn &lt;- knearneigh(st_centroid(kreise.spdf), k = 10)\n\nWarning: st_centroid assumes attributes are constant over\ngeometries\n\nnb &lt;- knn2nb(knn, row.names = kreise.spdf$ags)\nlistw &lt;- nb2listw(nb, style = \"W\")\n\n\nEstimate a cross-sectional spatial model for a selected year\n\nUse a spatial Durbin Error model\n\n# Spec formula\nfm &lt;- Lebenserwartung ~   Beschaeftigtenquote + Krankenhausbetten + pkw_dichte + HaushaltemitKindern + Straenverkehrsunfaelle\n\n# Estimate error model with Durbin = TRUE \nmod_1.durb &lt;- errorsarlm(fm,  \n                      data = inkar_2015.spdf, \n                      listw = listw,\n                      Durbin = TRUE)\n\nsummary(mod_1.durb)\n\n\nCall:\nerrorsarlm(formula = fm, data = inkar_2015.spdf, listw = listw, \n    Durbin = TRUE)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-2.3061430 -0.3833525 -0.0034956  0.4119499  1.8500085 \n\nType: error \nCoefficients: (asymptotic standard errors) \n                              Estimate  Std. Error z value\n(Intercept)                 7.5913e+01  2.3609e+00 32.1539\nBeschaeftigtenquote        -3.3911e-03  1.2187e-02 -0.2782\nKrankenhausbetten          -6.9723e-03  1.2376e-02 -0.5634\npkw_dichte                  6.6503e-05  6.7611e-04  0.0984\nHaushaltemitKindern         5.8928e-02  1.2635e-02  4.6639\nStraenverkehrsunfaelle     -2.7490e-04  4.5970e-04 -0.5980\nlag.Beschaeftigtenquote     7.3011e-02  3.4370e-02  2.1243\nlag.Krankenhausbetten      -9.1317e-02  5.5370e-02 -1.6492\nlag.pkw_dichte             -7.5386e-03  2.5851e-03 -2.9162\nlag.HaushaltemitKindern     7.1558e-02  3.6657e-02  1.9521\nlag.Straenverkehrsunfaelle  3.9637e-03  1.5674e-03  2.5288\n                            Pr(&gt;|z|)\n(Intercept)                &lt; 2.2e-16\nBeschaeftigtenquote         0.780824\nKrankenhausbetten           0.573192\npkw_dichte                  0.921647\nHaushaltemitKindern        3.103e-06\nStraenverkehrsunfaelle      0.549846\nlag.Beschaeftigtenquote     0.033644\nlag.Krankenhausbetten       0.099104\nlag.pkw_dichte              0.003543\nlag.HaushaltemitKindern     0.050928\nlag.Straenverkehrsunfaelle  0.011445\n\nLambda: 0.74639, LR test value: 93.205, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.049366\n    z-value: 15.12, p-value: &lt; 2.22e-16\nWald statistic: 228.6, p-value: &lt; 2.22e-16\n\nLog likelihood: -387.8592 for error model\nML residual variance (sigma squared): 0.3768, (sigma: 0.61384)\nNumber of observations: 401 \nNumber of parameters estimated: 13 \nAIC: NA (not available for weighted model), (AIC for lm: 892.92)\n\n# Calculate impacts (which is unnecessary in this case)\nmod_1.durb.imp &lt;- impacts(mod_1.durb, listw = listw, R = 300)\nsummary(mod_1.durb.imp, zstats = TRUE, short = TRUE)\n\nImpact measures (SDEM, glht, n):\n                              Direct     Indirect        Total\nBeschaeftigtenquote    -0.0033910753  0.073011426  0.069620351\nKrankenhausbetten      -0.0069723369 -0.091317318 -0.098289655\npkw_dichte              0.0000665025 -0.007538584 -0.007472081\nHaushaltemitKindern     0.0589281106  0.071558303  0.130486413\nStraenverkehrsunfaelle -0.0002748972  0.003963678  0.003688781\n========================================================\nStandard errors:\n                             Direct    Indirect       Total\nBeschaeftigtenquote    0.0121873300 0.034369504 0.034414827\nKrankenhausbetten      0.0123764058 0.055370186 0.062065936\npkw_dichte             0.0006761139 0.002585061 0.002876977\nHaushaltemitKindern    0.0126350740 0.036657394 0.038305933\nStraenverkehrsunfaelle 0.0004597008 0.001567400 0.001699889\n========================================================\nZ-values:\n                            Direct  Indirect     Total\nBeschaeftigtenquote    -0.27824596  2.124308  2.022975\nKrankenhausbetten      -0.56335716 -1.649215 -1.583633\npkw_dichte              0.09835991 -2.916211 -2.597198\nHaushaltemitKindern     4.66385163  1.952084  3.406428\nStraenverkehrsunfaelle -0.59799155  2.528824  2.170013\n\np-values:\n                       Direct     Indirect  Total     \nBeschaeftigtenquote    0.78082    0.0336444 0.04307568\nKrankenhausbetten      0.57319    0.0991037 0.11327729\npkw_dichte             0.92165    0.0035431 0.00939876\nHaushaltemitKindern    3.1035e-06 0.0509283 0.00065819\nStraenverkehrsunfaelle 0.54985    0.0114446 0.03000584\n\n\n\nEstimate a spatial panel model (use the specification you prefer)\n\nI am estimating a spatial FE SAR model here:\n\n# Drop NA\ninkar_sub.df &lt;- inkar.df[which(complete.cases(inkar.df[, all.vars(fm)])),]\n\n\n### Esimate FE SEM model\nsarfe.mod &lt;- spml(formula = fm, \n                  data = inkar_sub.df, \n                  index = c(\"Kennziffer\", \"year\"),  # ID column\n                  listw = listw,          # listw\n                  model = \"within\",       # one of c(\"within\", \"random\", \"pooling\").\n                  effect = \"individual\",  # type of fixed effects\n                  lag = TRUE,             # spatila lg of Y\n                  spatial.error = \"none\", # \"b\" (Baltagi), \"kkp\" (Kapoor, Kelejian and Prucha)\n                  method = \"eigen\",       # estimation method, for large data e.g. (\"spam\", \"Matrix\" or \"LU\")\n                  na.action = na.fail,    # handling of missings\n                  zero.policy = NULL)     # handling of missings\n\nsummary(sarfe.mod)\n\nSpatial panel fixed effects lag model\n \n\nCall:\nspml(formula = fm, data = inkar_sub.df, index = c(\"Kennziffer\", \n    \"year\"), listw = listw, na.action = na.fail, model = \"within\", \n    effect = \"individual\", lag = TRUE, spatial.error = \"none\", \n    method = \"eigen\", zero.policy = NULL)\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-0.8708649 -0.1206026 -0.0033267  0.1202969  0.7825994 \n\nSpatial autoregressive coefficient:\n       Estimate Std. Error t-value  Pr(&gt;|t|)    \nlambda 0.420096   0.028664  14.656 &lt; 2.2e-16 ***\n\nCoefficients:\n                         Estimate Std. Error t-value  Pr(&gt;|t|)    \nBeschaeftigtenquote    4.3692e-02 2.9643e-03 14.7398 &lt; 2.2e-16 ***\nKrankenhausbetten      1.9071e-02 8.6677e-03  2.2002 0.0277934 *  \npkw_dichte             7.2763e-04 1.8913e-04  3.8472 0.0001195 ***\nHaushaltemitKindern    1.6637e-02 5.8347e-03  2.8514 0.0043533 ** \nStraenverkehrsunfaelle 6.8419e-06 1.0239e-04  0.0668 0.9467237    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n### Impacts\n# Number of years\nT &lt;- length(unique(inkar_sub.df$year))\n\n# impacts\nsarfe.mod.imp &lt;- impacts(sarfe.mod,\n                         listw = listw,\n                         time = T)\nsummary(sarfe.mod.imp)     \n\nImpact measures (lag, trace):\n                             Direct     Indirect        Total\nBeschaeftigtenquote    4.456706e-02 3.077723e-02 7.534429e-02\nKrankenhausbetten      1.945235e-02 1.343345e-02 3.288580e-02\npkw_dichte             7.421961e-04 5.125476e-04 1.254744e-03\nHaushaltemitKindern    1.696972e-02 1.171899e-02 2.868872e-02\nStraenverkehrsunfaelle 6.978862e-06 4.819480e-06 1.179834e-05\n========================================================\nSimulation results ( variance matrix):\nDirect:\n\nIterations = 1:200\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 200 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                            Mean        SD  Naive SE Time-series SE\nBeschaeftigtenquote    4.459e-02 2.915e-03 2.061e-04      2.061e-04\nKrankenhausbetten      1.970e-02 9.607e-03 6.793e-04      6.793e-04\npkw_dichte             7.398e-04 1.913e-04 1.353e-05      1.353e-05\nHaushaltemitKindern    1.729e-02 5.564e-03 3.934e-04      3.934e-04\nStraenverkehrsunfaelle 2.642e-06 9.953e-05 7.038e-06      5.581e-06\n\n2. Quantiles for each variable:\n\n                             2.5%        25%        50%       75%\nBeschaeftigtenquote     0.0395345  4.271e-02  4.439e-02 4.654e-02\nKrankenhausbetten       0.0023178  1.273e-02  2.052e-02 2.610e-02\npkw_dichte              0.0003934  5.904e-04  7.354e-04 8.639e-04\nHaushaltemitKindern     0.0066456  1.350e-02  1.721e-02 2.102e-02\nStraenverkehrsunfaelle -0.0001944 -6.492e-05 -1.803e-06 7.394e-05\n                           97.5%\nBeschaeftigtenquote    0.0503857\nKrankenhausbetten      0.0378315\npkw_dichte             0.0011034\nHaushaltemitKindern    0.0287736\nStraenverkehrsunfaelle 0.0001771\n\n========================================================\nIndirect:\n\nIterations = 1:200\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 200 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                            Mean        SD  Naive SE Time-series SE\nBeschaeftigtenquote    3.081e-02 0.0042273 2.989e-04      4.939e-04\nKrankenhausbetten      1.349e-02 0.0067081 4.743e-04      4.743e-04\npkw_dichte             5.132e-04 0.0001535 1.085e-05      1.085e-05\nHaushaltemitKindern    1.197e-02 0.0041969 2.968e-04      2.968e-04\nStraenverkehrsunfaelle 2.023e-06 0.0000700 4.950e-06      3.991e-06\n\n2. Quantiles for each variable:\n\n                             2.5%        25%        50%       75%\nBeschaeftigtenquote     0.0230406  2.825e-02  3.074e-02 3.311e-02\nKrankenhausbetten       0.0016335  8.619e-03  1.344e-02 1.784e-02\npkw_dichte              0.0002607  3.996e-04  4.945e-04 6.111e-04\nHaushaltemitKindern     0.0042322  9.199e-03  1.173e-02 1.463e-02\nStraenverkehrsunfaelle -0.0001400 -4.243e-05 -1.254e-06 5.035e-05\n                           97.5%\nBeschaeftigtenquote    0.0397528\nKrankenhausbetten      0.0273577\npkw_dichte             0.0008314\nHaushaltemitKindern    0.0205899\nStraenverkehrsunfaelle 0.0001292\n\n========================================================\nTotal:\n\nIterations = 1:200\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 200 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                            Mean        SD  Naive SE Time-series SE\nBeschaeftigtenquote    7.540e-02 0.0062141 4.394e-04      6.929e-04\nKrankenhausbetten      3.320e-02 0.0161873 1.145e-03      1.145e-03\npkw_dichte             1.253e-03 0.0003369 2.383e-05      2.383e-05\nHaushaltemitKindern    2.926e-02 0.0095948 6.785e-04      6.785e-04\nStraenverkehrsunfaelle 4.665e-06 0.0001692 1.196e-05      9.546e-06\n\n2. Quantiles for each variable:\n\n                             2.5%        25%        50%       75%\nBeschaeftigtenquote     0.0635934  0.0707996  7.555e-02 0.0788972\nKrankenhausbetten       0.0038687  0.0212281  3.396e-02 0.0440139\npkw_dichte              0.0006616  0.0010025  1.260e-03 0.0014714\nHaushaltemitKindern     0.0108200  0.0231673  2.858e-02 0.0353547\nStraenverkehrsunfaelle -0.0003412 -0.0001057 -3.057e-06 0.0001213\n                           97.5%\nBeschaeftigtenquote    0.0889887\nKrankenhausbetten      0.0638877\npkw_dichte             0.0018925\nHaushaltemitKindern    0.0493892\nStraenverkehrsunfaelle 0.0002914\n\n\n\nCalculate the impacts\n\n\n\n12.3.3 Esimate an FE model with SLX specification\nLoops over years to generate WX\n\n# All years where we have a balanced sample\nyears &lt;- unique(inkar.df$year[which(complete.cases(inkar.df[, all.vars(fm)]))])\n\n# All variables we want ot lag\nvars &lt;- all.vars(fm)\n\n# create listw with the correct rownames (ID = Kennziffer)\nkreise.spdf$Kennziffer &lt;- kreise.spdf$ags\nknn &lt;- knearneigh(st_centroid(kreise.spdf), k = 10)\n\nWarning: st_centroid assumes attributes are constant over\ngeometries\n\nnb &lt;- knn2nb(knn, row.names = kreise.spdf$Kennziffer)\nlistw &lt;- nb2listw(nb, style = \"W\")\n\nfor(y in years){\n  # Select singe year\n  tmp &lt;- inkar.df[inkar.df$year == y ,]\n  # Select variables and make df\n  x &lt;- st_drop_geometry(tmp[, vars])\n  # Add ID as rownames (we retreive them again later)\n  rownames(x) &lt;- tmp$Kennziffer\n  # Perform lag transformation (rownames contian ids)\n  w.tmp &lt;- create_WX(as.matrix(x),\n                    listw = listw,\n                    prefix = \"w\",\n                    zero.policy = TRUE) # NAs will get zero\n  w.tmp &lt;- as.data.frame(w.tmp)\n  \n  # add indices back\n  w.tmp$Kennziffer &lt;- row.names(w.tmp)\n  w.tmp$year &lt;- y\n  \n  if(y == years[1]){\n    w.inkar.df &lt;- w.tmp\n  }else{\n    w.inkar.df &lt;- rbind(w.inkar.df, w.tmp)\n  }\n}\n\nWarning: Setting row names on a tibble is deprecated.\n\n\nWarning: Setting row names on a tibble is deprecated.\nSetting row names on a tibble is deprecated.\nSetting row names on a tibble is deprecated.\nSetting row names on a tibble is deprecated.\nSetting row names on a tibble is deprecated.\nSetting row names on a tibble is deprecated.\nSetting row names on a tibble is deprecated.\n\nhead(w.inkar.df)\n\n      w.Lebenserwartung w.Beschaeftigtenquote w.Krankenhausbetten\n01001            80.181                52.686               5.694\n01002            80.193                53.122               6.087\n01003            80.281                55.042               6.950\n01004            80.325                53.877               5.631\n01051            80.145                53.762               5.281\n01053            80.236                56.863               6.155\n      w.pkw_dichte w.HaushaltemitKindern w.Straenverkehrsunfaelle\n01001      549.458                27.703                  512.763\n01002      542.982                27.285                  519.162\n01003      518.670                26.150                  518.684\n01004      538.232                27.124                  516.199\n01051      546.799                28.170                  494.705\n01053      523.272                26.562                  513.071\n      Kennziffer year\n01001      01001 2013\n01002      01002 2013\n01003      01003 2013\n01004      01004 2013\n01051      01051 2013\n01053      01053 2013\n\n# Merge back \n\ninkar.df &lt;- merge(inkar.df, w.inkar.df, by = c(\"Kennziffer\", \"year\"))\n\nEstimate a twoways FE panel model\n\nfm2 &lt;- Lebenserwartung ~ Beschaeftigtenquote + Krankenhausbetten + pkw_dichte + HaushaltemitKindern + Straenverkehrsunfaelle +\n  w.Beschaeftigtenquote + w.Krankenhausbetten + w.pkw_dichte + w.HaushaltemitKindern + w.Straenverkehrsunfaelle\n\nslx.fe &lt;- plm(fm2,\n              data = inkar.df,\n              effects = \"twoways\",\n              model = \"within\")\n\nsummary(slx.fe)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = fm2, data = inkar.df, model = \"within\", effects = \"twoways\")\n\nBalanced Panel: n = 401, T = 8, N = 3208\n\nResiduals:\n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-0.88591742 -0.12535845 -0.00049348  0.12743249  0.72005574 \n\nCoefficients:\n                            Estimate  Std. Error t-value  Pr(&gt;|t|)\nBeschaeftigtenquote       0.00553516  0.00753337  0.7348  0.462552\nKrankenhausbetten         0.02732408  0.00960069  2.8461  0.004459\npkw_dichte                0.00058863  0.00021846  2.6945  0.007092\nHaushaltemitKindern       0.02062436  0.00814937  2.5308  0.011435\nStraenverkehrsunfaelle    0.00031042  0.00014097  2.2020  0.027748\nw.Beschaeftigtenquote     0.06172600  0.00945493  6.5284 7.863e-11\nw.Krankenhausbetten       0.02200257  0.03261281  0.6747  0.499947\nw.pkw_dichte              0.00143784  0.00095524  1.5052  0.132382\nw.HaushaltemitKindern    -0.01053720  0.01275188 -0.8263  0.408690\nw.Straenverkehrsunfaelle -0.00091095  0.00022556 -4.0387 5.519e-05\n                            \nBeschaeftigtenquote         \nKrankenhausbetten        ** \npkw_dichte               ** \nHaushaltemitKindern      *  \nStraenverkehrsunfaelle   *  \nw.Beschaeftigtenquote    ***\nw.Krankenhausbetten         \nw.pkw_dichte                \nw.HaushaltemitKindern       \nw.Straenverkehrsunfaelle ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    250.08\nResidual Sum of Squares: 132.01\nR-Squared:      0.47212\nAdj. R-Squared: 0.39474\nF-statistic: 250.159 on 10 and 2797 DF, p-value: &lt; 2.22e-16"
  },
  {
    "objectID": "01_refresher.html#packages",
    "href": "01_refresher.html#packages",
    "title": "\n1  Refresher\n",
    "section": "\n1.1 Packages",
    "text": "1.1 Packages\nPlease make sure that you have installed the following packages:\n\npks &lt;- c(\"dplyr\",\n\"gstat\",\n\"mapview\",\n\"nngeo\",\n\"nomisr\",\n\"osmdata\",\n\"rnaturalearth\",\n\"sf\",\n\"spatialreg\",\n\"spdep\",\n\"texreg\",\n\"tidyr\",\n\"tmap\",\n\"viridisLite\")\n\nThe most important package is sf: Simple Features for R. users are strongly encouraged to install the sf binary packages from CRAN. If that does not work, please have a look at the installation instructions. It requires software packages GEOS, GDAL and PROJ."
  },
  {
    "objectID": "01_refresher.html#importing-some-real-world-data",
    "href": "01_refresher.html#importing-some-real-world-data",
    "title": "\n1  Refresher\n",
    "section": "\n1.3 Importing some real world data",
    "text": "1.3 Importing some real world data\nsf imports many of the most common spatial data files, like geojson, gpkg, or shp.\n\n1.3.1 London shapefile (polygon)\nLet’s get some administrative boundaries for London from the London Datastore. We use the sf package and its funtion st_read() to import the data.\n\n# Create subdir (all data withh be stored in \"_data\")\ndn &lt;- \"_data\"\nifelse(dir.exists(dn), \"Exists\", dir.create(dn))\n\n[1] \"Exists\"\n\n# Download zip file and unzip\ntmpf &lt;- tempfile()\nboundary.link &lt;- \"https://data.london.gov.uk/download/statistical-gis-boundary-files-london/9ba8c833-6370-4b11-abdc-314aa020d5e0/statistical-gis-boundaries-london.zip\"\ndownload.file(boundary.link, tmpf)\nunzip(zipfile = tmpf, exdir = paste0(dn))\nunlink(tmpf)\n\n# This is a shapefile\n# We only need the MSOA layer for now\nmsoa.spdf &lt;- st_read(dsn = paste0(dn, \"/statistical-gis-boundaries-london/ESRI\"),\n                     layer = \"MSOA_2011_London_gen_MHW\" # Note: no file ending\n                     )\n\nReading layer `MSOA_2011_London_gen_MHW' from data source \n  `C:\\work\\Lehre\\Geodata_Spatial_Regression_short\\_data\\statistical-gis-boundaries-london\\ESRI' \n  using driver `ESRI Shapefile'\nSimple feature collection with 983 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n\nThe object msoa.spdf is our spatial data.frame. It looks essentially like a conventional data.frame, but has some additional attributes and geo-graphical information stored with it. Most importantly, notice the column geometry, which contains a list of polygons. In most cases, we have one polygon for each line / observation.\n\nhead(msoa.spdf)\n\nSimple feature collection with 6 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180510.7 xmax: 551943.8 ymax: 191139\nProjected CRS: OSGB36 / British National Grid\n   MSOA11CD                 MSOA11NM   LAD11CD              LAD11NM\n1 E02000001       City of London 001 E09000001       City of London\n2 E02000002 Barking and Dagenham 001 E09000002 Barking and Dagenham\n3 E02000003 Barking and Dagenham 002 E09000002 Barking and Dagenham\n4 E02000004 Barking and Dagenham 003 E09000002 Barking and Dagenham\n5 E02000005 Barking and Dagenham 004 E09000002 Barking and Dagenham\n6 E02000007 Barking and Dagenham 006 E09000002 Barking and Dagenham\n    RGN11CD RGN11NM USUALRES HHOLDRES COMESTRES POPDEN HHOLDS\n1 E12000007  London     7375     7187       188   25.5   4385\n2 E12000007  London     6775     6724        51   31.3   2713\n3 E12000007  London    10045    10033        12   46.9   3834\n4 E12000007  London     6182     5937       245   24.8   2318\n5 E12000007  London     8562     8562         0   72.1   3183\n6 E12000007  London     8791     8672       119   50.6   3441\n  AVHHOLDSZ                       geometry\n1       1.6 MULTIPOLYGON (((531667.6 18...\n2       2.5 MULTIPOLYGON (((548881.6 19...\n3       2.6 MULTIPOLYGON (((549102.4 18...\n4       2.6 MULTIPOLYGON (((551550 1873...\n5       2.7 MULTIPOLYGON (((549099.6 18...\n6       2.5 MULTIPOLYGON (((549819.9 18...\n\n\nShapefiles are still among the most common formats to store and transmit spatial data, despite them being inefficient (file size and file number).\nHowever, sf reads everything spatial, such as geo.json, which usually is more efficient, but less common (but we’re getting there).\n\n# Download file\nulez.link &lt;- \"https://data.london.gov.uk/download/ultra_low_emissions_zone/936d71d8-c5fc-40ad-a392-6bec86413b48/CentralUltraLowEmissionZone.geojson\"\ndownload.file(ulez.link, paste0(dn, \"/ulez.json\"))\n\n# Read geo.json\nst_layers(paste0(dn, \"/ulez.json\"))\n\nDriver: GeoJSON \nAvailable layers:\n                   layer_name geometry_type features fields\n1 CentralUltraLowEmissionZone Multi Polygon        1      4\n                        crs_name\n1 OSGB36 / British National Grid\n\nulez.spdf &lt;- st_read(dsn = paste0(dn, \"/ulez.json\")) # here dsn is simply the file\n\nReading layer `CentralUltraLowEmissionZone' from data source \n  `C:\\work\\Lehre\\Geodata_Spatial_Regression_short\\_data\\ulez.json' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 527271.5 ymin: 178041.5 xmax: 533866.3 ymax: 183133.4\nProjected CRS: OSGB36 / British National Grid\n\nhead(ulez.spdf)\n\nSimple feature collection with 1 feature and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 527271.5 ymin: 178041.5 xmax: 533866.3 ymax: 183133.4\nProjected CRS: OSGB36 / British National Grid\n  fid OBJECTID BOUNDARY Shape_Area                       geometry\n1   1        1 CSS Area   21.37557 MULTIPOLYGON (((531562.7 18...\n\n\nAgain, this looks like a conventional data.frame but has the additional column geometry containing the coordinates of each observation. st_geometry() returns only the geographic object and st_drop_geometry() only the data.frame without the coordinates. We can plot the object using mapview().\n\nmapview(msoa.spdf[, \"POPDEN\"])\n\n\n\n\n\n\n\n1.3.2 Census API (admin units)\nNow that we have some boundaries and shapes of spatial units in London, we can start looking for different data sources to populate the geometries.\nA good source for demographic data is for instance the 2011 census. Below we use the nomis API to retrieve population data for London, See the Vignette for more information (Guest users are limited to 25,000 rows per query). Below is a wrapper to avoid some errors with sex and urban-rural cross-tabulation in some of the data.\n\n### For larger request, register and set key\n# Sys.setenv(NOMIS_API_KEY = \"XXX\")\n# nomis_api_key(check_env = TRUE)\n\nx &lt;- nomis_data_info()\n\n# Get London ids\nlondon_ids &lt;- msoa.spdf$MSOA11CD\n\n### Get key statistics ids\n# select requires tables (https://www.nomisweb.co.uk/sources/census_2011_ks)\n# Let's get KS201EW (ethnic group), KS205EW (passport held), and KS402EW (housing tenure)\n\n# Get internal ids\nstats &lt;- c(\"KS201EW\", \"KS402EW\", \"KS205EW\")\noo &lt;- which(grepl(paste(stats, collapse = \"|\"), x$name.value))\nksids &lt;- x$id[oo]\nksids # This are the internal ids\n\n[1] \"NM_608_1\" \"NM_612_1\" \"NM_619_1\"\n\n### look at meta information\nq &lt;- nomis_overview(ksids[1])\nhead(q)\n\n# A tibble: 6 × 2\n  name           value           \n  &lt;chr&gt;          &lt;list&gt;          \n1 analyses       &lt;named list [1]&gt;\n2 analysisname   &lt;chr [1]&gt;       \n3 analysisnumber &lt;int [1]&gt;       \n4 contact        &lt;named list [4]&gt;\n5 contenttypes   &lt;named list [1]&gt;\n6 coverage       &lt;chr [1]&gt;       \n\na &lt;- nomis_get_metadata(id = ksids[1], concept = \"GEOGRAPHY\", type = \"type\")\na # TYPE297 is MSOA level\n\n# A tibble: 24 × 3\n   id      label.en                                   description.en\n   &lt;chr&gt;   &lt;chr&gt;                                      &lt;chr&gt;         \n 1 TYPE265 NHS area teams                             NHS area teams\n 2 TYPE266 clinical commissioning groups              clinical comm…\n 3 TYPE267 built-up areas including subdivisions      built-up area…\n 4 TYPE269 built-up areas                             built-up areas\n 5 TYPE273 national assembly for wales electoral reg… national asse…\n 6 TYPE274 postcode areas                             postcode areas\n 7 TYPE275 postcode districts                         postcode dist…\n 8 TYPE276 postcode sectors                           postcode sect…\n 9 TYPE277 national assembly for wales constituencie… national asse…\n10 TYPE279 parishes 2011                              parishes 2011 \n# ℹ 14 more rows\n\nb &lt;- nomis_get_metadata(id = ksids[1], concept = \"MEASURES\", type = \"TYPE297\")\nb # 20100 is the measure of absolute numbers\n\n# A tibble: 2 × 3\n  id    label.en description.en\n  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;         \n1 20100 value    value         \n2 20301 percent  percent       \n\n### Query data in loop over the required statistics\nfor(i in ksids){\n\n  # Determin if data is divided by sex or urban-rural\n  nd &lt;- nomis_get_metadata(id = i)\n  if(\"RURAL_URBAN\" %in% nd$conceptref){\n    UR &lt;- TRUE\n  }else{\n    UR &lt;- FALSE\n  }\n  if(\"C_SEX\" %in% nd$conceptref){\n    SEX &lt;- TRUE\n  }else{\n    SEX &lt;- FALSE\n  }\n\n  # make data request\n  if(UR == TRUE){\n    if(SEX == TRUE){\n      tmp_en &lt;- nomis_get_data(id = i, time = \"2011\",\n                               geography = london_ids, # replace with \"TYPE297\" for all MSOAs\n                               measures = 20100, RURAL_URBAN = 0, C_SEX = 0)\n    }else{\n      tmp_en &lt;- nomis_get_data(id = i, time = \"2011\",\n                               geography = london_ids, # replace with \"TYPE297\" for all MSOAs\n                               measures = 20100, RURAL_URBAN = 0)\n    }\n  }else{\n    if(SEX == TRUE){\n      tmp_en &lt;- nomis_get_data(id = i, time = \"2011\",\n                               geography = london_ids, # replace with \"TYPE297\" for all MSOAs\n                               measures = 20100, C_SEX = 0)\n    }else{\n      tmp_en &lt;- nomis_get_data(id = i, time = \"2011\",\n                               geography = london_ids, # replace with \"TYPE297\" for all MSOAs\n                               measures = 20100)\n    }\n\n  }\n\n  # Append (in case of different regions)\n  ks_tmp &lt;- tmp_en\n\n  # Make lower case names\n  names(ks_tmp) &lt;- tolower(names(ks_tmp))\n  names(ks_tmp)[names(ks_tmp) == \"geography_code\"] &lt;- \"msoa11\"\n  names(ks_tmp)[names(ks_tmp) == \"geography_name\"] &lt;- \"name\"\n\n  # replace weird cell codes\n  onlynum &lt;- which(grepl(\"^[[:digit:]]+$\", ks_tmp$cell_code))\n  if(length(onlynum) != 0){\n    code &lt;- substr(ks_tmp$cell_code[-onlynum][1], 1, 7)\n    if(is.na(code)){\n      code &lt;- i\n    }\n    ks_tmp$cell_code[onlynum] &lt;- paste0(code, \"_\", ks_tmp$cell_code[onlynum])\n  }\n\n  # save codebook\n  ks_cb &lt;- unique(ks_tmp[, c(\"date\", \"cell_type\", \"cell\", \"cell_code\", \"cell_name\")])\n\n  ### Reshape\n  ks_res &lt;- tidyr::pivot_wider(ks_tmp, id_cols = c(\"msoa11\", \"name\"),\n                               names_from = \"cell_code\",\n                               values_from = \"obs_value\")\n\n  ### Merge\n  if(i == ksids[1]){\n    census_keystat.df &lt;- ks_res\n    census_keystat_cb.df &lt;- ks_cb\n  }else{\n    census_keystat.df &lt;- merge(census_keystat.df, ks_res, by = c(\"msoa11\", \"name\"), all = TRUE)\n    census_keystat_cb.df &lt;- rbind(census_keystat_cb.df, ks_cb)\n  }\n\n}\n\n\n# Descriptions are saved in the codebook\nhead(census_keystat_cb.df)\n\n# A tibble: 6 × 5\n   date cell_type     cell cell_code   cell_name                    \n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;                        \n1  2011 Ethnic Group     0 KS201EW0001 All usual residents          \n2  2011 Ethnic Group   100 KS201EW_100 White                        \n3  2011 Ethnic Group     1 KS201EW0002 White: English/Welsh/Scottis…\n4  2011 Ethnic Group     2 KS201EW0003 White: Irish                 \n5  2011 Ethnic Group     3 KS201EW0004 White: Gypsy or Irish Travel…\n6  2011 Ethnic Group     4 KS201EW0005 White: Other White           \n\nsave(census_keystat_cb.df, file = \"_data/Census_codebook.RData\")\n\nNow, we have one file containing the geometries of MSOAs and one file with the census information on ethnic groups. Obviously, we can easily merge them together using the MSOA identifiers.\n\nmsoa.spdf &lt;- merge(msoa.spdf, census_keystat.df,\n                   by.x = \"MSOA11CD\", by.y = \"msoa11\", all.x = TRUE)\n\nAnd we can, for instance, plot the spatial distribution of ethnic groups.\n\nmsoa.spdf$per_white &lt;- msoa.spdf$KS201EW_100 / msoa.spdf$KS201EW0001 * 100\nmsoa.spdf$per_mixed &lt;- msoa.spdf$KS201EW_200 / msoa.spdf$KS201EW0001 * 100\nmsoa.spdf$per_asian &lt;- msoa.spdf$KS201EW_300 / msoa.spdf$KS201EW0001 * 100\nmsoa.spdf$per_black &lt;- msoa.spdf$KS201EW_400 / msoa.spdf$KS201EW0001 * 100\nmsoa.spdf$per_other &lt;- msoa.spdf$KS201EW_500 / msoa.spdf$KS201EW0001 * 100\n\nmapview(msoa.spdf[, \"per_white\"])\n\n\n\n\n\n\nIf you’re interested in more data sources, see for instance APIs for social scientists: A collaborative review by Paul C. Bauer, Camille Landesvatter, Lion Behrens. It’s a collection of several APIs for social sciences.\n\n1.3.3 Gridded data\nSo far, we have queried data on administrative units. However, often data comes on other spatial scales. For instance, we might be interested in the amount of air pollution, which is provided on a regular grid across the UK from Defra.\n\n# Download\npol.link &lt;- \"https://uk-air.defra.gov.uk/datastore/pcm/mapno22011.csv\"\ndownload.file(pol.link, paste0(dn, \"/mapno22011.csv\"))\npol.df &lt;- read.csv(paste0(dn, \"/mapno22011.csv\"), skip = 5, header = T, sep = \",\",\n                      stringsAsFactors = F, na.strings = \"MISSING\")\n\nhead(pol.df)\n\n  ukgridcode      x       y no22011\n1      54291 460500 1221500      NA\n2      54292 461500 1221500      NA\n3      54294 463500 1221500      NA\n4      54979 458500 1220500      NA\n5      54980 459500 1220500      NA\n6      54981 460500 1220500      NA\n\n\nThe data comes as point data with x and y as coordinates. We have to transform this into spatial data first. We first setup a spatial points object with st_as_sf. Subsequently, we transform the point coordinates into a regular grid. We use a buffer method st_buffer with “diameter”, and only one segment per quadrant (nQuadSegs). This gives us a 1x1km regular grid.\n\n# Build spatial object\npol.spdf &lt;- st_as_sf(pol.df, coords = c(\"x\", \"y\"),\n                    crs = 27700)\n\n# we transform the point coordinates into a regular grid with \"diameter\" 500m\npol.spdf &lt;- st_buffer(pol.spdf, dist = 500, nQuadSegs  = 1,\n                      endCapStyle = 'SQUARE')\n\n# Plot NO2\nplot(pol.spdf[, \"no22011\"], border = NA)\n\n\n\n\n\n1.3.4 OpenStreetMap (points)\nAnother interesting data source is the OpenStreetMap API, which provides information about the geographical location of a serious of different indicators. Robin Lovelace provides a nice introduction to the osmdata API. Available features can be found on OSM wiki.\nFirst we create a bounding box of where we want to query data. st_bbox() can be used to get bounding boxes of an existing spatial object (needs CRS = 4326). An alternative would be to use opq(bbox = 'greater london uk').\n\n# bounding box of where we want to query data\nq &lt;- opq(bbox = st_bbox(st_transform(msoa.spdf, 4326)))\n\nAnd we want to get data for all pubs and bars which are within this bounding box.\n\n# First build the query of location of pubs in London\nosmq &lt;- add_osm_feature(q, key = \"amenity\", value = \"pub\")\n\n# And then query the data\npubs.osm &lt;- osmdata_sf(osmq)\n\nRight now there are some results in polygons, some in points, and they overlap. Often, data from OSM needs some manual cleaning. Sometimes the same features are represented by different spatial objects (e.g. points + polygons).\n\n# Make unique points / polygons\npubs.osm &lt;- unique_osmdata(pubs.osm)\n\n# Get points and polygons (there are barley any pubs as polygons, so we ignore them)\npubs.points &lt;- pubs.osm$osm_points\npubs.polys &lt;- pubs.osm$osm_multipolygons\n\n# # Drop OSM file\n# rm(pubs.osm); gc()\n\n# Reduce to point object only\npubs.spdf &lt;- pubs.points\n\n# Reduce to a few variables\npubs.spdf &lt;- pubs.spdf[, c(\"osm_id\", \"name\", \"addr:postcode\", \"diet:vegan\")]\n\nAgain, we can inspect the results with mapview.\n\nmapview(st_geometry(pubs.spdf))\n\n\n\n\n\nNote that OSM is solely based on contribution by users, and the quality of OSM data varies. Usually data quality is better in larger cities, and better for more stable features (such as hospitals, train stations, highways) rahter than pubs or restaurants which regularly appear and disappear. However, data from London Datastore would indicate more pubs than what we find with OSM.\n\n1.3.5 Save\nWe will store the created data to use them again in the next session.\n\nsave(msoa.spdf, file = \"_data/msoa_spatial.RData\")\nsave(ulez.spdf, file = \"_data/ulez_spatial.RData\")\nsave(pol.spdf, file = \"_data/pollution_spatial.RData\")\nsave(pubs.spdf, file = \"_data/pubs_spatial.RData\")"
  },
  {
    "objectID": "01_refresher.html#data-manipulation-visualization",
    "href": "01_refresher.html#data-manipulation-visualization",
    "title": "\n1  Refresher\n",
    "section": "\n1.4 Data Manipulation & Visualization",
    "text": "1.4 Data Manipulation & Visualization\nRequired packages\n\npkgs &lt;- c(\"sf\", \"gstat\", \"mapview\", \"nngeo\", \"rnaturalearth\", \"dplyr\",\n          \"nomisr\", \"osmdata\", \"tidyr\", \"texreg\", \"downlit\", \"xml2\") \nlapply(pkgs, require, character.only = TRUE)\n\nFor mapping\n\npkgs &lt;- c(\"tmap\", \"tmaptools\", \"viridisLite\", \n          \"ggplot2\", \"ggthemes\", \"rmapshaper\") \nlapply(pkgs, require, character.only = TRUE)"
  },
  {
    "objectID": "01_refresher.html#manipulation-and-linkage",
    "href": "01_refresher.html#manipulation-and-linkage",
    "title": "\n1  Refresher\n",
    "section": "\n1.5 Manipulation and linkage",
    "text": "1.5 Manipulation and linkage\nHaving data with geo-spatial information allows to perform a variety of methods to manipulate and link different data sources. Commonly used methods include 1) subsetting, 2) point-in-polygon operations, 3) distance measures, 4) intersections or buffer methods.\nThe online Vignettes of the sf package provide a comprehensive overview of the multiple ways of spatial manipulations.\nCheck if data is on common projection\n\nst_crs(msoa.spdf) == st_crs(pol.spdf)\n\n[1] FALSE\n\nst_crs(msoa.spdf) == st_crs(pubs.spdf)\n\n[1] FALSE\n\nst_crs(msoa.spdf) == st_crs(ulez.spdf)\n\n[1] FALSE\n\n# MSOA in different crs --&gt; transform\npol.spdf &lt;- st_transform(pol.spdf, crs = st_crs(msoa.spdf))\npubs.spdf &lt;- st_transform(pubs.spdf, crs = st_crs(msoa.spdf))\nulez.spdf &lt;- st_transform(ulez.spdf, crs = st_crs(msoa.spdf))\n\n\n# Check if all geometries are valid, and make valid if needed\nmsoa.spdf &lt;- st_make_valid(msoa.spdf)\n\n\n1.5.1 Subsetting\nWe can subset spatial data in a similar way as we subset conventional data.frames or matrices. For instance, below we simply reduce the pollution grid across the UK to observations in London only.\n\n# Subset to pollution estimates in London\npol_sub.spdf &lt;- pol.spdf[msoa.spdf, ] # or:\npol_sub.spdf &lt;- st_filter(pol.spdf, msoa.spdf)\nmapview(pol_sub.spdf)\n\n\n\n\n\n\nOr we can reverse the above and exclude all intersecting units by specifying st_disjoint as alternative spatial operation using the op = option (note the empty space for column selection). st_filter() with the .predicate option does the same job. See the sf Vignette for more operations.\n\n# Subset pubs to pubs not in the ulez area\nsub2.spdf &lt;- pubs.spdf[ulez.spdf, , op = st_disjoint] # or:\nsub2.spdf &lt;- st_filter(pubs.spdf, ulez.spdf, .predicate = st_disjoint)\nmapview(sub2.spdf)\n\n\n\n\n\n\nWe can easily create indicators of whether an MSOA is within ulez or not.\n\nmsoa.spdf$ulez &lt;- 0\n\n# intersecting lsoas\nwithin &lt;- msoa.spdf[ulez.spdf,]\n\n# use their ids to create binary indicator \nmsoa.spdf$ulez[which(msoa.spdf$MSOA11CD %in% within$MSOA11CD)] &lt;- 1\ntable(msoa.spdf$ulez)\n\n\n  0   1 \n955  28 \n\n\n\n1.5.2 Point in polygon\nWe are interested in the number of pubs in each MSOA. So, we count the number of points in each polygon.\n\n# Assign MSOA to each point\npubs_msoa.join &lt;- st_join(pubs.spdf, msoa.spdf, join = st_within)\n\n# Count N by MSOA code (drop geometry to speed up)\npubs_msoa.join &lt;- dplyr::count(st_drop_geometry(pubs_msoa.join),\n                               MSOA11CD = pubs_msoa.join$MSOA11CD,\n                               name = \"pubs_count\")\nsum(pubs_msoa.join$pubs_count)\n\n[1] 1601\n\n# Merge and replace NAs with zero (no matches, no pubs)\nmsoa.spdf &lt;- merge(msoa.spdf, pubs_msoa.join,\n                   by = \"MSOA11CD\", all.x = TRUE)\nmsoa.spdf$pubs_count[is.na(msoa.spdf$pubs_count)] &lt;- 0\n\n\n1.5.3 Distance measures\nWe might be interested in the distance to the nearest pub. Here, we use the package nngeo to find k nearest neighbours with the respective distance.\n\n# Use geometric centroid of each MSOA\ncent.sp &lt;- st_centroid(msoa.spdf[, \"MSOA11CD\"])\n\nWarning: st_centroid assumes attributes are constant over\ngeometries\n\n# Get K nearest neighbour with distance\nknb.dist &lt;- st_nn(cent.sp, \n                  pubs.spdf,\n                  k = 1,             # number of nearest neighbours\n                  returnDist = TRUE, # we also want the distance\n                  progress = FALSE)\n\nprojected points\n\nmsoa.spdf$dist_pubs &lt;- unlist(knb.dist$dist)\nsummary(msoa.spdf$dist_pubs)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   9.079  305.149  565.018  701.961  948.047 3735.478 \n\n\n\n1.5.4 Intersections + Buffers\nWe may also want the average pollution within 1 km radius around each MSOA centroid. Note that it is usually better to use a ego-centric method where you calculate the average within a distance rather than using the characteristic of the intersecting cells only (Lee et al. 2008; Mohai and Saha 2007).\nTherefore, we first create a buffer with st_buffer() around each midpoint and subsequently use st_intersetion() to calculate the overlap.\n\n# Create buffer (1km radius)\ncent.buf &lt;- st_buffer(cent.sp, \n                      dist = 1000) # dist in meters\nmapview(cent.buf)\n\n\n\n\n\n# Add area of each buffer (in this constant) \ncent.buf$area &lt;- as.numeric(st_area(cent.buf))\n\n# Calculate intersection of pollution grid and buffer\nint.df &lt;- st_intersection(cent.buf, pol.spdf)\n\nWarning: attribute variables are assumed to be spatially constant\nthroughout all geometries\n\nint.df$int_area &lt;- as.numeric(st_area(int.df)) # area of intersection\n\n# Area of intersection as share of buffer\nint.df$area_per &lt;- int.df$int_area / int.df$area\n\nAnd we use the percent overalp areas as the weights to calculate a weighted mean.\n\n# Aggregate as weighted mean\nint.df &lt;- st_drop_geometry(int.df)\nint.df$no2_weighted &lt;- int.df$no22011 * int.df$area_per\nint.df &lt;- aggregate(list(no2 = int.df[, \"no2_weighted\"]), \n                    by = list(MSOA11CD = int.df$MSOA11CD),\n                    sum)\n\n# Merge back to spatial data.frame\nmsoa.spdf &lt;- merge(msoa.spdf, int.df, by = \"MSOA11CD\", all.x = TRUE)\n\nmapview(msoa.spdf[, \"no2\"])\n\n\n\n\n\n\nNote: for buffer related methods, it often makes sense to use population weighted centroids instead of geographic centroids (see here for MSOA population weighted centroids). However, often this information is not available.\n\n1.5.5 and more\nThere are more spatial operation possible using sf. Have a look at the sf Cheatsheet.\n\n\n\n\n\n\n\nLee, Barrett A., Sean F. Reardon, Glenn Firebaugh, Chad R. Farrell, Stephen A. Matthews, and David O’Sullivan. 2008. “Beyond the Census Tract: Patterns and Determinants of Racial Segregation at Multiple Geographic Scales.” American Sociological Review 73 (5): 766–91. https://doi.org/10.1177/000312240807300504.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. 1st ed. Chapman & Hall/CRC the R Series. Boca Raton: Chapman & Hall/CRC.\n\n\nMohai, Paul, and Robin Saha. 2007. “Racial Inequality in the Distribution of Hazardous Waste: A National-Level Reassessment.” Social Problems 54 (3): 343–70. https://doi.org/10.1525/sp.2007.54.3.343."
  },
  {
    "objectID": "01_refresher.html#data-manipulation",
    "href": "01_refresher.html#data-manipulation",
    "title": "\n1  Refresher\n",
    "section": "\n1.4 Data Manipulation",
    "text": "1.4 Data Manipulation\nRequired packages\n\npkgs &lt;- c(\"sf\", \"gstat\", \"mapview\", \"nngeo\", \"rnaturalearth\", \"dplyr\",\n          \"nomisr\", \"osmdata\", \"tidyr\", \"texreg\", \"downlit\", \"xml2\") \nlapply(pkgs, require, character.only = TRUE)\n\nHaving data with geo-spatial information allows to perform a variety of methods to manipulate and link different data sources. Commonly used methods include 1) subsetting, 2) point-in-polygon operations, 3) distance measures, 4) intersections or buffer methods.\nThe online Vignettes of the sf package provide a comprehensive overview of the multiple ways of spatial manipulations.\nCheck if data is on common projection\n\nst_crs(msoa.spdf) == st_crs(pol.spdf)\n\n[1] FALSE\n\nst_crs(msoa.spdf) == st_crs(pubs.spdf)\n\n[1] FALSE\n\nst_crs(msoa.spdf) == st_crs(ulez.spdf)\n\n[1] FALSE\n\n\nThe spatial data files are on different projections. Before we can do any spatial operations with them, we have to transform them into a common projection.\n\n# MSOA in different crs --&gt; transform\npol.spdf &lt;- st_transform(pol.spdf, crs = st_crs(msoa.spdf))\npubs.spdf &lt;- st_transform(pubs.spdf, crs = st_crs(msoa.spdf))\nulez.spdf &lt;- st_transform(ulez.spdf, crs = st_crs(msoa.spdf))\n\n\n# Check if all geometries are valid, and make valid if needed\nmsoa.spdf &lt;- st_make_valid(msoa.spdf)\n\nThe st_make_valid() can help if the spatial geometries have some problems such as holes or points that don’t match exactly.\n\n1.4.1 Subsetting\nWe can subset spatial data in a similar way as we subset conventional data.frames or matrices. For instance, below we simply reduce the pollution grid across the UK to observations in London only.\n\n# Subset to pollution estimates in London\npol_sub.spdf &lt;- pol.spdf[msoa.spdf, ] # or:\npol_sub.spdf &lt;- st_filter(pol.spdf, msoa.spdf)\nmapview(pol_sub.spdf)\n\n\n\n\n\n\nOr we can reverse the above and exclude all intersecting units by specifying st_disjoint as alternative spatial operation using the op = option (note the empty space for column selection). st_filter() with the .predicate option does the same job. See the sf Vignette for more operations.\n\n# Subset pubs to pubs not in the ulez area\nsub2.spdf &lt;- pubs.spdf[ulez.spdf, , op = st_disjoint] # or:\nsub2.spdf &lt;- st_filter(pubs.spdf, ulez.spdf, .predicate = st_disjoint)\nmapview(sub2.spdf)\n\n\n\n\n\n\nWe can easily create indicators of whether an MSOA is within ulez or not.\n\nmsoa.spdf$ulez &lt;- 0\n\n# intersecting lsoas\nwithin &lt;- msoa.spdf[ulez.spdf,]\n\n# use their ids to create binary indicator \nmsoa.spdf$ulez[which(msoa.spdf$MSOA11CD %in% within$MSOA11CD)] &lt;- 1\ntable(msoa.spdf$ulez)\n\n\n  0   1 \n955  28 \n\n\n\n1.4.2 Point in polygon\nWe are interested in the number of pubs in each MSOA. So, we count the number of points in each polygon.\n\n# Assign MSOA to each point\npubs_msoa.join &lt;- st_join(pubs.spdf, msoa.spdf, join = st_within)\n\n# Count N by MSOA code (drop geometry to speed up)\npubs_msoa.join &lt;- dplyr::count(st_drop_geometry(pubs_msoa.join),\n                               MSOA11CD = pubs_msoa.join$MSOA11CD,\n                               name = \"pubs_count\")\nsum(pubs_msoa.join$pubs_count)\n\n[1] 1601\n\n# Merge and replace NAs with zero (no matches, no pubs)\nmsoa.spdf &lt;- merge(msoa.spdf, pubs_msoa.join,\n                   by = \"MSOA11CD\", all.x = TRUE)\nmsoa.spdf$pubs_count[is.na(msoa.spdf$pubs_count)] &lt;- 0\n\n\n1.4.3 Distance measures\nWe might be interested in the distance to the nearest pub. Here, we use the package nngeo to find k nearest neighbours with the respective distance.\n\n# Use geometric centroid of each MSOA\ncent.sp &lt;- st_centroid(msoa.spdf[, \"MSOA11CD\"])\n\nWarning: st_centroid assumes attributes are constant over\ngeometries\n\n# Get K nearest neighbour with distance\nknb.dist &lt;- st_nn(cent.sp, \n                  pubs.spdf,\n                  k = 1,             # number of nearest neighbours\n                  returnDist = TRUE, # we also want the distance\n                  progress = FALSE)\n\nprojected points\n\nmsoa.spdf$dist_pubs &lt;- unlist(knb.dist$dist)\nsummary(msoa.spdf$dist_pubs)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   9.079  305.149  565.018  701.961  948.047 3735.478 \n\n\n\n1.4.4 Intersections + Buffers\nWe may also want the average pollution within 1 km radius around each MSOA centroid. Note that it is usually better to use a ego-centric method where you calculate the average within a distance rather than using the characteristic of the intersecting cells only (Lee et al. 2008; Mohai and Saha 2007).\nTherefore, we first create a buffer with st_buffer() around each midpoint and subsequently use st_intersetion() to calculate the overlap.\n\n# Create buffer (1km radius)\ncent.buf &lt;- st_buffer(cent.sp, \n                      dist = 1000) # dist in meters\nmapview(cent.buf)\n\n\n\n\n\n# Add area of each buffer (in this constant) \ncent.buf$area &lt;- as.numeric(st_area(cent.buf))\n\n# Calculate intersection of pollution grid and buffer\nint.df &lt;- st_intersection(cent.buf, pol.spdf)\n\nWarning: attribute variables are assumed to be spatially constant\nthroughout all geometries\n\nint.df$int_area &lt;- as.numeric(st_area(int.df)) # area of intersection\n\n# Area of intersection as share of buffer\nint.df$area_per &lt;- int.df$int_area / int.df$area\n\nAnd we use the percent overalp areas as the weights to calculate a weighted mean.\n\n# Aggregate as weighted mean\nint.df &lt;- st_drop_geometry(int.df)\nint.df$no2_weighted &lt;- int.df$no22011 * int.df$area_per\nint.df &lt;- aggregate(list(no2 = int.df[, \"no2_weighted\"]), \n                    by = list(MSOA11CD = int.df$MSOA11CD),\n                    sum)\n\n# Merge back to spatial data.frame\nmsoa.spdf &lt;- merge(msoa.spdf, int.df, by = \"MSOA11CD\", all.x = TRUE)\n\nmapview(msoa.spdf[, \"no2\"])\n\n\n\n\n\n\nNote: for buffer related methods, it often makes sense to use population weighted centroids instead of geographic centroids (see here for MSOA population weighted centroids). However, often this information is not available.\n\n1.4.5 and more\nThere are more spatial operation possible using sf. Have a look at the sf Cheatsheet."
  },
  {
    "objectID": "01_refresher.html#data-visualisation",
    "href": "01_refresher.html#data-visualisation",
    "title": "\n1  Refresher\n",
    "section": "\n1.5 Data visualisation",
    "text": "1.5 Data visualisation\nFor mapping\n\npkgs &lt;- c(\"tmap\", \"tmaptools\", \"viridisLite\", \n          \"ggplot2\", \"ggthemes\", \"rmapshaper\") \nlapply(pkgs, require, character.only = TRUE)\n\nA large advantage of spatial data is that different data sources can be connected and combined. Another nice advantage is: you can create very nice maps. And it’s quite easy to do! Stefan Jünger & Anne-Kathrin Stroppe provide more comprehensive materials on mapping in their GESIS workshop on geospatial techniques in R.\nMany packages and functions can be used to plot maps of spatial data. For instance, ggplot as a function to plot spatial data using geom_sf(). I am personally a fan of tmap, which makes many steps easier (but sometimes is less flexible).\nA great tool for choosing coulour is for instance Colorbrewer. viridisLite provides another great resource to chose colours.\n\n1.5.1 Tmaps\nFor instance, lets plot the NO2 estimates using tmap + tm_fill() (there are lots of alternatives like tm_shape, tm_points(), tm_dots()).\n\n# Define colours\ncols &lt;- viridis(n = 7, direction = 1, option = \"C\")\n\nmp1 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = \"no2\", \n          style = \"fisher\", # algorithm to def cut points\n          n = 7, # Number of requested cut points\n          palette = cols, # colours\n          alpha = 1, # transparency \n          title = \"NO2\", \n          legend.hist = FALSE # histogram next to map?\n          ) +\n  tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) \n\nmp1\n\n\n\n\nTmap allows to easily combine different objects by defining a new object via tm_shape().\n\n# Define colours\ncols &lt;- viridis(n = 7, direction = 1, option = \"C\")\n\nmp1 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = \"no2\", \n          style = \"fisher\", # algorithm to def cut points\n          n = 7, # Number of requested cut points\n          palette = cols, # colours\n          alpha = 1, # transparency \n          title = \"NO2\", \n          legend.hist = FALSE # histogram next to map?\n          ) +\n  tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(ulez.spdf) +\n  tm_borders(col = \"red\", lwd = 1, alpha = 1) \n\nmp1\n\n\n\n\nAnd it is easy to change the layout.\n\n# Define colours\ncols &lt;- viridis(n = 7, direction = 1, option = \"C\")\n\nmp1 &lt;-  tm_shape(msoa.spdf) + \n  tm_fill(col = \"no2\", \n          style = \"fisher\", # algorithm to def cut points\n          n = 7, # Number of requested cut points\n          palette = cols, # colours\n          alpha = 1, # transparency \n          title = expression('in'~mu*'g'/m^{3}), \n          legend.hist = FALSE # histogram next to map?\n          ) +\n  tm_borders(col = \"white\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(ulez.spdf) +\n  tm_borders(col = \"red\", lwd = 1, alpha = 1) +\n  tm_layout(frame = FALSE,\n            legend.frame = TRUE, legend.bg.color = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            legend.outside = FALSE,\n            main.title = \"NO2\", \n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.title.size = 0.8,\n            legend.text.size = 0.8)\n\nmp1\n\n\n\n\n\n1.5.2 ggplot\nFor those of you have rather stick with the basic ggplot package, we can also use ggplot for spatial maps.\n\ngp &lt;- ggplot(msoa.spdf)+\n    geom_sf(aes(fill = no2))+\n    scale_fill_viridis_c(option = \"B\")+\n    coord_sf(datum = NA)+\n    theme_map()+\n    theme(legend.position = c(.9, .6))\ngp\n\n\n\n\n\n# Get some larger scale boundaries\nborough.spdf &lt;- st_read(dsn = paste0(\"_data\", \"/statistical-gis-boundaries-london/ESRI\"),\n                     layer = \"London_Borough_Excluding_MHW\" # Note: no file ending\n                     )\n\nReading layer `London_Borough_Excluding_MHW' from data source \n  `C:\\work\\Lehre\\Geodata_Spatial_Regression_short\\_data\\statistical-gis-boundaries-london\\ESRI' \n  using driver `ESRI Shapefile'\nSimple feature collection with 33 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n# transform to only inner lines\nborough_inner &lt;- ms_innerlines(borough.spdf)\n\n# Plot with inner lines\ngp &lt;- ggplot(msoa.spdf)+\n    geom_sf(aes(fill = no2), color = NA)+\n    scale_fill_viridis_c(option = \"A\")+\n    geom_sf(data = borough_inner, color = \"gray92\")+\n    geom_sf(data = ulez.spdf, color = \"red\", fill = NA)+\n    coord_sf(datum = NA)+\n    theme_map()+\n    labs(fill = \"NO2\")+\n    theme(legend.position = c(.9, .6))\ngp"
  },
  {
    "objectID": "01_refresher.html#exercise",
    "href": "01_refresher.html#exercise",
    "title": "\n1  Refresher\n",
    "section": "\n1.6 Exercise",
    "text": "1.6 Exercise\n\nWhat is the difference between a spatial “sf” object and a conventional “data.frame”? What’s the purpose of the function st_drop_geometry()?\nUsing msoa.spdf, please create a spatial data frame that contains only the MSOA areas that are within the ulez zone.\nPlease create a map for London (or only the msoa-ulez subset) which shows the share of Asian residents (or any other ethnic group).\nPlease calculate the distance of each MSOA to the London city centre\n\n\nuse google maps to get lon and lat,\nus st_as_sf() to create the spatial point\nuse st_distance() to calculate the distance\n\n\nCan you create a plot with the distance to the city centre and pub counts next to each other?\n\n\n\n\n\n\n\nLee, Barrett A., Sean F. Reardon, Glenn Firebaugh, Chad R. Farrell, Stephen A. Matthews, and David O’Sullivan. 2008. “Beyond the Census Tract: Patterns and Determinants of Racial Segregation at Multiple Geographic Scales.” American Sociological Review 73 (5): 766–91. https://doi.org/10.1177/000312240807300504.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. 1st ed. Chapman & Hall/CRC the R Series. Boca Raton: Chapman & Hall/CRC.\n\n\nMohai, Paul, and Robin Saha. 2007. “Racial Inequality in the Distribution of Hazardous Waste: A National-Level Reassessment.” Social Problems 54 (3): 343–70. https://doi.org/10.1525/sp.2007.54.3.343."
  }
]