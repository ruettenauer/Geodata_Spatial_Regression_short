\newcommand{\Exp}{\mathrm{E}}
\newcommand\given[1][]{\:#1\vert\:}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}

# Exercise II

### Required packages {.unnumbered}

```{r, message = FALSE, warning = FALSE, results = 'hide'}
pkgs <- c("sf", "mapview", "spdep", "spatialreg", "tmap", "viridisLite", 
          "plm", "splm", "SDPDmod")
lapply(pkgs, require, character.only = TRUE)

```

### Session info {.unnumbered}

```{r}
sessionInfo()

```




## Inkar data

Below, we read and transform some characteristics of the [INKAR data](https://www.inkar.de/) on German counties.


<!-- ```{r} -->
<!-- di <- c("_data/") -->

<!-- # Define the downloaded filed -->
<!-- j <- c("inkar.csv") -->
<!-- c <- 1 -->

<!-- for(k in j){ -->
<!--   header <- as.vector(t(read.table(paste0(di, k), nrows = 1, sep = ";")[1,])) -->
<!--   # Clean header -->
<!--   header <- stringi::stri_replace_all_fixed( -->
<!--     header,  -->
<!--     c("ä", "ö", "ü", "Ä", "Ö", "Ü"),  -->
<!--     c("ae", "oe", "ue", "Ae", "Oe", "Ue"),  -->
<!--     vectorize_all = FALSE -->
<!--   ) -->
<!--   header <- gsub(" ", "", header) -->
<!--   header <- gsub("\\.", "", header) -->
<!--   header <- iconv(header, "latin1", "ASCII", sub = "") -->

<!--   # Combine with second row header (year) -->
<!--   header2 <- as.vector(t(read.table(paste0(di, k), skip = 1, nrows = 1, sep = ";")[1,])) -->
<!--   header3 <- paste(header, header2, sep = "_") -->
<!--   header3 <- gsub("_NA", "", header3) -->

<!--   nc <- length(header3) -->
<!--   # Input and rename data -->
<!--   data <- read.csv(paste0(di, k), skip = 2, header = FALSE, sep = ";",  -->
<!--                    quote = "\"", dec = ",", stringsAsFactors = F, -->
<!--                    colClasses = "character") -->
<!--   names(data) <- header3 -->
<!--   data1 <- data -->

<!--   # Correct character vars (containing thousands separator) -->
<!--   vars <- which(sapply(data1, is.character)) -->
<!--   vars <- vars[-which(vars %in% c(1:3))] -->
<!--   for(l in vars){ -->
<!--     data1[,l] <- gsub("\\.", "", data1[,l]) -->
<!--     data1[,l] <- gsub("\\,", ".", data1[,l]) -->
<!--     data1[,l] <- as.numeric(data1[,l]) -->
<!--   } -->


<!--   # #Save -->
<!--   # l <- paste("bearb", k, sep = "_") -->
<!--   # write.table(data1, file = l, row.names = FALSE, sep = ";", dec = ".", na = ".") -->

<!--   # #Reshape -->
<!--   # helpvar1 <- unique(header[4:length(header)]) -->
<!--   # helpvar2 <-  sort(unique(header2[!is.na(header2)])) -->
<!--   # n_vars <- length(helpvar1) -->
<!--   # n_times <- length(helpvar2) -->
<!--   # helpvar1 <- sort(rep(helpvar1, times = n_times)) -->
<!--   # helpvar2 <- rep(helpvar2, times = n_vars) -->
<!--   # helpvar3 <- paste(helpvar1, helpvar2, sep = "_") -->
<!--   # count <- ncol(data1)+1 -->
<!--   # for(v in helpvar3) { -->
<!--   #   if(v %in% names(data1)) {} -->
<!--   #   else{ -->
<!--   #     data1[,count] <- NA -->
<!--   #     colnames(data1)[count] <- v -->
<!--   #     count <- count+1 -->
<!--   #   } -->
<!--   # } -->
<!--   # data1 <- data1[c(colnames(data1)[1:3], sort(helpvar3))] -->
<!--   #  -->
<!--   # data1 <- reshape(data1, direction = "long", varying = 4:ncol(data1),  -->
<!--   #                  sep = "_") -->
<!--   data.long <- tidyr::pivot_longer(data1,  -->
<!--                                   cols = 4:ncol(data1), -->
<!--                                   names_to = c(".value", "year"), -->
<!--                                   names_pattern = "(.*)_(.*)") -->


<!--   colnames(data.long) <- substr(colnames(data.long), 1, 30) -->

<!--   if(c == 1){ -->
<!--     inkar.df <- data.long -->
<!--   }else{ -->
<!--     inkar.df <- merge(inkar.df, data.long, all.x = TRUE, all.y = TRUE) -->
<!--   } -->

<!--   c <- c+1 -->

<!-- } -->



<!-- inkar.df$year <- as.numeric(inkar.df$year) -->

<!-- names(inkar.df)[which(names(inkar.df) == "Pkw-Dichte")] <- "pkw_dichte" -->

<!-- save(inkar.df, file = "_data/inkar.Rdata") -->

<!-- ``` -->

```{r}
load("_data/inkar2.Rdata")
```


Variables are

| Variable | Description |
| ------------					   | ------------ |
| "Kennziffer"                      | ID                                         |
| "Raumeinheit"                     | Name                                       |
| "Aggregat"                        | Level                                      |
| "year"                            | Year                                       |
| "poluation_density"               | Population Density       |
| "median_income"                   | Median Household income (only for 2020)                   |
| "gdp_in1000EUR"                   | Gross Domestic Product in 1000 euros                            |
| "unemployment_rate"               | Unemployment rate                            |
| "share_longterm_unemployed"       | Share of longterm unemployed (among unemployed)                               |
| "share_working_indutry"           | Share of employees in undistrial sector                    |
| "share_foreigners"                | Share of foreign nationals                              |
| "share_college"                   | Share of school-finishers with college degree                              |
| "recreational_space"              | Recreational space per inhabitant                           |
| "car_density"                     | Density of cars                                 |
| "life_expectancy"                 | Life expectancy       |


## County shapes

```{r}
kreise.spdf <- st_read(dsn = "_data/vg5000_ebenen_1231",
                       layer = "VG5000_KRS")
```

## Exercises

### Please map the life expectancy across Germany

### Test the effect of regional characteristics on life expectancy

1) Merge data with the shape file (as with conventional data)

```{r}
# Merge
inkar_2020.spdf <- merge(kreise.spdf, inkar.df[inkar.df$year == 2020, ], 
                         by.x = "AGS", by.y = "Kennziffer")
```

2) Create a map of life-expectancy

```{r}
cols <- viridis(n = 100, direction = -1, option = "G")

mp1 <-  tm_shape(inkar_2020.spdf) + 
  tm_fill(col = "life_expectancy", 
          style = "cont", # algorithm to def cut points
          palette = cols, # colours
          stretch.palette = TRUE,
          title = "in years"
          ) +
  tm_borders(col = "white", lwd = 0.5, alpha = 0.5) +
  tm_layout(frame = FALSE,
            legend.frame = TRUE, legend.bg.color = TRUE,
            legend.position = c("right", "bottom"),
            legend.outside = FALSE,
            main.title = "Life expectancy", 
            main.title.position = "center",
            main.title.size = 1.6,
            legend.title.size = 0.8,
            legend.text.size = 0.8)

mp1
```

3) Chose some variables that could predict life expectancy. See for instance the [following paper](https://doi.org/10.1073/pnas.2003719117).


4) Generate a neighbours object (e.g. the 10 nearest neighbours).

```{r}
# nb <- poly2nb(kreise.spdf, row.names = "ags", queen = TRUE)
knn <- knearneigh(st_centroid(kreise.spdf), k = 10)
nb <- knn2nb(knn, row.names = kreise.spdf$ags)
listw <- nb2listw(nb, style = "W")
```


5) Estimate a cross-sectional spatial model for the year 2020 and calculate the impacts.


```{r}

### Use a spatial Durbin Error model

# Spec formula
fm <- life_expectancy ~ median_income + unemployment_rate + share_college + car_density

# Estimate error model with Durbin = TRUE 
mod_1.durb <- errorsarlm(fm,  
                      data = inkar_2020.spdf, 
                      listw = listw,
                      Durbin = TRUE)

summary(mod_1.durb)

# Calculate impacts (which is unnecessary in this case)
mod_1.durb.imp <- impacts(mod_1.durb, listw = listw, R = 300)
summary(mod_1.durb.imp, zstats = TRUE, short = TRUE)

```


6) Calculate the spatial lagged variables for your covariates (e.g. use create_WX(), which needs a non-spatial df as input) .


```{r}
# Extract covariate names
covars <- attr(terms(fm),"term.labels")

w_vars <- create_WX(st_drop_geometry(inkar_2020.spdf)[, covars],
                    listw = listw,
                    prefix = "w")

inkar_2020.spdf <- cbind(inkar_2020.spdf, w_vars)
```


6) Can you run a spatial machine learning model? (for instance, using `randomForest`)?

```{r}
library(randomForest)

# Train
rf.mod <- randomForest(life_expectancy ~ median_income + unemployment_rate + share_college + car_density +
                         w.median_income + w.unemployment_rate + w.share_college + w.car_density,
                       data = st_drop_geometry(inkar_2020.spdf), 
                       ntree = 1000,
                       importance = TRUE)

# Inspect the mechanics of the model
importance(rf.mod)
varImpPlot(rf.mod)
```

You could even go further and use higher order neighbours (e.g. `nblag(queens.nb, maxlag = 3)`) to check the importance of direct neighbours and the neighbours neighbours and so on ...


<!-- b) Estimate a spatial panel model (use the specification you prefer) -->

<!-- I am estimating a spatial FE SAR model here: -->

<!-- ```{r} -->

<!-- # Drop NA -->
<!-- inkar_sub.df <- inkar.df[which(complete.cases(inkar.df[, all.vars(fm)])),] -->


<!-- ### Esimate FE SEM model -->
<!-- sarfe.mod <- spml(formula = fm,  -->
<!--                   data = inkar_sub.df,  -->
<!--                   index = c("Kennziffer", "year"),  # ID column -->
<!--                   listw = listw,          # listw -->
<!--                   model = "within",       # one of c("within", "random", "pooling"). -->
<!--                   effect = "individual",  # type of fixed effects -->
<!--                   lag = TRUE,             # spatila lg of Y -->
<!--                   spatial.error = "none", # "b" (Baltagi), "kkp" (Kapoor, Kelejian and Prucha) -->
<!--                   method = "eigen",       # estimation method, for large data e.g. ("spam", "Matrix" or "LU") -->
<!--                   na.action = na.fail,    # handling of missings -->
<!--                   zero.policy = NULL)     # handling of missings -->

<!-- summary(sarfe.mod) -->

<!-- ### Impacts -->
<!-- # Number of years -->
<!-- T <- length(unique(inkar_sub.df$year)) -->

<!-- # impacts -->
<!-- sarfe.mod.imp <- impacts(sarfe.mod, -->
<!--                          listw = listw, -->
<!--                          time = T) -->
<!-- summary(sarfe.mod.imp)      -->

<!-- ``` -->


<!-- 3) Calculate the impacts -->


<!-- ### Esimate an FE model with SLX specification -->

<!-- Loops over years to generate WX -->

<!-- ```{r} -->

<!-- # All years where we have a balanced sample -->
<!-- years <- unique(inkar.df$year[which(complete.cases(inkar.df[, all.vars(fm)]))]) -->

<!-- # All variables we want ot lag -->
<!-- vars <- all.vars(fm) -->

<!-- # create listw with the correct rownames (ID = Kennziffer) -->
<!-- kreise.spdf$Kennziffer <- kreise.spdf$ags -->
<!-- knn <- knearneigh(st_centroid(kreise.spdf), k = 10) -->
<!-- nb <- knn2nb(knn, row.names = kreise.spdf$Kennziffer) -->
<!-- listw <- nb2listw(nb, style = "W") -->

<!-- for(y in years){ -->
<!--   # Select singe year -->
<!--   tmp <- inkar.df[inkar.df$year == y ,] -->
<!--   # Select variables and make df -->
<!--   x <- st_drop_geometry(tmp[, vars]) -->
<!--   # Add ID as rownames (we retreive them again later) -->
<!--   rownames(x) <- tmp$Kennziffer -->
<!--   # Perform lag transformation (rownames contian ids) -->
<!--   w.tmp <- create_WX(as.matrix(x), -->
<!--                     listw = listw, -->
<!--                     prefix = "w", -->
<!--                     zero.policy = TRUE) # NAs will get zero -->
<!--   w.tmp <- as.data.frame(w.tmp) -->

<!--   # add indices back -->
<!--   w.tmp$Kennziffer <- row.names(w.tmp) -->
<!--   w.tmp$year <- y -->

<!--   if(y == years[1]){ -->
<!--     w.inkar.df <- w.tmp -->
<!--   }else{ -->
<!--     w.inkar.df <- rbind(w.inkar.df, w.tmp) -->
<!--   } -->
<!-- } -->

<!-- head(w.inkar.df) -->

<!-- # Merge back  -->

<!-- inkar.df <- merge(inkar.df, w.inkar.df, by = c("Kennziffer", "year")) -->

<!-- ``` -->


<!-- Estimate a twoways FE panel model -->

<!-- ```{r} -->
<!-- fm2 <- Lebenserwartung ~ Beschaeftigtenquote + Krankenhausbetten + pkw_dichte + HaushaltemitKindern + Straenverkehrsunfaelle + -->
<!--   w.Beschaeftigtenquote + w.Krankenhausbetten + w.pkw_dichte + w.HaushaltemitKindern + w.Straenverkehrsunfaelle -->

<!-- slx.fe <- plm(fm2, -->
<!--               data = inkar.df, -->
<!--               effects = "twoways", -->
<!--               model = "within") -->

<!-- summary(slx.fe) -->
<!-- ``` -->


<!-- <!-- We use the `WDI` API package to retrieve data from the World Bank. --> -->

<!-- <!-- You can open the [World Bank Data browser](https://databank.worldbank.org/home.aspx) to go though the data. --> -->

<!-- <!-- You can search for indicators with `WDIsearch()`. --> -->
<!-- <!-- ```{r warning=FALSE} --> -->
<!-- <!-- library(WDI) --> -->

<!-- <!-- # Search GDP per capita --> -->
<!-- <!-- WDIsearch("CO2 intensity") --> -->

<!-- <!-- # Political Stability --> -->
<!-- <!-- WDIsearch("Political Stability") --> -->

<!-- <!-- #  --> -->
<!-- <!-- WDIsearch("democracy") --> -->
<!-- <!-- # The Democracy indicator is an additive eleven-point scale (0-10) --> -->

<!-- <!-- ``` --> -->

<!-- <!-- ```{r, cache=TRUE} --> -->
<!-- <!-- # Define countries, indicators to query, and time period --> -->
<!-- <!-- wd.df <- WDI(country = "all",  --> -->
<!-- <!--              indicator = c('population' = "SP.POP.TOTL",  --> -->
<!-- <!--                            'gdp_pc' = "NY.GDP.PCAP.KD",  --> -->
<!-- <!--                            'co2_pc' = "EN.ATM.CO2E.PC", --> -->
<!-- <!--                            'co2_intesity' = "EN.ATM.CO2E.EG.ZS", --> -->
<!-- <!--                            'gini' = "SI.POV.GINI", --> -->
<!-- <!--                            'political_stability' = "GV.POLI.ST.ES", --> -->
<!-- <!--                            'inst_democr' = "UPP.INS.DEMO.XQ"), --> -->
<!-- <!--              extra = TRUE, --> -->
<!-- <!--              start = 2010, end = 2019) --> -->

<!-- <!-- # Save --> -->
<!-- <!-- save(wd.df, file = "_data/WDI_data.RData") --> -->
<!-- <!-- ``` --> -->


<!-- <!-- ## Diffusion of political regimes --> -->

<!-- <!-- See for instance @Gleditsch.2006 for an example for the diffusion of democratization. --> -->




